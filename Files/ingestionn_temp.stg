//import "datatype_mapping|artifact_ingestion"

table_metadata_query(templateData)::=<<
{
  "input_data": $generate_metadata_input(templateData.query_input)$,
   "sequential_templates":[
       {
         "query_template_group": "ingestion_templates|artifact_ingestion",
         "query_template_name": "fetching_script_inputs",
         "query_output_key": "spark_script_inputs",
         "query_type":"select"
       },
       {
         "query_template_group": "ingestion_templates|artifact_ingestion",
          "query_template_name": "get_table_adv_opt_partition_details",
          "query_output_key": "table_adv_opt_details_partition_details",
          "query_type":"select"
       },
       {
         "query_template_group": "ingestion_templates|artifact_ingestion",
         "query_template_name": "fetching_destination_type",
         "query_output_key": "fetching_destination_type",
         "query_type":"select"
        },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "get_ingestion_information",
        "query_output_key": "table_metadata_output",
        "query_type":"select"
      },
       {
       "query_template_group": "ingestion_templates|artifact_ingestion",
       "query_template_name": "get_partition_information_test",
       "query_output_key": "table_partition_output",
       "query_type":"select"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "fetch_object_column_metadata",
        "query_output_key": "column_metadata_output",
        "query_type":"select"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "insert_into_process_id_table_map",
        "query_output_key": "insert_into_process_id_table_map",
        "query_type":"insert"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "generate_credential_json",
        "query_output_key": "base64_encrypted_json_creds",
        "query_type":"json",
        "encryption_details" : {
                                    "encryption" : "yes",
                                    "encryption_type" : "BASE64"
                                }
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "convert_base64_encrypted_json_creds_to_rsa",
        "query_output_key": "base64_rsa_compressed_encrypted_creds",
        "query_type":"json",
        "encryption_details" : {
                                    "encryption" : "yes",
                                    "encryption_type" : "RSA_BASE64"
                               }
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "generate_source_input_json_for_checkpoint",
        "query_output_key": "input_json_for_checkpoint_status",
        "query_type":"json",
        "encryption_details" : {
                                    "encryption" : "yes",
                                    "encryption_type" : "BASE64"
                               }
      },
      {
         "query_template_group": "ingestion_templates|artifact_ingestion",
         "query_template_name": "generate_almaren_artifact",
         "query_output_key": "encrypted_script",
         "query_type":"json",
         "encryption_details" : {
                                     "encryption" : "yes",
                                     "encryption_type" : "BASE64"
                                 }
      }

   ],
   "output_keys":["table_partition_output","table_metadata_output","column_metadata_output","encrypted_script",
   "input_data",
   "base64_rsa_compressed_encrypted_creds","input_json_for_checkpoint_status","spark_script_inputs"]
}
>>

get_table_adv_opt_partition_details(templateData)::=<<
Select t.*,case when ((((advance_option_details ->\>'advanced_table_options')::json ->\> 'parallel_ingestion')::json)
->\> 'enable') = 'true'
           then true
           else false end as is_parallel_ingestion,case when ((((advance_option_details ->\>'advanced_table_options')::json ->\>'parallel_ingestion')::json) ->\> 'enable') = 'true'
           then ((((advance_option_details ->\>'advanced_table_options')::json ->\> 'parallel_ingestion')::json) ->\> 'max_connections')
           else null end as maximum_parallel_connections from nabu.advanced_options_object_details t where object_id =$templateData.input_data.where_condition$ and
data_movement_id=$templateData.input_data.data_movement_id$ and valid_to_ts='9999-12-31'
>>
table_metadata_query_almaran_spark(templateData)::=<<
{
  "input_data": $generate_curation_metadata_input(templateData)$,
   "sequential_templates":[
       {
         "query_template_group": "ingestion_templates|artifact_ingestion",
         "query_template_name": "fetching_script_inputs",
         "query_output_key": "spark_script_inputs_for_almaren_pyspark",
         "query_type":"select"
       },
      {
       "query_template_group": "ingestion_templates|artifact_ingestion",
       "query_template_name": "get_source_type",
       "query_output_key": "get_source_type",
       "query_type":"select"
       },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "get_metadata_information_pyspark_almaren",
        "query_output_key": "table_metadata_output_almaren_pyspark",
        "query_type":"select"
      },
       {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "insert_into_process_id_table_map",
        "query_output_key": "insert_into_process_id_table_map",
        "query_type":"insert"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "generate_spark_submit_command",
        "query_output_key": "generate_spark_submit_command",
        "query_type":"json",
        "encryption_details" : {
                                    "encryption" : "yes",
                                    "encryption_type" : "BASE64"
                               }
      }
   ],
   "output_keys":["spark_script_inputs_for_almaren_pyspark","table_metadata_output_almaren_pyspark","get_source_type","table_metadata_output","column_metadata_output","input_data","generate_spark_submit_command"]
}
>>


table_metadata_query_for_cleanup(templateData)::=<<
{
  "input_data": $generate_metadata_input(templateData.query_input)$,
   "sequential_templates":[
       {
         "query_template_group": "ingestion_templates|artifact_ingestion",
         "query_template_name": "fetching_script_inputs",
         "query_output_key": "spark_script_inputs_for_cleanup",
         "query_type":"select"
       },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "get_metadata_information_for_cleanup",
        "query_output_key": "table_metadata_output_for_cleanup",
        "query_type":"select"
      }
   ],
   "output_keys":["spark_script_inputs_for_cleanup","table_metadata_output_for_cleanup"]
}
>>

generate_curation_metadata_input(templateData)::=<<
{
"data_movement_id" : $templateData.query_input.data_movement_id$,
"flow_number" : $templateData.query_input.flow_number$,
"where_condition":"$templateData.query_input.where_condition$",
"process_id" : $templateData.query_input.process_id$,
"jwt_token" : "$templateData.query_input.botLogicOutputMap.jwt_token$",
"end_point" : "$templateData.query_input.botLogicOutputMap.end_point$",
"batch_id": $templateData.query_input.batch_id$,
"batch_name": "$templateData.query_input.batch_name$",
"job_type_id": "$templateData.query_input.job_type_id$",
"metadata_category" : "$templateData.query_input.metadata_category$",
"source": "$templateData.query_input.source$",
"datamovement_engine_type" : "$templateData.query_input.datamovement_engine_type$",
"retry_attempt" : "$templateData.retry_attempt$"
}
>>

query_for_status_insertion(templateData)::=<<
{
  "input_data": $generate_status_insertion_required_input(templateData.query_input)$,
   "sequential_templates":[
       {
         "query_template_group": "ingestion_templates|artifact_ingestion",
         "query_template_name": "insert_into_spark_job_result",
         "query_output_key": "insert_into_spark_job_result",
         "query_type":"insert"
       }
   ],
   "output_keys":[]
}
>>


fetching_script_inputs(templateData)::=<<
with fetch_ssh_configuration as (
select  case when c.config_json->\>'hostname' = '' or c.config_json->\>'hostname' is null then 'null' else c.config_json->\>'hostname' end as ssh_host,
        case when c.config_json->\>'username' = '' or c.config_json->\>'username' is null then 'null' else c.config_json->\>'username' end as ssh_username,
       case when b.compute_engine_info->\>'nabu_external_path'= '' or b.compute_engine_info->\>'nabu_external_path' is null  then 'null' else b.compute_engine_info->\>'nabu_external_path' end as spark_location,
        c.config_json->\>'kerberos' as kerberos,
        a.data_movement_id , b.engine_mapping_id,b.compute_engine_info
      from nabu.data_movement_physical a
inner join nabu.compute_engine b on a.compute_engine_id = b.compute_engine_id
inner join nabu.config c on  b.compute_engine_config_id = c.config_id
where a.data_movement_id =$templateData.input_data.data_movement_id$
)
select ssh_host,ssh_username,kerberos::boolean,d.spark_location,
case when engine_sub_type ~*'cdh|cdp' and kerberos ='false'
          then additional_properties ->\> 'non_kerberos_sub_engine_path'
          else additional_properties ->\> 'default_sub_engine_path' end as engine_path,replace(encode(compute_engine_info::text::bytea,'base64'),E'\n','') as compute_engine_info
from fetch_ssh_configuration d
inner join nabu.engine_mapping_lookup e on d.engine_mapping_id = e.engine_mapping_id
inner join nabu.engine_sub_type_lookup f on f.engine_sub_type_id = e.engine_sub_type_id
>>

generate_spark_submit_command(templateData)::=<<
$([first(templateData.get_source_type).dataplace_sub_component_type,"_spark_submit_command_generation"])(templateData)$
>>

almaren_spark_submit_command_generation(templateData)::=<<
$if(first(templateData.table_metadata_output_almaren_pyspark).is_cde)$
$(first(templateData.table_metadata_output_almaren_pyspark).cde_submit)$ $(first(templateData.table_metadata_output_almaren_pyspark).gitfilepath)$ $(first(templateData.table_metadata_output_almaren_pyspark).jobname_conf)$ $(first(templateData.table_metadata_output_almaren_pyspark).endline)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_default_command_options)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_bots_token_command_options)$
$if((first(templateData.spark_script_inputs_for_almaren_pyspark).kerberos))$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_kerberos_command_options)$
$endif$
$(first(templateData.table_metadata_output_almaren_pyspark).cluster_endpoint)$
$(first(templateData.table_metadata_output_almaren_pyspark).credentials_filepath)$
$(first(templateData.table_metadata_output_almaren_pyspark).cde_user)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_fireshots_url)$
$else$
$(first(templateData.table_metadata_output_almaren_pyspark).cat_for_almaren)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_default_command_options)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_bots_token_command_options)$
$if((first(templateData.spark_script_inputs_for_almaren_pyspark).kerberos))$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_kerberos_command_options)$
$endif$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_fireshots_url)$ $endif$
>>

pyspark_spark_submit_command_generation(templateData)::=<<
$if(first(templateData.table_metadata_output_almaren_pyspark).is_cde)$
$(first(templateData.table_metadata_output_almaren_pyspark).cde_submit)$ $(first(templateData.table_metadata_output_almaren_pyspark).gitfilepath)$ $(first(templateData.table_metadata_output_almaren_pyspark).jobname_conf)$ $(first(templateData.table_metadata_output_almaren_pyspark).endline)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_default_command_options)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_bots_token_command_options)$
$if((first(templateData.spark_script_inputs_for_almaren_pyspark).kerberos))$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_kerberos_command_options)$
$endif$
$(first(templateData.table_metadata_output_almaren_pyspark).cluster_endpoint)$
$(first(templateData.table_metadata_output_almaren_pyspark).credentials_filepath)$
$(first(templateData.table_metadata_output_almaren_pyspark).cde_user)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_fireshots_url)$
$else$
$(first(templateData.table_metadata_output_almaren_pyspark).cat_for_pyspark)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_default_command_options)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_bots_token_command_options)$
$if((first(templateData.spark_script_inputs_for_almaren_pyspark).kerberos))$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_kerberos_command_options)$
$endif$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_fireshots_url)$$(first(templateData.table_metadata_output_almaren_pyspark).endline)$
$(first(templateData.table_metadata_output_almaren_pyspark).git_file_path)$$endif$
>>

get_source_type(templateData)::=<<
select b.dataplace_sub_component_type from nabu.data_movement_physical a
inner join nabu.dataplace_sub_component_lookup b on a.dataplace_sub_component_id  = b.dataplace_sub_component_id
where data_movement_id  = $templateData.input_data.data_movement_id$ and valid_to_ts  = '9999-12-31'
>>

get_metadata_information_pyspark_almaren(templateData)::=<<
with cte as (
select $templateData.input_data.process_id$ as process_id,data_movement_id,'$templateData.input_data.jwt_token$' as jwt_token,
 convert_from(decode(((data_movement_additional_info->'flow_details')->'spark_config')->\>'spark_kerberos_command_options'||'==','base64'), 'utf-8') as spark_kerberos_command_options,
 regexp_replace(convert_from(decode(((data_movement_additional_info->'flow_details')->'spark_config')->\>'spark_bots_token_command_options'||'=','base64'), 'utf-8'),'<%bots_token%>','"$templateData.input_data.jwt_token$"'||' \') as spark_bots_token_command_options,
 regexp_replace(convert_from(decode(((data_movement_additional_info->'flow_details')->'spark_config')->\>'spark_default_command_options','base64'), 'utf-8'),'<%pipeline_name%>',data_movement_name) as spark_default_command_options,
(((data_movement_additional_info->'flow_details')->'git_info'))->\>'git_branch_or_tag' as git_branch_or_tag,
(((data_movement_additional_info->'flow_details')->'git_info'))->\>'git_file_path' as git_file_path,
'$(first(templateData.get_source_type).dataplace_sub_component_type)$' as curation_type,
'$templateData.input_data.retry_attempt$' as retry_attempt,
(((data_movement_additional_info->'flow_details')->'git_info'))->\>'git_url' as git_url,
(((data_movement_additional_info->'flow_details')->'git_info'))->\>'project_name' as project_name,
data_movement_name as pipeline_name,'--conf spark.nabu.fireshots_url="$templateData.input_data.end_point$"' as spark_fireshots_url
from nabu.data_movement_physical where data_movement_id = $templateData.input_data.data_movement_id$ and valid_to_ts ='9999-12-31'
)
,
fetch_ssh_configuration as (
select
case when estl.engine_sub_type~*'cde' then true else false end as is_cde, data_movement_id ,b.compute_engine_info
from nabu.data_movement_physical a
inner join nabu.compute_engine b on a.compute_engine_id = b.compute_engine_id
inner join nabu.engine_mapping_lookup eml on eml.engine_mapping_id =b.engine_mapping_id
inner join nabu.engine_sub_type_lookup estl on estl.engine_sub_type_id =eml.engine_sub_type_id
where a.data_movement_id =$templateData.input_data.data_movement_id$ and a.valid_to_ts ='9999-12-31'
),
spark_submit_details as(
select cte.*,fs.is_cde as is_cde,case when fs.is_cde
then
	concat('--vcluster-endpoint ',fs.compute_engine_info->\>'cluster_endpoint',' \') else null end as cluster_endpoint,
	case when fs.is_cde
then
	concat('--credentials-file ',fs.compute_engine_info->\>'credentials_filepath',' \') else null end as credentials_filepath,
	case when fs.is_cde
then
	concat('--user ',fs.compute_engine_info->\>'cde_user',' \') else null end as cde_user,
	'spark-submit  \' as cat_for_pyspark,
	case when fs.is_cde
then
	'cde spark submit'  end as cde_submit,
	        case when fs.is_cde
then
    'nabu-'||cte.curation_type||'-job-'||(select replace(to_char(current_timestamp ,'yyyy-mm-dd'),'-','') ||'-'||replace(to_char(current_timestamp ,'HH24-MI-SS'),'-','') ||'-'|| (SELECT floor(random() * 100000 + 1)::int))
    else null end as job_name,
      case when fs.is_cde
then	concat(cte.process_id,'_',cte.retry_attempt,'/',cte.project_name,'/',cte.git_file_path) else null end as gitfilepath ,
	'cat '||git_file_path||' | spark-shell \' as cat_for_almaren ,' \' as endline
from cte inner join fetch_ssh_configuration fs on cte.data_movement_id =fs.data_movement_id
)
select *,concat('--job-name ',job_name) as jobname_conf from spark_submit_details
>>

get_metadata_information_for_cleanup(templateData)::=<<
select b.process_id as process_id,
(((data_movement_additional_info->'flow_details')->'git_info'))->\>'git_branch_or_tag' as git_branch_or_tag,
(((data_movement_additional_info->'flow_details')->'git_info'))->\>'git_file_path' as git_file_path,
(((data_movement_additional_info->'flow_details')->'git_info'))->\>'git_url' as git_url,
(((data_movement_additional_info->'flow_details')->'git_info'))->\>'project_name' as project_name,
data_movement_name as pipeline_name
from nabu.data_movement_physical a
inner join nabu.process_id_table_map b
on a.data_movement_id = b.data_movement_id and batch_id = $templateData.input_data.batch_id$
where a.data_movement_id = $templateData.input_data.data_movement_id$
>>

get_table_metadata(templateData)::=<<
select distinct table_id,table_name,a.dataplace_id,a.schema_name,a.schema_id,b.ingestion_table_format,
e.dataplace_sub_component_type as source_type,g.dataplace_sub_component_type as destination_type,
b.destination_dataplace_id as dest_dataplace_id,b.destination_schema_directory_id as dest_schema_id,
h.schema_name as dest_schema_name,
case when d.dataplace_info->\>'connection_type' = 'hive' or d.dataplace_info->\>'connection_type' = 'cloudera_hive' then
lower(trim(replace(replace(replace( replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(((dataplace_info->\>'jdbc_info')::json->\>'database_name')||'_'||schema_name||'_'||table_name,'\$','_dlr_' ) ,'|','_pe_' ),'}','_rbr_' ),'{','_lbr_' ),'#','_hash_' ) ,'*','_astk_' ) ,'(','_op_' ) , ')','_cp_' ) ,'@','_at_' ),'.','_dot_' ),' ','_' ),'&','_amp_' ),'+','_pls_' ),'-','_hyp_' ),'?','_qtn_' ),':','_cln_' ),'/','_fs_' ),'''','_sq_' ),'%','_prst_' ),'!','_exp_')))
when d.dataplace_info->\>'connection_type' = 'sas' then a.schema_name||a.table_name
else d.dataplace_info->\>'database_name'||'_'||a.schema_name||'_'||a.table_name end as dest_table_name
from nabu.dataplace_table_metadata_physical a
inner join nabu.data_movement_details_physical b on b.source_dataplace_id = a.dataplace_id and b.valid_to_ts = '9999-12-31'
and table_id = $templateData.input_data.table_id$ and a.valid_to_ts = '9999-12-31'
inner join nabu.data_movement_physical c on b.data_movement_id = c.data_movement_id and c.valid_to_ts = '9999-12-31' and c.data_movement_id = $templateData.input_data.data_movement_id$
left outer join nabu.dataplace_physical d on b.source_dataplace_id = d.dataplace_id and d.valid_to_ts = '9999-12-31'
left outer join nabu.dataplace_sub_component_lookup e on d.dataplace_sub_component_id = e.dataplace_sub_component_id
left outer join nabu.dataplace_physical f on b.destination_dataplace_id = f.dataplace_id and d.valid_to_ts = '9999-12-31'
left outer join nabu.dataplace_sub_component_lookup g on f.dataplace_sub_component_id = g.dataplace_sub_component_id
left outer join nabu.dataplace_relational_component h on b.destination_dataplace_id = h.dataplace_id and b.destination_schema_directory_id = h.schema_id
>>

insert_destination_info_into_nabu(templateData)::=<<
$if(templateData.query_input.data.query_input.metadata.data.query_input.input_data.metadata_category)$
$(["insert_destination_info_into_nabu_for_",templateData.query_input.data.query_input.metadata.data.query_input.input_data.metadata_category,"_to_",templateData.query_input.data.query_input.metadata.data.query_input.input_data.metadata_category_destination])(templateData)$
$else$
$(["insert_destination_info_into_nabu_for_",templateData.query_input.data.data.query_input.input_data.metadata_category,"_to_",templateData.query_input.data.data.query_input.input_data.metadata_category_destination])(templateData)$
$endif$
>>

insert_destination_info_into_nabu_if_schema_drift(templateData)::=<<
$(["insert_destination_info_into_nabu_if_schema_drift_for_",templateData.query_input.data.query_input.metadata.data.templateData.query_input.input_data.metadata_category])(templateData)$
>>

insert_destination_info_into_nabu_for_file_to_relational(templateData)::=<<
{
  "input_data": $generate_destination_required_input(templateData.query_input)$,
   "sequential_templates":[
	  {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "get_data_movement_info",
        "query_output_key": "data_movement_details",
        "query_type":"select"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "insert_destination_file_metadata",
        "query_output_key": "insert_destination_table_metadata",
        "query_type":"insert"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "insert_destination_job_metadata_file",
        "query_output_key": "insert_destination_job_metadata",
        "query_type":"insert"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "insert_structured_jobtotable_file",
        "query_output_key": "insert_structured_jobtotable",
        "query_type":"insert"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "get_metadata_for_data_lineage_file",
        "query_output_key": "get_metadata_for_data_lineage",
        "query_type":"select"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "insert_data_entity_lineage_file",
        "query_output_key": "insert_data_entity_lineage",
        "query_type":"insert"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "insert_data_movement_errors",
        "query_output_key": "insert_data_movement_errors",
        "query_type":"insert"
      },
      {
         "query_template_group": "ingestion_templates|artifact_ingestion",
         "query_template_name": "advanced_options_mapping_details",
         "query_output_key": "advanced_options_mapping_details",
         "query_type":"select"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "insert_data_movement_schema_drift_details_file",
        "query_output_key": "insert_data_movement_schema_drift_details",
        "query_type":"insert"
      }
	],
       "output_keys":[]
}
>>


insert_destination_info_into_nabu_for_relational_to_file(templateData)::=<<
{
  "input_data": $generate_destination_required_input(templateData.query_input)$,
   "sequential_templates":[
	  {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "get_data_movement_info",
        "query_output_key": "data_movement_details",
        "query_type":"select"
      },

      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "insert_data_movement_errors",
        "query_output_key": "insert_data_movement_errors",
        "query_type":"insert"
      }
	],
       "output_keys":[]
}
>>

insert_destination_info_into_nabu_for_relational_to_relational(templateData)::=<<
{
  "input_data": $generate_destination_required_input(templateData.query_input)$,
   "sequential_templates":[

      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "get_data_movement_info",
        "query_output_key": "data_movement_details",
        "query_type":"select"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "insert_destination_table_metadata",
        "query_output_key": "insert_destination_table_metadata",
        "query_type":"insert"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "insert_destination_job_metadata",
        "query_output_key": "insert_destination_job_metadata",
        "query_type":"insert"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "insert_structured_jobtotable",
        "query_output_key": "insert_structured_jobtotable",
        "query_type":"insert"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "get_metadata_for_data_lineage",
        "query_output_key": "get_metadata_for_data_lineage",
        "query_type":"select"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "insert_data_entity_lineage",
        "query_output_key": "insert_data_entity_lineage",
        "query_type":"insert"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "insert_data_movement_errors",
        "query_output_key": "insert_data_movement_errors",
        "query_type":"insert"
      },
      {
         "query_template_group": "ingestion_templates|artifact_ingestion",
         "query_template_name": "advanced_options_mapping_details",
         "query_output_key": "advanced_options_mapping_details",
         "query_type":"select"
      },
      {
         "query_template_group": "ingestion_templates|artifact_ingestion",
         "query_template_name": "insert_data_movement_schema_drift_details",
         "query_output_key": "insert_data_movement_schema_drift_details",
         "query_type":"insert"
      }

    ],
  "output_keys":[]
}
>>

insert_destination_info_into_nabu_if_schema_drift_for_relational(templateData)::=<<
{
  "input_data": $generate_destination_required_input_if_schema_drift(templateData.query_input)$,
   "sequential_templates":[
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "get_data_movement_info",
        "query_output_key": "data_movement_details",
        "query_type":"select"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "insert_destination_table_metadata",
        "query_output_key": "insert_destination_table_metadata",
        "query_type":"insert"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "insert_destination_job_metadata",
        "query_output_key": "insert_destination_job_metadata",
        "query_type":"insert"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "insert_structured_jobtotable",
        "query_output_key": "insert_structured_jobtotable",
        "query_type":"insert"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "get_metadata_for_data_lineage",
        "query_output_key": "get_metadata_for_data_lineage",
        "query_type":"select"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "insert_data_entity_lineage",
        "query_output_key": "insert_data_entity_lineage",
        "query_type":"insert"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "insert_data_movement_errors",
        "query_output_key": "insert_data_movement_errors",
        "query_type":"insert"
      },
      {
         "query_template_group": "ingestion_templates|artifact_ingestion",
         "query_template_name": "advanced_options_mapping_details",
         "query_output_key": "advanced_options_mapping_details",
         "query_type":"select"
      },
      {
         "query_template_group": "ingestion_templates|artifact_ingestion",
         "query_template_name": "insert_data_movement_schema_drift_details",
         "query_output_key": "insert_data_movement_schema_drift_details",
         "query_type":"insert"
      }

    ],
  "output_keys":[]
}
>>

insert_destination_info_into_nabu_if_schema_drift_for_file(templateData)::=<<
{
  "input_data": $generate_destination_required_input_if_schema_drift(templateData.query_input)$,
   "sequential_templates":[
   	  {
           "query_template_group": "ingestion_templates|artifact_ingestion",
           "query_template_name": "get_data_movement_info",
           "query_output_key": "data_movement_details",
           "query_type":"select"
         },
         {
           "query_template_group": "ingestion_templates|artifact_ingestion",
           "query_template_name": "insert_destination_file_metadata",
           "query_output_key": "insert_destination_table_metadata",
           "query_type":"insert"
         },
         {
           "query_template_group": "ingestion_templates|artifact_ingestion",
           "query_template_name": "insert_destination_job_metadata_file",
           "query_output_key": "insert_destination_job_metadata",
           "query_type":"insert"
         },
         {
           "query_template_group": "ingestion_templates|artifact_ingestion",
           "query_template_name": "insert_structured_jobtotable_file",
           "query_output_key": "insert_structured_jobtotable",
           "query_type":"insert"
         },
         {
           "query_template_group": "ingestion_templates|artifact_ingestion",
           "query_template_name": "get_metadata_for_data_lineage_file",
           "query_output_key": "get_metadata_for_data_lineage",
           "query_type":"select"
         },
         {
           "query_template_group": "ingestion_templates|artifact_ingestion",
           "query_template_name": "insert_data_entity_lineage_file",
           "query_output_key": "insert_data_entity_lineage",
           "query_type":"insert"
         },
         {
           "query_template_group": "ingestion_templates|artifact_ingestion",
           "query_template_name": "insert_data_movement_errors",
           "query_output_key": "insert_data_movement_errors",
           "query_type":"insert"
         },
         {
            "query_template_group": "ingestion_templates|artifact_ingestion",
            "query_template_name": "advanced_options_mapping_details",
            "query_output_key": "advanced_options_mapping_details",
            "query_type":"select"
         },
         {
           "query_template_group": "ingestion_templates|artifact_ingestion",
           "query_template_name": "insert_data_movement_schema_drift_details_file",
           "query_output_key": "insert_data_movement_schema_drift_details",
           "query_type":"insert"
         }
   	],
  "output_keys":[]
}
>>

insert_data_movement_errors(templateData)::=<<
INSERT INTO nabu.data_movement_errors
(process_id, bot_uuid, process_context, error_json, created_ts, bot_type)
select process_id,'$templateData.input_data.bot_uuid$' as bot_uuid,'spark_ingestion' as process_context,error_msg as erro_json,current_timestamp as created_ts,
'SparkScriptBot' as bot_type
 from nabu.checkpoint_status
where job_type in ('nabu-ingestion-bot','nabu-sparkbot-ingestion') and status = 'ERROR' and valid_to_ts = '9999-12-31'
and data_movement_id = $templateData.input_data.data_movement_id$

>>

advanced_options_mapping_details(templateData)::=<<
select case when ((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'advanced_options_sub_type') is null then 'drop_create_table'
else ((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'advanced_options_sub_type') end as schema_drift_option,
((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'advanced_options_sub_type_id')::int as schema_drift_option_id,data_movement_id
from nabu.data_movement_physical
where data_movement_id = $templateData.input_data.data_movement_id$
and valid_to_ts = '9999-12-31'
>>

insert_data_movement_schema_drift_details(templateData)::=<<
INSERT INTO nabu.data_movement_schema_drift_details
(data_movement_id, dataplace_id, schema_directory_id, object_id, advanced_options_sub_type_id, advanced_options_sub_type, cru_by, cru_ts, batch_id, process_id)
select $templateData.input_data.data_movement_id$ as data_movement_id, a.dataplace_id, a.schema_id as schema_directory_id, a.table_id as object_id,
$first(templateData.advanced_options_mapping_details).schema_drift_option_id$ as advanced_options_sub_type_id,
'$first(templateData.advanced_options_mapping_details).schema_drift_option$' as advanced_options_sub_type, 'Modak', now(),
$templateData.input_data.batch_id$, $templateData.input_data.process_id$
from nabu.dataplace_table_schema_drift_details a
where a.valid_to_ts = '9999-12-31'
and a.table_id in ($templateData.input_data.where_condition$)
and $templateData.input_data.schema_drift_flag$ = true

>>

insert_data_movement_schema_drift_details_file(templateData)::=<<
INSERT INTO nabu.data_movement_schema_drift_details
(data_movement_id, dataplace_id, schema_directory_id, object_id, advanced_options_sub_type_id, advanced_options_sub_type, cru_by, cru_ts, batch_id, process_id)

select $templateData.input_data.data_movement_id$ as data_movement_id, a.dataplace_id, a.directory_id as schema_directory_id, a.file_id as object_id,
$first(templateData.advanced_options_mapping_details).schema_drift_option_id$ as advanced_options_sub_type_id,
'$first(templateData.advanced_options_mapping_details).schema_drift_option$' as advanced_options_sub_type, 'Modak', now(),
$templateData.input_data.batch_id$, $templateData.input_data.process_id$
from nabu.semi_structured_file_schema_drift a
where a.file_id in ($templateData.input_data.where_condition$)
and $templateData.input_data.schema_drift_flag$ = true

>>

get_data_movement_info(templateData)::=<<
select a.data_movement_id,b.destination_schema_directory_id,b.destination_dataplace_id,b.source_dataplace_id,b.source_schema_directory_id
from nabu.data_movement_physical a
inner join nabu.data_movement_details_physical b on a.data_movement_id = b.data_movement_id
and a.data_movement_id = $templateData.input_data.data_movement_id$
where a.valid_to_ts = '9999-12-31' and b.valid_to_ts = '9999-12-31'
>>

insert_destination_table_metadata(templateData)::=<<
INSERT INTO nabu.dataplace_table_metadata_physical
(dataplace_id, dataplace_component_type_id, schema_id, schema_name, table_name, create_table_ts, table_type, table_format,table_layer,valid_from_ts, valid_to_ts)
with source_table_details as (
select  table_id,b.dataplace_id,table_name,
case when dataplace_info->\>'connection_type' = 'hive' or dataplace_info->\>'connection_type' = 'cloudera_hive' then
(dataplace_info->\>'jdbc_info')::json->\>'database_name'
else dataplace_info->\>'database_name' end as database_name,
schema_name
from (
select
table_id,dataplace_id,schema_id,schema_name,table_name from
nabu.dataplace_table_metadata_physical
where table_id in ($templateData.input_data.where_condition$)   and valid_to_ts = '9999-12-31'
and table_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'
),
data_movement_details as (
select ingestion_table_format,valid_db_name,valid_schema_name,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value'||'_'
else '' end as ingestion_table_format,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'database_name')::boolean as valid_db_name,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'schema_name')::boolean as valid_schema_name
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id  from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
)
--select * from data_movement_details
,
source_details as (
select *,
$replacing_special_characters("coalesce(ingestion_table_format,'') || coalesce(db_name,'') || coalesce(source_schema_name,'') || table_name")$ as dest_table_name_2
from (
select a.*,
c.data_movement_id , destination_dataplace_id , destination_schema_directory_id,
ingestion_table_format,
case when valid_db_name is true then database_name || '_' else '' end as db_name,
case when valid_schema_name is true then schema_name || '_'  else '' end as source_schema_name
from source_table_details a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)a
)
--select * from source_details

,
destination_schema_details as (
select a.*,g.schema_name as destination_schema_name
from source_details a
inner join nabu.dataplace_relational_component_physical g on g.dataplace_id = a.destination_dataplace_id
and g.schema_id = a.destination_schema_directory_id
),
destination_dataplace_details as(
select a.*,f.dataplace_component_type_id
from destination_schema_details a
inner join nabu.dataplace_physical f on a.destination_dataplace_id = f.dataplace_id
inner join nabu.dataplace_sub_component_lookup e on f.dataplace_sub_component_id = e.dataplace_sub_component_id
),
destination_table_exists as(
select * from (
select a.*, case when a.dest_table_name_2 = b.table_name then true else false end as table_exists
from
destination_dataplace_details a
left outer join nabu.dataplace_table_metadata_physical b on a.destination_dataplace_id = b.dataplace_id and a.destination_schema_directory_id = b.schema_id
and a.dest_table_name_2 = b.table_name and valid_to_ts = '9999-12-31'
)a
where table_exists is false
),
destination_entry_table_metadata as  (
select * from (
select a.*,  case when table_id = d.source_table_id and dest_table_id is not null and table_exists is true then true else false end as flag
from destination_table_exists a
left outer join (  select
job_id,
data_movement_id,
max( case when object_type = 'S' then object_id end )as source_table_id,
max( case when object_type = 'S' then dataplace_id end )as source_dataplace_id,
max( case when object_type = 'F' then object_id end )as dest_table_id,
max( case when object_type = 'F' then dataplace_id end )as dest_dataplace_id
 from nabu.structured_jobtotable
where data_movement_id = $templateData.input_data.data_movement_id$
group by job_id,data_movement_id )  d on a.table_id = d.source_table_id and a.data_movement_id = d.data_movement_id
and a.destination_dataplace_id = d.dest_dataplace_id
) x
where
flag is false and table_exists is false
)
select  destination_dataplace_id as dataplace_id, dataplace_component_type_id,destination_schema_directory_id as   schema_id,
destination_schema_name, dest_table_name_2 as table_name, current_timestamp  as  create_table_ts, 'T' as table_type,'ingestion' as table_format,
'source' as table_layer,current_timestamp as valid_from_ts,'9999-12-31 00:00:00' :: timestamp as valid_to_ts  from destination_entry_table_metadata
>>

insert_destination_file_metadata(templateData)::=<<
INSERT INTO nabu.dataplace_table_metadata_physical
(dataplace_id, dataplace_component_type_id, schema_id, schema_name, table_name, create_table_ts, table_type, table_format,table_layer,valid_from_ts, valid_to_ts)
with source_table_details as (
select file_id,b.dataplace_id,file_name
from (
select file_id,dataplace_id,directory_id,file_name from
nabu.dataplace_file_metadata
where file_id in ($templateData.input_data.where_condition$) and valid_to_ts = '9999-12-31'
and file_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'
),
data_movement_details as (
select ingestion_table_format,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value'||'_'
else '' end as ingestion_table_format
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id  from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
)
--select * from data_movement_details
,
source_details as (
select *,
$replacing_special_characters_file("coalesce(ingestion_table_format,'') || file_name")$ as dest_file_name_2
from (
select a.*,
c.data_movement_id , destination_dataplace_id , destination_schema_directory_id,
ingestion_table_format
from source_table_details a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)a
)
--select * from source_details
,
destination_schema_details as (
select a.*,g.schema_name as destination_schema_name
from source_details a
inner join nabu.dataplace_relational_component_physical g on g.dataplace_id = a.destination_dataplace_id
and g.schema_id = a.destination_schema_directory_id
),
destination_dataplace_details as(
select a.*,f.dataplace_component_type_id
from destination_schema_details a
inner join nabu.dataplace_physical f on a.destination_dataplace_id = f.dataplace_id
inner join nabu.dataplace_sub_component_lookup e on f.dataplace_sub_component_id = e.dataplace_sub_component_id
),
destination_table_exists as(
select * from (
select a.*, case when a.dest_file_name_2 = b.table_name then true else false end as table_exists
from
destination_dataplace_details a
left outer join nabu.dataplace_table_metadata_physical b on a.destination_dataplace_id = b.dataplace_id and a.destination_schema_directory_id = b.schema_id
and a.dest_file_name_2 = b.table_name and valid_to_ts = '9999-12-31'
)a
where table_exists is false
),
destination_entry_table_metadata as  (
select * from (
select a.*,  case when file_id = d.source_table_id and dest_table_id is not null and table_exists is true then true else false end as flag
from destination_table_exists a
left outer join (  select
job_id,
data_movement_id,
max( case when object_type = 'S' then object_id end )as source_table_id,
max( case when object_type = 'S' then dataplace_id end )as source_dataplace_id,
max( case when object_type = 'F' then object_id end )as dest_table_id,
max( case when object_type = 'F' then dataplace_id end )as dest_dataplace_id
 from nabu.structured_jobtotable
where data_movement_id = $templateData.input_data.data_movement_id$
group by job_id,data_movement_id )  d on a.file_id = d.source_table_id and a.data_movement_id = d.data_movement_id
and a.destination_dataplace_id = d.dest_dataplace_id
) x
where
flag is false and table_exists is false
)
select  destination_dataplace_id as dataplace_id, dataplace_component_type_id,destination_schema_directory_id as schema_id,
destination_schema_name, dest_file_name_2 as table_name, current_timestamp  as  create_table_ts, 'T' as table_type,'ingestion' as table_format,
'source' as table_layer,current_timestamp as valid_from_ts,'9999-12-31 00:00:00' :: timestamp as valid_to_ts  from destination_entry_table_metadata
>>

insert_destination_job_metadata_file(templateData)::=<<
INSERT INTO nabu.job_metadata
(job_name, job_description, type_of_job, job_added_dt, job_owner, data_movement_id, cru_by, cru_ts)
with source_table_details as (
select file_id,b.dataplace_id,file_name
from (
select file_id,dataplace_id,directory_id,file_name from
nabu.dataplace_file_metadata
where file_id in ($templateData.input_data.where_condition$) and valid_to_ts = '9999-12-31'
and file_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'
),
data_movement_details as (
select ingestion_table_format,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value' || '_'
else '' end as ingestion_table_format
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id
from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
)
--select * from data_movement_details
,
source_details as (
select *,
$replacing_special_characters_file("coalesce(file_name,'') ")$ || '_' || destination_schema_directory_id || '_' || data_movement_id as job_name_2
from (
select a.*,
c.data_movement_id , destination_dataplace_id , destination_schema_directory_id,
ingestion_table_format
from source_table_details a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)a
)
--select * from source_details
-- Note : below query for matching dataplace_physical is only for validation
,
validating_dataplace as(
select a.*
from source_details a
inner join nabu.dataplace_physical f on a.destination_dataplace_id = f.dataplace_id
where f.valid_to_ts = '9999-12-31'
)
--select * from validating_dataplace;
,
entry_job_metadata as  (
select * from (
select a.*,  case when file_id = d.source_table_id and dest_table_id is not null  then true else false end as flag
from validating_dataplace a
left outer join (  select
job_id,
data_movement_id,
max( case when object_type = 'S' then object_id end )as source_table_id,
max( case when object_type = 'S' then dataplace_id end )as source_dataplace_id,
max( case when object_type = 'F' then object_id end )as dest_table_id,
max( case when object_type = 'F' then dataplace_id end )as dest_dataplace_id
 from nabu.structured_jobtotable
where data_movement_id = $templateData.input_data.data_movement_id$
group by job_id,data_movement_id )  d on a.file_id = d.source_table_id and a.data_movement_id = d.data_movement_id
and a.destination_dataplace_id = d.dest_dataplace_id
) x
where flag is false
)
select job_name,job_description,type_of_job,job_added_dt,job_owner,data_movement_id,cru_by,cru_ts from (
select job_name_2 as job_name,'Fetch Data for Ingestion' as job_description,'ingestion' as type_of_job,
current_timestamp as job_added_dt,'Modak' as job_owner,data_movement_id,'Modak' as cru_by,current_timestamp as cru_ts  from entry_job_metadata)x
>>


insert_destination_job_metadata(templateData)::=<<
INSERT INTO nabu.job_metadata
(job_name, job_description, type_of_job, job_added_dt, job_owner, data_movement_id, cru_by, cru_ts)
with source_table_details as (
select  table_id,b.dataplace_id,table_name,schema_name,
case when dataplace_info->\>'connection_type' = 'hive' or dataplace_info->\>'connection_type' = 'cloudera_hive' then
(dataplace_info->\>'jdbc_info')::json->\>'database_name'
else dataplace_info->\>'database_name' end as database_name
from (
select
table_id,dataplace_id,schema_id,schema_name,table_name from
nabu.dataplace_table_metadata_physical
where table_id in ($templateData.input_data.where_condition$)   and valid_to_ts = '9999-12-31'
and table_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'
),
data_movement_details as (
select ingestion_table_format,valid_db_name,valid_schema_name,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value' || '_'
else '' end as ingestion_table_format,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'database_name')::boolean as valid_db_name,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'schema_name')::boolean as valid_schema_name
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id
from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
),
source_details as (
select *,
$replacing_special_characters("coalesce(db_name,'') || coalesce(source_schema_name,'') ||  coalesce(table_name,'') ")$ || '_' || destination_schema_directory_id || '_' || data_movement_id as job_name_2
from (
select a.*,
c.data_movement_id , destination_dataplace_id , destination_schema_directory_id,
ingestion_table_format,
case when valid_db_name is true then database_name||'_' else '' end as db_name,
case when valid_schema_name is true then schema_name||'_' else '' end as source_schema_name
from source_table_details a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)a
),
-- Note : below query for matching dataplace_physical is only for validation
validating_dataplace as(
select a.*
from source_details a
inner join nabu.dataplace_physical f on a.destination_dataplace_id = f.dataplace_id
where f.valid_to_ts = '9999-12-31'
)
,
entry_job_metadata as  (
select * from (
select a.*,  case when table_id = d.source_table_id and dest_table_id is not null  then true else false end as flag
from validating_dataplace a
left outer join (  select
job_id,
data_movement_id,
max( case when object_type = 'S' then object_id end )as source_table_id,
max( case when object_type = 'S' then dataplace_id end )as source_dataplace_id,
max( case when object_type = 'F' then object_id end )as dest_table_id,
max( case when object_type = 'F' then dataplace_id end )as dest_dataplace_id
 from nabu.structured_jobtotable
where data_movement_id = $templateData.input_data.data_movement_id$
group by job_id,data_movement_id )  d on a.table_id = d.source_table_id and a.data_movement_id = d.data_movement_id
and a.destination_dataplace_id = d.dest_dataplace_id
) x
where flag is false
)
--select * from entry_job_metadata

select  job_name,job_description,type_of_job,job_added_dt,job_owner,data_movement_id,cru_by,cru_ts from (
select job_name_2 as job_name,'Fetch Data for Ingestion' as job_description,'ingestion' as type_of_job,
current_timestamp as job_added_dt,'Modak' as job_owner,data_movement_id,'Modak' as cru_by,current_timestamp as cru_ts  from entry_job_metadata)x
>>



insert_structured_jobtotable(templateData)::=<<
INSERT INTO nabu.structured_jobtotable
(dataplace_id, data_movement_id, job_id, object_id, object_type, cru_by, cru_ts)
with source_table_details as (
select  table_id,b.dataplace_id,schema_id,schema_name,table_name,
case when dataplace_info->\>'connection_type' = 'hive' or dataplace_info->\>'connection_type' = 'cloudera_hive' then
(dataplace_info->\>'jdbc_info')::json->\>'database_name'
else dataplace_info->\>'database_name' end as database_name
from (
select
table_id,dataplace_id,schema_id,schema_name,table_name from
nabu.dataplace_table_metadata_physical
where table_id in ($templateData.input_data.where_condition$)   and valid_to_ts = '9999-12-31'
and table_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'

),
data_movement_details as (
select ingestion_table_format,valid_db_name,valid_schema_name,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value' || '_'
else '' end as ingestion_table_format,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'database_name')::boolean as valid_db_name,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'schema_name')::boolean as valid_schema_name
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id
from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
),
source_details as (
select *,case when ingestion_table_format is not null then ingestion_table_format || job_name else job_name end as dest_table_name,
job_name || '_' || destination_schema_directory_id || '_' || data_movement_id as job_name_2
from (
select *,
$replacing_special_characters("coalesce(db_name,'') || coalesce(source_schema_name,'') ||  coalesce(table_name,'')")$ as job_name
from (
select a.*,
c.data_movement_id , destination_dataplace_id , destination_schema_directory_id,
ingestion_table_format,
case when valid_db_name is true then database_name||'_' else '' end as db_name,
case when valid_schema_name is true then schema_name||'_' else '' end as source_schema_name
from source_table_details a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)a
)b
)
--select * from source_details
,
job_details as (
select a.*,b.job_id
from source_details a
left outer join (select *  from nabu.job_metadata where data_movement_id = $templateData.input_data.data_movement_id$) b
on a.job_name_2 = b.job_name
),
destination_table_id_details as (
select a.*, b.table_id as destination_table_id
from job_details a left outer join  nabu.dataplace_table_metadata_physical b
    on  a.destination_schema_directory_id = b.schema_id  and a.destination_dataplace_id = b.dataplace_id
and a.dest_table_name = b.table_name and  b.valid_to_ts = '9999-12-31'

)
--select * from destination_table_id
,
no_entry_in_table_metadata as  (
select * from (
select a.*,  case when table_id = d.source_table_id and dest_table_id is not null then true else false end as flag
from destination_table_id_details a
left outer join (  select
job_id,
data_movement_id,
max( case when object_type = 'S' then object_id end )as source_table_id,
max( case when object_type = 'S' then dataplace_id end )as source_dataplace_id,
max( case when object_type = 'F' then object_id end )as dest_table_id,
max( case when object_type = 'F' then dataplace_id end )as dest_dataplace_id
 from nabu.structured_jobtotable
where data_movement_id = $templateData.input_data.data_movement_id$
group by job_id,data_movement_id )  d on a.table_id = d.source_table_id and a.data_movement_id = d.data_movement_id
and a.destination_dataplace_id = d.dest_dataplace_id
) x
where flag is false
)
select dataplace_id,data_movement_id, job_id,table_id as object_id,'S' as object_type,'Modak' as cru_by,current_timestamp as cru_ts
from no_entry_in_table_metadata
union
select destination_dataplace_id, data_movement_id, job_id,destination_table_id,'F' as object_type,'Modak' as cru_by,current_timestamp as cru_ts
from no_entry_in_table_metadata
>>

insert_structured_jobtotable_file(templateData)::=<<
INSERT INTO nabu.structured_jobtotable
(dataplace_id, data_movement_id, job_id, object_id, object_type, cru_by, cru_ts)
with source_table_details as (
select file_id,b.dataplace_id,file_name
from (
select file_id,dataplace_id,directory_id,file_name from
nabu.dataplace_file_metadata
where file_id in ($templateData.input_data.where_condition$) and valid_to_ts = '9999-12-31'
and file_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'

),
data_movement_details as (
select ingestion_table_format,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value' || '_'
else '' end as ingestion_table_format
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id
from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
),
source_details as (
select *,case when ingestion_table_format is not null then ingestion_table_format || job_name else job_name end as dest_table_name,
job_name || '_' || destination_schema_directory_id || '_' || data_movement_id as job_name_2
from (
select *,
$replacing_special_characters_file("coalesce(file_name,'')")$ as job_name
from (
select a.*,
c.data_movement_id , destination_dataplace_id , destination_schema_directory_id,
ingestion_table_format
from source_table_details a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)a
)b
)
--select * from source_details
,
job_details as (
select a.*,b.job_id
from source_details a
left outer join (select *  from nabu.job_metadata where data_movement_id = $templateData.input_data.data_movement_id$) b
on a.job_name_2 = b.job_name
),
destination_table_id_details as (
select a.*, b.table_id as destination_table_id
from job_details a left outer join  nabu.dataplace_table_metadata_physical b
    on  a.destination_schema_directory_id = b.schema_id  and a.destination_dataplace_id = b.dataplace_id
and a.dest_table_name = b.table_name and  b.valid_to_ts = '9999-12-31'

),
no_entry_in_table_metadata as  (
select * from (
select a.*,  case when file_id = d.source_table_id and dest_table_id is not null then true else false end as flag
from destination_table_id_details a
left outer join (  select
job_id,
data_movement_id,
max( case when object_type = 'S' then object_id end )as source_table_id,
max( case when object_type = 'S' then dataplace_id end )as source_dataplace_id,
max( case when object_type = 'F' then object_id end )as dest_table_id,
max( case when object_type = 'F' then dataplace_id end )as dest_dataplace_id
 from nabu.structured_jobtotable
where data_movement_id = $templateData.input_data.data_movement_id$
group by job_id,data_movement_id )  d on a.file_id = d.source_table_id and a.data_movement_id = d.data_movement_id
and a.destination_dataplace_id = d.dest_dataplace_id
) x
where flag is false
)
select dataplace_id,data_movement_id, job_id,a.file_id as object_id,'S' as object_type,'Modak' as cru_by,current_timestamp as cru_ts
from no_entry_in_table_metadata a
union
select destination_dataplace_id, data_movement_id, job_id,destination_table_id,'F' as object_type,'Modak' as cru_by,current_timestamp as cru_ts
from no_entry_in_table_metadata
>>


get_metadata_for_data_lineage_file(templateData)::=<<
with source_table_details as (
select file_id,b.dataplace_id,e.dataplace_sub_component_id as source_dataplace_sub_component_id ,file_name
from (
select file_id,dataplace_id,directory_id,file_name from
nabu.dataplace_file_metadata
where file_id in ($templateData.input_data.where_condition$) and valid_to_ts = '9999-12-31'
and file_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'
),
data_movement_details as (
select ingestion_table_format,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value' || '_'
else '' end as ingestion_table_format
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id
from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
),
source_details as (
select *,
$replacing_special_characters_file(" coalesce(ingestion_table_format,'')  ||  coalesce(file_name,'') ")$ as dest_table_name_2
from (
select a.*,
c.data_movement_id , destination_dataplace_id , destination_schema_directory_id,
ingestion_table_format
from source_table_details a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)a
)
--select * from source_details
,
destination_dataplace_details as(
select a.*, f.dataplace_sub_component_id as destination_dataplace_sub_component_id
from source_details a
inner join nabu.dataplace_physical f on a.destination_dataplace_id = f.dataplace_id
where f.valid_to_ts = '9999-12-31'
),
destination_table_id as (
select a.*, b.table_id as destination_table_id
from destination_dataplace_details a left outer join  nabu.dataplace_table_metadata_physical b
    on  a.destination_schema_directory_id = b.schema_id  and a.destination_dataplace_id = b.dataplace_id
and a.dest_table_name_2 = b.table_name and  b.valid_to_ts = '9999-12-31'
),
get_source_destination_type as(
select a.* , f.dataplace_sub_component_type as source_type,g.dataplace_sub_component_type as dest_type
from destination_table_id a
left outer join nabu.dataplace_sub_component_lookup f on a.source_dataplace_sub_component_id  = f.dataplace_sub_component_id
left outer join nabu.dataplace_sub_component_lookup g on a.destination_dataplace_sub_component_id = g.dataplace_sub_component_id
)
select file_id,data_movement_id as data_movement_id, destination_table_id ,  source_type,dest_type,current_timestamp as req_ts
from get_source_destination_type
>>

get_metadata_for_data_lineage(templateData)::=<<
with source_table_details as (
select  table_id,b.dataplace_id,e.dataplace_sub_component_id as source_dataplace_sub_component_id ,schema_name,table_name,
case when dataplace_info->\>'connection_type' = 'hive' or dataplace_info->\>'connection_type' = 'cloudera_hive' then
(dataplace_info->\>'jdbc_info')::json->\>'database_name'
else dataplace_info->\>'database_name' end as database_name
from (
select
table_id,dataplace_id,schema_id,schema_name,table_name from
nabu.dataplace_table_metadata_physical
where table_id in ($templateData.input_data.where_condition$) and valid_to_ts = '9999-12-31'
and table_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'
),
data_movement_details as (
select ingestion_table_format,valid_db_name,valid_schema_name,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value' || '_'
else '' end as ingestion_table_format,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'database_name')::boolean as valid_db_name,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'schema_name')::boolean as valid_schema_name
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id
from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
),
source_details as (
select *,
$replacing_special_characters(" coalesce(ingestion_table_format,'') || coalesce(db_name,'') || coalesce(source_schema_name,'') ||  coalesce(table_name,'') ")$ as dest_table_name_2
from (
select a.*,
c.data_movement_id , destination_dataplace_id , destination_schema_directory_id,
ingestion_table_format,
case when valid_db_name is true then database_name || '_' else '' end as db_name,
case when valid_schema_name is true then schema_name || '_'  else '' end as source_schema_name
from source_table_details a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)a
)
--select * from source_details
,
destination_dataplace_details as(
select a.*, f.dataplace_sub_component_id as destination_dataplace_sub_component_id
from source_details a
inner join nabu.dataplace_physical f on a.destination_dataplace_id = f.dataplace_id
where f.valid_to_ts = '9999-12-31'
),
destination_table_id as (
select a.*, b.table_id as destination_table_id
from destination_dataplace_details a left outer join  nabu.dataplace_table_metadata_physical b
    on  a.destination_schema_directory_id = b.schema_id  and a.destination_dataplace_id = b.dataplace_id
and a.dest_table_name_2 = b.table_name and  b.valid_to_ts = '9999-12-31'
),
get_source_destination_type as(
select a.* , f.dataplace_sub_component_type as source_type,g.dataplace_sub_component_type as dest_type
from destination_table_id a
left outer join nabu.dataplace_sub_component_lookup f on a.source_dataplace_sub_component_id  = f.dataplace_sub_component_id
left outer join nabu.dataplace_sub_component_lookup g on a.destination_dataplace_sub_component_id = g.dataplace_sub_component_id
)
select  table_id,data_movement_id as data_movement_id, destination_table_id ,  source_type,dest_type,current_timestamp as req_ts
from get_source_destination_type
>>

insert_data_entity_lineage(templateData)::=<<
$if(templateData.get_metadata_for_data_lineage)$
INSERT INTO nabu.data_entity_lineage_physical
(data_movement_id, source_object_id, destination_object_id, lineage_uuid, lineage_document, cru_by, cru_ts)
VALUES $templateData.get_metadata_for_data_lineage:generate_data_lineage_insert_statement();separator=","$
$endif$
>>

insert_data_entity_lineage_file(templateData)::=<<
$if(templateData.get_metadata_for_data_lineage)$
INSERT INTO nabu.data_entity_lineage_physical
(data_movement_id, source_object_id, destination_object_id, lineage_uuid, lineage_document, cru_by, cru_ts)
VALUES $templateData.get_metadata_for_data_lineage:generate_data_lineage_insert_statement_file();separator=","$
$endif$
>>

generate_data_lineage_insert_statement(templateData)::=<<
($templateData.data_movement_id$,$templateData.table_id$,$templateData.destination_table_id$,uuid_in((md5((random())::text))::cstring),'$generate_data_lineage(templateData)$','Modak',current_timestamp)
>>

generate_data_lineage_insert_statement_file(templateData)::=<<
($templateData.data_movement_id$,$templateData.file_id$,$templateData.destination_table_id$,uuid_in((md5((random())::text))::cstring),'$generate_data_lineage_file(templateData)$','Modak',current_timestamp)
>>

lineage_requested_by(templateData)::=<<
Modak
>>
generate_data_lineage(templateData) ::=<<
{"data_lineage": { "meta_data": { "created_by": "lineage_bot","requested_by": "$lineage_requested_by(templateData)$","requested_timestamp": "$templateData.req_ts$","executed_timestamp": "$templateData.req_ts$","tags": {}},"source_to_target_info":$get_sources(templateData)$}}
>>

generate_data_lineage_file(templateData) ::=<<
{"data_lineage": { "meta_data": { "created_by": "lineage_bot","requested_by": "$lineage_requested_by(templateData)$","requested_timestamp": "$templateData.req_ts$","executed_timestamp": "$templateData.req_ts$","tags": {}},"source_to_target_info":$get_sources_file(templateData)$}}
>>

get_sources(sourceMap)::=<<
{"src_type": "$templateData.source_type$","src_encrypted": "n","target_type": "$templateData.dest_type$","target_encrypted": "n","src_table_id":$templateData.table_id$,"dest_table_id":$templateData.destination_table_id$,"columns":[]}
>>

get_sources_file(sourceMap)::=<<
{"src_type": "$templateData.source_type$","src_encrypted": "n","target_type": "$templateData.dest_type$","target_encrypted": "n","src_table_id":$templateData.file_id$,"dest_table_id":$templateData.destination_table_id$,"columns":[]}
>>

insert_into_process_id_table_map(templateData)::=<<
 INSERT INTO nabu.process_id_table_map (data_movement_id, process_id, object_id, process_started_ts, batch_id, batch_name) VALUES
 ($templateData.input_data.data_movement_id$, $templateData.input_data.process_id$, $templateData.input_data.where_condition$, current_timestamp, $templateData.input_data.batch_id$, '$templateData.input_data.batch_name$')
 >>

generate_status_insertion_required_input(templateData)::=<<
{
"data_movement_id" : $templateData.data.data.input_data.data_movement_id$,
"bot_uuid" : "$templateData.bot_uuid$",
"process_id": $templateData.process_id$,
"batch_id": $templateData.data.data.input_data.batch_id$,
"table_id": $templateData.data.data.input_data.where_condition$,
"ssh_host": "$templateData.ssh_host$",
"applicationId": "$templateData.data.botLogicOutputMap.applicationId$",
$if(templateData.data.botLogicOutputMap.error_details)$"error_details": "$templateData.data.botLogicOutputMap.error_details$",$endif$
"exit_status": "$templateData.data.botLogicOutputMap.exit_status$",
"status": "$templateData.data.botLogicOutputMap.status$"
}
>>


//Here, As schema_drift_flag is a boolean type, the if else case for schema_drift_flage is based on data_movement_id.
generate_destination_required_input(templateData)::=<<
{
"data_movement_id" : $if(templateData.data.data.data_movement_id)$$templateData.data.data.data_movement_id$$else$$templateData.data.data_movement_id$$endif$,
"flow_number" : $if(templateData.data.data.flow_number)$$templateData.data.data.flow_number$$else$$templateData.data.flow_number$$endif$,
"where_condition" : $if(templateData.data.data.where_condition)$"$templateData.data.data.where_condition$"$else$"$templateData.data.where_condition$"$endif$,
"bot_uuid" : "$templateData.bot_uuid$",
"schema_drift_flag": $if(templateData.data.data.data_movement_id)$$templateData.data.data.schema_drift_flag$$else$$templateData.data.schema_drift_flag$$endif$,
"process_id": $templateData.process_id$,
"batch_id": $templateData.batch_id$
}
>>

generate_destination_required_input_if_schema_drift(templateData)::=<<
{
"data_movement_id" : $templateData.data.query_input.metadata.data_movement_id$,
"flow_number" : $templateData.data.query_input.metadata.flow_number$,
"where_condition" : "$templateData.data.query_input.metadata.where_condition$",
"bot_uuid" : "$templateData.bot_uuid$",
"schema_drift_flag": $templateData.data.query_input.metadata.schema_drift_flag$,
"process_id": $templateData.process_id$,
"batch_id": $templateData.batch_id$
}
>>

generate_metadata_input(templateData)::=<<
{
"data_movement_id" : $templateData.data_movement_id$,
"flow_number" : $templateData.flow_number$,
"where_condition":"$templateData.where_condition$",
"process_id" : $templateData.process_id$,
"jwt_token" : "$templateData.botLogicOutputMap.jwt_token$",
"end_point" : "$templateData.botLogicOutputMap.end_point$",
"batch_id": $templateData.batch_id$,
"batch_name": "$templateData.batch_name$",
"job_type_id": "$templateData.job_type_id$",
"metadata_category" : "$templateData.metadata_category$",
"metadata_category_destination" : "$templateData.metadata_category_destination$",
"source": "$templateData.source$",
"intermediate_type": "$templateData.intermediate_type$",
"is_create_table" : $templateData.is_create_table$,
"destination" : "$templateData.destination$",
"datamovement_engine_type" : "$templateData.datamovement_engine_type$"
}
>>

data_movement_info(templateData) ::=<<
select  string_agg('('||where_condition||')',' or ') as where_condition,workflow_engine_name,workflow_name,workflow_engine_id
from(
select  data_movement_id,source_dataplace_id,source_schema_id,
case when where_condition is null then 'schema_id = '|| source_schema_id || ' and table_type in ('||ingest_all_tables_views||')'||' and dataplace_id = '||source_dataplace_id
          else string_agg('('||where_condition||')',' and ') over(partition by data_movement_id,source_dataplace_id,source_schema_id) || ' and schema_id = ' || source_schema_id || ' and table_type in ('||ingest_all_tables_views||')'||' and dataplace_id = '||source_dataplace_id end as where_condition,
ingest_all_tables_views,workflow_engine_name,workflow_name,workflow_engine_id
from(
select  data_movement_id,source_dataplace_id,source_schema_id,
string_agg('('||where_condition||')',' or ') over(partition by data_movement_id,source_dataplace_id,filter_type_id,source_schema_id) as where_condition,
ingest_all_tables_views,workflow_engine_name,workflow_name,workflow_engine_id
from (
select a.data_movement_id, a.source_dataplace_id, a.source_schema_id, a.filter_type_id,workflow_engine_name,workflow_name,
case when a.filter_type_id = 1 then 'table_name ~* '''||regexp_replace(a.filter_rule::text,'\[|\]|"'::text,''::text,'g')||''''
     when a.filter_type_id = 2 then 'table_name !~* '''||regexp_replace(a.filter_rule::text,'\[|\]|"'::text,''::text,'g')||''''
     when a.filter_type_id = 3 then 'table_name like ''%'||regexp_replace(a.filter_rule::text,'\[|\]|"'::text,''::text,'g')||'%'''
     when a.filter_type_id = 4 then 'table_name not like ''%'||regexp_replace(a.filter_rule::text,'\[|\]|"'::text,''::text,'g')||'%'''
     when a.filter_type_id = 5 then 'table_id in ('||regexp_replace(a.filter_rule::text,'\[|\]|"'::text,''::text,'g')||')'
     else null
     end as where_condition,workflow_engine_id,
     regexp_replace(regexp_replace(a.ingest_all_tables_views::text,'"','''','g'),'\[|\]|'::text,''::text,'g') as ingest_all_tables_views
from
(select b.data_movement_id,source_dataplace_id,
source_schema_directory_id as source_schema_id,destination_dataplace_id,destination_schema_directory_id as destination_schema_id,
filter_rule,filter_type_id
,ingest_all_tables_views,a.priority_order,c.workflow_engine_name,d.workflow_name,c.workflow_engine_id
from nabu.data_movement_details_physical a
inner join nabu.data_movement_physical b
on a.data_movement_id = b.data_movement_id
inner join nabu.workflow_engine c on c.workflow_engine_id = b.workflow_engine_id
inner join nabu.bot_configuration_workflow d on d.workflow_id = b.workflow_id
where b.data_movement_id = $templateData.input_data.data_movement_id$ and a.valid_to_ts = '9999-12-31'
and b.valid_to_ts = '9999-12-31') a
inner join nabu.filter_type_lookup b
on a.filter_type_id = b.filter_type_id
order by data_movement_id,priority_order
) x
) y
) z
group by data_movement_id,workflow_engine_name,workflow_name,workflow_engine_id
>>

fetching_destination_type(templateData)::=<<
select case when ('$templateData.input_data.metadata_category_destination$'='file') then true else false end as is_destination_file
>>

get_ingestion_information(templateData)::=<<
$(["get_ingestion_information_for_",templateData.input_data.metadata_category,"_type"])(templateData)$
>>

get_ingestion_information_for_relational_type(templateData)::=<<
with flow_tables as (
select table_id,schema_id,table_name,trim(schema_name) as schema_name,dataplace_id,estimated_rows
from nabu.dataplace_table_metadata_physical
where table_id in ($templateData.input_data.where_condition$)
and valid_to_ts = '9999-12-31'
),
data_movement_schema_drift_table_ids as(
select * from(select data_movement_id, dataplace_id, schema_directory_id, object_id, cru_ts, row_number() over(partition by object_id order by cru_ts desc) as rownum
from nabu.data_movement_schema_drift_details
where data_movement_id = $templateData.input_data.data_movement_id$
and object_id in ($templateData.input_data.where_condition$))a
where rownum = 1
),

get_non_schema_drifted_table_ids as(
select a.dataplace_id,a.schema_id, a.dataplace_component_type_id,a.table_id
from nabu.dataplace_table_schema_drift_details a
inner join
data_movement_schema_drift_table_ids b
on a.table_id = b.object_id
and a.schema_id = b.schema_directory_id
and a.dataplace_id = b.dataplace_id
where a.valid_to_ts = '9999-12-31'
and a.crt_ts < b.cru_ts
and b.data_movement_id = $templateData.input_data.data_movement_id$
and table_id in ($templateData.input_data.where_condition$)
union all
select a.dataplace_id,a.schema_id, a.dataplace_component_type_id,a.table_id
from nabu.dataplace_table_schema_drift_details a
inner join
nabu.structured_jobtotable b
on a.table_id = b.object_id
and a.dataplace_id = b.dataplace_id
where b.data_movement_id = $templateData.input_data.data_movement_id$
and a.valid_to_ts = '9999-12-31'
and table_id in ($templateData.input_data.where_condition$)
and b.cru_ts > a.crt_ts
and b.object_type = 'S'
union all
select a.dataplace_id,a.schema_id, a.dataplace_component_type_id,a.table_id
from nabu.dataplace_table_schema_drift_details a
where table_id in ($templateData.input_data.where_condition$)
and a.valid_to_ts = '9999-12-31'
and table_id not in (select object_id as table_id from nabu.structured_jobtotable b
where b.data_movement_id = $templateData.input_data.data_movement_id$ and b.object_type = 'S'
)
),

dataplace_schema_drifted_tables as(
select * from nabu.dataplace_table_schema_drift_details dtsdd where dataplace_id in
(select source_dataplace_id from nabu.data_movement_details_physical dmdp
where data_movement_id = $templateData.input_data.data_movement_id$
and valid_to_ts = '9999-12-31')
and dtsdd.valid_to_ts = '9999-12-31'
),

flag_for_schema_drifted_tables as(
select a.table_id, a.schema_id,a.table_name,a.schema_id,a.dataplace_id,case when b.table_id is null and $templateData.input_data.job_type_id$ != 6
and c.table_id is not null then true
else false
end as schema_drift_flag
from flow_tables a
left join
get_non_schema_drifted_table_ids b
on a.table_id = b.table_id
left join
dataplace_schema_drifted_tables c
on a.table_id = c.table_id
),

source_dataplace_details as (
select a.dataplace_id as source_dataplace_id,
case when  (b.dataplace_sub_component_type='mysql') then true else false end as is_mysql,
case when  (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') then ((dataplace_info->\>'jdbc_info')::json)->\>'jdbc_url'
else dataplace_info->\>'host_name' end as source_url,
case when (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')  then trim(((dataplace_info->\>'jdbc_info')::json)->\>'database_name')
else trim(dataplace_info->\>'database_name') end as source_db,
case when  (b.dataplace_sub_component_type='sas') then dataplace_info->\>'location_path' end as source_directory,
case when b.dataplace_sub_component_type = 'oracle' then dataplace_info->\>'jdbc_url' end as oracle_jdbc_url,
case when b.dataplace_sub_component_type = 'hive' or b.dataplace_sub_component_type = 'cloudera_hive' then
(dataplace_info->\>'jdbc_info')::json->\>'database_name'
else dataplace_info->\>'database_name' end as database_name,
case when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')) then true else false end as is_source_hive,
case when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') and dataplace_info->\>'connection_mode' = 'Cluster mode') then true else false end as is_source_spark_hive,
lower(case when b.dataplace_sub_component_type  = 'cloudera_hive' then 'hive' else b.dataplace_sub_component_type end) as source_type, b.additional_info->\>'jdbc_driver' as source_jdbc_driver,
a.credential_id ,dataplace_info,
dataplace_info->\>'project_id'  as project,
dataplace_info->\>'bucket_name'  as bucket_name,
additional_info->\>'default_credential' as source_default_credential_exists,
(dataplace_info->\>'filesystem_info')::JSON->\>'endpoint' as source_endpoint
from nabu.dataplace_physical a, nabu.dataplace_sub_component_lookup b
where a.dataplace_sub_component_id = b.dataplace_sub_component_id
and a.valid_to_ts = '9999-12-31'
and a.dataplace_id in (select dataplace_id from flow_tables)
  ),
source_credential_details as (
select a.*,b.credential_id as source_credential_id,b.credential_type_id as source_credential_type_id
from source_dataplace_details a left outer join nabu.credential_info b
on a.credential_id = b.credential_id
and b.valid_to_ts = '9999-12-31'
),
data_movement_details as (
select ingestion_table_format,valid_db_name,valid_schema_name,destination_dataplace_id ,destination_schema_directory_id ,
destination_file_format,source_dataplace_id, data_movement_id,verification_threshold, schema_drift_option, table_suffix, is_empty_suffix, table_timestamp from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value' || '_'
else '' end as ingestion_table_format,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'database_name')::boolean as valid_db_name,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'schema_name')::boolean as valid_schema_name,
coalesce((data_movement_additional_info->\>'flow_details')::json->\>'destination_file_format','parquet') as destination_file_format,
case
when
coalesce(((data_movement_additional_info->'flow_details'->\>'verification_details')::json->\>'skip_verification'),'false')::boolean = 'false' then COALESCE(((data_movement_additional_info->'flow_details'->\>'verification_details')::json->\>'verification_threshold')::float/100,0.1)
else 1
end as verification_threshold,
case when ((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'advanced_options_sub_type') is null then 'drop_create_table'
else ((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'advanced_options_sub_type') end as schema_drift_option,
((((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'config')::json)->\>'suffix') as table_suffix,
coalesce(TRIM((((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'config')::json)->\>'suffix'), '') = '' as is_empty_suffix,
((((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'config')::json)->\>'timestamp')::text as table_timestamp
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id  from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
where valid_to_ts = '9999-12-31'
)a
)
--select * from data_movement_details
,
source_details as (
select a.*,c.*
from flow_tables a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)
--select * from source_details
,
source_level_details as (
select *,
lower(trim(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace
((coalesce(ingestion_table_format,'') || coalesce(db_name,'') || coalesce(source_schema_name,'') || table_name),'\$','_dlr_' ) ,'|','_pe_' ),'}','_rbr_' ),'{','_lbr_' ),'#','_hash_' ) ,'*','_astk_' ) ,'(','_op_' ) , ')','_cp_' ) ,'@','_at_' ),'.','_dot_' ),' ','_' ),'&','_amp_' ),'+','_pls_' ),'-','_hyp_' ),'?','_qtn_' ),':','_cln_' ),'/','_fs_' ),'''','_sq_' ),'%','_prst_' ),'!','_exp_'))) as destination_table_name
--coalesce(ingestion_table_format,'') || coalesce(db_name,'') || coalesce(source_schema_name,'') || table_name as destination_table_name
from (
select b.*,a.*,
case when valid_db_name is true then database_name || '_' else '' end as db_name,
case when valid_schema_name is true then schema_name || '_' else '' end as source_schema_name
from source_credential_details a
inner join source_details b on a.source_dataplace_id = b.dataplace_id
)a
),
file_system_credentials as (
select c.credential_id,d.dataplace_id ,c.credential_name,c.credential_type_id, credential_type,extract(epoch from c.mod_ts) as credential_epoch from nabu.dataplace_physical d left outer join nabu.credential_info c
on (((dataplace_info->\>'filesystem_info')::json)->\>'credential_id')::integer = c.credential_id left join nabu.credential_type_lookup ct on c.credential_type_id = ct.credential_type_id
where d.dataplace_id in (select destination_dataplace_id from data_movement_details)
),
dest_dataplace_details as (
-- For the destination dataplace, get it's details
select a.dataplace_id as destination_dataplace_id,
case when  (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') then ((dataplace_info->\>'jdbc_info')::json)->\>'jdbc_url'
else dataplace_info->\>'host_name' end as destination_url,
case when b.dataplace_sub_component_type = 'oracle' then dataplace_info->\>'jdbc_url' end as destination_oracle_jdbc_url,
case
       when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')
        and dataplace_info->\>'connection_mode' = 'Cluster mode') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
       when (b.dataplace_sub_component_type= 'azure_synapse') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
	   when (b.dataplace_sub_component_type = 'bigquery') then dataplace_info->\>'ingestion_root_path'
       when (b.dataplace_sub_component_type = 'redshift') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
       when (b.dataplace_sub_component_type = 'snowflake') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
       when ((b.dataplace_sub_component_type = 'adls_gen2')) then ((dataplace_info->\>'ingestion_root_path'))
        else null
    end as path,
    case
       when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')
       and dataplace_info->\>'connection_mode' = 'Cluster mode') then ((dataplace_info->\>'filesystem_info')::json)->\>'endpoint'
        else null
    end as filesystem_endpoint,
    case
        when (b.dataplace_sub_component_type ='hive' or b.dataplace_sub_component_type='cloudera_hive')
        and dataplace_info->\>'connection_mode' = 'Cluster mode' then lower(((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type')
        when (b.dataplace_sub_component_type  = 'azure_synapse') then ((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type'
		when (b.dataplace_sub_component_type = 'bigquery') then 'gcs'
        when (b.dataplace_sub_component_type  = 'redshift' and '$templateData.input_data.intermediate_type$' = 'parquet') then ((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type'
        when (b.dataplace_sub_component_type = 'adls_gen2') then additional_info->\>'file_system_type'
        when (b.dataplace_sub_component_type  = 'snowflake') then ((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type'
        else null
    end as  file_system_type,
case
   when ((b.dataplace_sub_component_type = 'hive')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
   when ((b.dataplace_sub_component_type  = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
   when ((b.dataplace_sub_component_type = 'bigquery')) then  cast(a.credential_id as text)
   when ((b.dataplace_sub_component_type = 'redshift' and '$templateData.input_data.intermediate_type$' = 'parquet')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
   when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
   else null end as filesystem_credential_id,
case
   when ((b.dataplace_sub_component_type  = 'hive')) then cast(coalesce (fs.credential_type_id,'1') as text)
   when ((b.dataplace_sub_component_type = 'azure_synapse')) then cast(fs.credential_type_id as text)
   when ((b.dataplace_sub_component_type = 'bigquery')) then cast(fs.credential_type_id as text)
   when ((b.dataplace_sub_component_type = 'redshift' and '$templateData.input_data.intermediate_type$' = 'parquet')) then cast(fs.credential_type_id as text)
   when ((b.dataplace_sub_component_type = 'snowflake')) then cast(fs.credential_type_id as text)
   else null end as filesystem_credential_type_id,
case
   when ((b.dataplace_sub_component_type = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'directory_name'
   when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'directory_name'
   else null
end as adls_gen2_directory_name,
case
    when ((b.dataplace_sub_component_type = 'redshift' and '$templateData.input_data.intermediate_type$' = 'parquet')) then ((dataplace_info->\>'filesystem_info')::json)->\>'bucket'
    when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'bucket'
    else null
end as s3_bucket,
case
   when ((b.dataplace_sub_component_type = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'container'
   when ((b.dataplace_sub_component_type = 'adls_gen2')) then ((dataplace_info->\>'container'))
   when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'container'
   else null
end as adls_gen2_container,
case
   when ((b.dataplace_sub_component_type = 'bigquery')) then dataplace_info->\>'bucket_name'
   else null
end as gcs_bucket,
case
   when ((b.dataplace_sub_component_type = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'accountName'
   when ((b.dataplace_sub_component_type = 'adls_gen2')) then ((dataplace_info->\>'accountName'))
   when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'accountName'
   else null
end as storage_account,
case when (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')  then ((dataplace_info->\>'jdbc_info')::json)->\>'database_name'
else dataplace_info->\>'database_name' end as destination_db,
case when  (b.dataplace_sub_component_type='sas') then dataplace_info->\>'location_path' end as destination_directory,
case when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') and dataplace_info->\>'connection_mode' = 'Cluster mode') then true else false end as is_spark_hive,
lower(case when b.dataplace_sub_component_type  = 'cloudera_hive' then 'hive' else b.dataplace_sub_component_type end) as destination_type,
lower(case when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') and dataplace_info->\>'connection_mode' = 'Cluster mode') then (select coalesce ((data_movement_additional_info->\>'flow_details')::json->\>'destination_file_format','parquet') from nabu.data_movement_physical dmp where data_movement_id=$templateData.input_data.data_movement_id$) else b.dataplace_sub_component_type end) as target_type,
b.additional_info->\>'jdbc_driver' as destination_jdbc_driver,
b.dataplace_sub_component_id as destination_dataplace_sub_component_id,
a.credential_id as destination_credential_id,
additional_info->\>'default_credential' as dest_default_credential_exists,
(dataplace_info->\>'filesystem_info')::JSON->\>'endpoint' as destination_endpoint,
fs.credential_type as filesystem_credential_type,
fs.credential_epoch as filesystem_credential_epoch
from nabu.dataplace_physical a, nabu.dataplace_sub_component_lookup b, file_system_credentials fs
where a.dataplace_sub_component_id = b.dataplace_sub_component_id
and a.valid_to_ts = '9999-12-31'
and a.dataplace_id in (select destination_dataplace_id from data_movement_details)
 ),
dest_credential_details as (
select a.*,b.credential_type_id as destination_credential_type_id,
case when (a.file_system_type = 's3a') then true else false end as is_file_system_type_s3a,
case when (a.filesystem_credential_type_id = '2' or a.filesystem_credential_id is null) then true else false end as is_filesystem_credential_type_aws,
case when (b.credential_type_id = 8 or b.credential_type_id =9) then true else false end as is_destination_credential_kerberos,
case when ( b.credential_type_id =9) then true else false end as is_apache_jdbc_driver,
ct.credential_type as destination_credential_type
from dest_dataplace_details a left outer join nabu.credential_info b
on a.destination_credential_id = b.credential_id
left join nabu.credential_type_lookup ct on b.credential_type_id = ct.credential_type_id
and b.valid_to_ts = '9999-12-31'
)
$if(first(templateData.fetching_destination_type).is_destination_file)$
select extract(year from current_timestamp)||'/'||extract(month from current_timestamp)||'/'
       ||extract(day from current_timestamp)||'/'||extract(hour from current_timestamp)||'/'||extract(minute from current_timestamp)||'/'
       ||extract(second from current_timestamp)::bigint as destination_folder_format,
$first(templateData.fetching_destination_type).is_destination_file$ as is_destination_file,a.dataplace_id,a.schema_id,a.schema_name,a.table_id,
table_timestamp, table_suffix, h.schema_drift_flag,
case when schema_drift_option = 'new_table' and is_empty_suffix= false and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix,'_' ,to_char(current_timestamp ,table_timestamp))
          when schema_drift_option = 'new_table' and is_empty_suffix= false and table_timestamp = '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix)
          when schema_drift_option = 'new_table' and is_empty_suffix= true and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',to_char(current_timestamp ,table_timestamp))
          else destination_table_name end as destination_table_name,
     case when schema_drift_option = 'bkp_table' and is_empty_suffix= false and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix,'_' ,to_char(current_timestamp ,table_timestamp))
          when schema_drift_option = 'bkp_table' and is_empty_suffix= false and table_timestamp = '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix)
          when schema_drift_option = 'bkp_table' and is_empty_suffix= true and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',to_char(current_timestamp ,table_timestamp))
          else destination_table_name end as destination_backup_table_name,project,a.table_name,source_url,source_credential_id,source_credential_type_id,source_jdbc_driver,source_type,
estimated_rows,f.directory_name as destination_schema_name,a.destination_dataplace_id,destination_dataplace_sub_component_id,
 a.verification_threshold, destination_url,$templateData.input_data.process_id$ as process_id,
$templateData.input_data.batch_id$ as batch_id,
'$templateData.input_data.batch_name$' as batch_name,$templateData.input_data.data_movement_id$ as data_movement_id,
'$templateData.input_data.where_condition$' as where_condition,
path,bucket_name,project,filesystem_endpoint,
file_system_type,destination_oracle_jdbc_url,oracle_jdbc_url,dest_default_credential_exists::boolean,source_default_credential_exists::boolean,
is_spark_hive, destination_db,destination_directory, destination_type,target_type, destination_file_format,destination_jdbc_driver,$templateData.input_data.flow_number$ as flow_number,
source_directory, destination_credential_id, destination_credential_type_id,is_destination_credential_kerberos,is_apache_jdbc_driver,ingestion_table_format,source_db,is_source_hive,is_source_spark_hive,
case when schema_drift_option = 'bkp_table' and h.schema_drift_flag = true
then true END as create_backup_table, schema_drift_option,
e.filesystem_credential_id,e.filesystem_credential_type_id,e.filesystem_credential_type,e.is_filesystem_credential_type_aws,e.is_file_system_type_s3a,
e.adls_gen2_directory_name,e.adls_gen2_container,e.storage_account,e.s3_bucket,e.gcs_bucket,is_mysql,
'$templateData.input_data.jwt_token$' as jwt_token, '$templateData.input_data.end_point$' as end_point,
case when source_endpoint is null then '' else source_endpoint end as source_endpoint,
case when destination_endpoint is null then '' else destination_endpoint end as destination_endpoint,
case when (e.filesystem_credential_type = 'Azure Active Directory' or destination_credential_type ='Azure Active Directory') then true else false end as is_azure_oauth,
case when (e.filesystem_credential_type = 'AWS IAM Role' or destination_credential_type ='AWS IAM Role') then true else false end as is_aws_iam,
e.filesystem_credential_epoch,
upper(destination_table_name) as upper_case_destination_table_name
from source_level_details a,  dest_credential_details e, nabu.dataplace_file_system_component f,
flag_for_schema_drifted_tables h
where a.destination_dataplace_id = e.destination_dataplace_id
    and a.data_movement_id = $templateData.input_data.data_movement_id$
    and f.dataplace_id = e.destination_dataplace_id
    and a.destination_schema_directory_id = f.directory_id
    and f.valid_to_ts = '9999-12-31'
    and a.table_id = h.table_id
$else$
select $first(templateData.fetching_destination_type).is_destination_file$ as is_destination_file,a.dataplace_id,a.schema_id,a.schema_name,a.table_id,
table_timestamp, table_suffix, h.schema_drift_flag,
case when schema_drift_option = 'new_table' and is_empty_suffix= false and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix,'_' ,to_char(current_timestamp ,table_timestamp))
          when schema_drift_option = 'new_table' and is_empty_suffix= false and table_timestamp = '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix)
          when schema_drift_option = 'new_table' and is_empty_suffix= true and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',to_char(current_timestamp ,table_timestamp))
          else destination_table_name end as destination_table_name,
     case when schema_drift_option = 'bkp_table' and is_empty_suffix= false and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix,'_' ,to_char(current_timestamp ,table_timestamp))
          when schema_drift_option = 'bkp_table' and is_empty_suffix= false and table_timestamp = '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix)
          when schema_drift_option = 'bkp_table' and is_empty_suffix= true and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',to_char(current_timestamp ,table_timestamp))
          else destination_table_name end as destination_backup_table_name,project,a.table_name,source_url,source_credential_id,source_credential_type_id,source_jdbc_driver,source_type,
estimated_rows,f.schema_name as destination_schema_name,a.destination_dataplace_id,destination_dataplace_sub_component_id,
 a.verification_threshold, destination_url,$templateData.input_data.process_id$ as process_id,
$templateData.input_data.batch_id$ as batch_id,
'$templateData.input_data.batch_name$' as batch_name,$templateData.input_data.data_movement_id$ as data_movement_id,
'$templateData.input_data.where_condition$' as where_condition,
path,bucket_name,project,filesystem_endpoint,
file_system_type,destination_oracle_jdbc_url,oracle_jdbc_url,dest_default_credential_exists::boolean,source_default_credential_exists::boolean,
is_spark_hive, destination_db,destination_directory, destination_type,target_type, destination_file_format,destination_jdbc_driver,$templateData.input_data.flow_number$ as flow_number,
source_directory, destination_credential_id, destination_credential_type_id,is_destination_credential_kerberos,is_apache_jdbc_driver,ingestion_table_format,source_db,is_source_hive,is_source_spark_hive,
case when schema_drift_option = 'bkp_table' and h.schema_drift_flag = true
then true END as create_backup_table, schema_drift_option,
e.filesystem_credential_id,e.filesystem_credential_type_id,e.filesystem_credential_type,e.is_filesystem_credential_type_aws,e.is_file_system_type_s3a,
e.adls_gen2_directory_name,e.adls_gen2_container,e.storage_account,e.s3_bucket,e.gcs_bucket,is_mysql,
'$templateData.input_data.jwt_token$' as jwt_token, '$templateData.input_data.end_point$' as end_point,
case when source_endpoint is null then '' else source_endpoint end as source_endpoint,
case when destination_endpoint is null then '' else destination_endpoint end as destination_endpoint,
case when (e.filesystem_credential_type = 'Azure Active Directory' or destination_credential_type ='Azure Active Directory') then true else false end as is_azure_oauth,
case when (e.filesystem_credential_type = 'AWS IAM Role' or destination_credential_type ='AWS IAM Role') then true else false end as is_aws_iam,
e.filesystem_credential_epoch
from source_level_details a, dest_credential_details e, nabu.dataplace_relational_component_physical f,
flag_for_schema_drifted_tables h
where a.destination_dataplace_id = e.destination_dataplace_id
    and a.data_movement_id = $templateData.input_data.data_movement_id$
    and f.dataplace_id = e.destination_dataplace_id
    and a.destination_schema_directory_id = f.schema_id
    and f.valid_to_ts = '9999-12-31'
    and a.table_id = h.table_id
$endif$

>>

get_ingestion_information_for_file_type(templateData)::=<<
with flow_files as(
select file_id,directory_id,file_name,fm.dataplace_id,file_size,
case when file_format like 'csv%' then 'csv'
     when file_format like 'tsv%' then 'csv' else file_format
end as file_format,
additional_info ->\> 'hasHeader' as has_header,
regexp_replace(additional_info ->\> 'delimiter',E'\t','\\\t') as delimiter,
case
	when file_format like 'csv%' then additional_info ->\> 'quoteEscapeChar'
	when file_format like 'tsv%' then additional_info ->\> 'escapeChar'
end as escape_char,
file_relative_path,file_absolute_path
from (
select file_id,directory_id,file_name,dataplace_id,size as file_size,file_format,
additional_info,
file_relative_path,file_absolute_path from nabu.dataplace_file_metadata
where file_id in ($templateData.input_data.where_condition$)
and valid_to_ts = '9999-12-31') fm
),
data_movement_schema_drift_file_ids as(
select * from(select data_movement_id, dataplace_id, schema_directory_id, object_id, cru_ts, row_number() over(partition by object_id order by cru_ts desc) as rownum
from nabu.data_movement_schema_drift_details
where data_movement_id = $templateData.input_data.data_movement_id$
and object_id in ($templateData.input_data.where_condition$))a
where rownum = 1
),

get_non_schema_drifted_file_ids as(
select a.dataplace_id,a.directory_id, a.dataplace_component_type_id,a.file_id
from nabu.semi_structured_file_schema_drift a
inner join
data_movement_schema_drift_file_ids b
on a.file_id = b.object_id
and a.directory_id = b.schema_directory_id
and a.dataplace_id = b.dataplace_id
and a.cru_ts < b.cru_ts
and b.data_movement_id = $templateData.input_data.data_movement_id$
and file_id in ($templateData.input_data.where_condition$)
union all
select a.dataplace_id,a.directory_id, a.dataplace_component_type_id,a.file_id
from nabu.semi_structured_file_schema_drift a
inner join
nabu.structured_jobtotable b
on a.file_id = b.object_id
and a.dataplace_id = b.dataplace_id
where b.data_movement_id = $templateData.input_data.data_movement_id$
and file_id in ($templateData.input_data.where_condition$)
and b.cru_ts > a.cru_ts
and b.object_type = 'S'
union all
select a.dataplace_id,a.directory_id, a.dataplace_component_type_id,a.file_id
from nabu.semi_structured_file_schema_drift a
where file_id in ($templateData.input_data.where_condition$)
and file_id not in (select object_id as file_id from nabu.structured_jobtotable b
where b.data_movement_id = $templateData.input_data.data_movement_id$ and b.object_type = 'S'
)
),

dataplace_schema_drifted_files as(
select * from nabu.semi_structured_file_schema_drift where dataplace_id in
(select source_dataplace_id from nabu.data_movement_details_physical dmdp
where data_movement_id = $templateData.input_data.data_movement_id$
and valid_to_ts = '9999-12-31')
),

flag_for_schema_drifted_files as(
select a.file_id, a.directory_id,a.file_name,a.dataplace_id,case when b.file_id is null and $templateData.input_data.job_type_id$ != 6
and c.file_id is not null then true
else false
end as schema_drift_flag
from flow_files a
left join
get_non_schema_drifted_file_ids b
on a.file_id = b.file_id
left join
dataplace_schema_drifted_files c
on a.file_id = c.file_id
),

source_dataplace_details as (
select a.dataplace_id as source_dataplace_id,
case when (b.dataplace_sub_component_type!='unix' )  then dataplace_info->\>'host_name'else null end as host_name,
case when (b.dataplace_sub_component_type='smb' )  then dataplace_info->\>'share_name' else null end as share_name,
case when (b.dataplace_sub_component_type='smb' )  then dataplace_info->\>'domain' else null end as domain,
case when (b.dataplace_sub_component_type='unix' )  then dataplace_info->\>'path'  else null end as root_location_path,
case when (b.dataplace_sub_component_type = 's3') then dataplace_info->\>'bucket'  else null end as source_s3_bucket,
case when (b.dataplace_sub_component_type = 'adls_gen2') then dataplace_info->\> 'container' else null end as source_adls_gen2_container,
case when (b.dataplace_sub_component_type = 'adls_gen2') then dataplace_info->\> 'accountName' else null end as source_storage_account,
lower(b.dataplace_sub_component_type)  as source_type,
dataplace_info->\>'project_id'  as project,
dataplace_info->\>'bucket_name'  as bucket_name,
a.credential_id ,dataplace_info,
(dataplace_info->\>'filesystem_info')::JSON->\>'endpoint' as source_endpoint
from nabu.dataplace_physical a inner join nabu.dataplace_sub_component_lookup b
on a.dataplace_sub_component_id = b.dataplace_sub_component_id
left outer join nabu.credential_info c
on a.credential_id = c.credential_id
inner join flow_files g
on a.dataplace_id = g.dataplace_id
and a.valid_to_ts = '9999-12-31'
)
--select * from source_dataplace_details;
,
source_credential_details as (
select a.*,b.credential_id as source_credential_id,b.credential_type_id as source_credential_type_id,ct.credential_type as source_credential_type
from source_dataplace_details a left outer join nabu.credential_info b
on a.credential_id = b.credential_id
left join nabu.credential_type_lookup ct on b.credential_type_id = ct.credential_type_id
and b.valid_to_ts = '9999-12-31'
)
--select * from source_dataplace_details;
,
data_movement_details as (
select ingestion_table_format,destination_dataplace_id ,destination_schema_directory_id ,
destination_file_format,source_dataplace_id, data_movement_id,verification_threshold, schema_drift_option, table_suffix, is_empty_suffix, table_timestamp
from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value' || '_'
else '' end as ingestion_table_format,
coalesce((data_movement_additional_info->\>'flow_details')::json->\>'destination_file_format','parquet') as destination_file_format,
case
when
coalesce(((data_movement_additional_info->'flow_details'->\>'verification_details')::json->\>'skip_verification'),'false')::boolean = 'false' then COALESCE(((data_movement_additional_info->'flow_details'->\>'verification_details')::json->\>'verification_threshold')::float/100,0.1)
else 1
end as verification_threshold,
case when ((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'advanced_options_sub_type') is null then 'drop_create_table'
else ((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'advanced_options_sub_type') end as schema_drift_option,
((((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'config')::json)->\>'suffix') as table_suffix,
coalesce(TRIM((((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'config')::json)->\>'suffix'), '') = '' as is_empty_suffix,
((((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'config')::json)->\>'timestamp')::text as table_timestamp
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id  from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
where valid_to_ts = '9999-12-31'
)a
)
--select * from data_movement_details
,
source_details as (
select a.*,c.*
from flow_files a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)
--select * from source_details
,
source_level_details as (
select *,
lower(trim(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace
((a.file_name),'\$','_dlr_' ) ,'|','_pe_' ),'}','_rbr_' ),'{','_lbr_' ),'#','_hash_' ) ,'*','_astk_' ) ,'(','_op_' ) , ')','_cp_' ) ,'@','_at_' ),'.','_dot_' ),' ','_' ),'&','_amp_' ),'+','_pls_' ),'-','_hyp_' ),'?','_qtn_' ),':','_cln_' ),'/','_fs_' ),'''','_sq_' ),'%','_prst_' ),'!',
'_exp_'))) as destination_table_name
--regexp_replace(concat('t_',a.file_name),'\..*\$','') as destination_table_name
--coalesce(ingestion_table_format,'') || file_name as destination_table_name
from (
select b.*,a.*
from source_credential_details a
inner join source_details b on a.source_dataplace_id = b.dataplace_id
)a
),
file_system_credentials as (
select c.credential_id,d.dataplace_id ,c.credential_name,c.credential_type_id, credential_type,extract(epoch from c.mod_ts) as credential_epoch from nabu.dataplace_physical d left outer join nabu.credential_info c
on (((dataplace_info->\>'filesystem_info')::json)->\>'credential_id')::integer = c.credential_id
left outer join nabu.credential_type_lookup ct
on c.credential_type_id = ct.credential_type_id
where d.dataplace_id in (select destination_dataplace_id from data_movement_details)
),
dest_dataplace_details as (
-- For the destination dataplace, get it's details
select $templateData.input_data.data_movement_id$ as data_movement_id,a.dataplace_id as destination_dataplace_id,
case when  (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') then ((dataplace_info->\>'jdbc_info')::json)->\>'jdbc_url'
else dataplace_info->\>'host_name' end as destination_url,
(select schema_name from nabu.dataplace_relational_component_physical drcp where schema_id = (select destination_schema_directory_id from data_movement_details) and valid_to_ts = '9999-12-31') as destination_schema_name,
case when b.dataplace_sub_component_type = 'oracle' then dataplace_info->\>'jdbc_url' end as destination_oracle_jdbc_url,
case
       when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')
        and dataplace_info->\>'connection_mode' = 'Cluster mode') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
       when (b.dataplace_sub_component_type= 'azure_synapse') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
       when (b.dataplace_sub_component_type = 'redshift') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
       when (b.dataplace_sub_component_type = 'snowflake') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
        else null
    end as path,
case
       when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')
       and dataplace_info->\>'connection_mode' = 'Cluster mode') then ((dataplace_info->\>'filesystem_info')::json)->\>'endpoint'
        else null
      end as filesystem_endpoint,
case
        when (b.dataplace_sub_component_type ='hive' or b.dataplace_sub_component_type='cloudera_hive')
        and dataplace_info->\>'connection_mode' = 'Cluster mode' then lower(((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type')
        when (b.dataplace_sub_component_type  = 'azure_synapse') then ((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type'
        when (b.dataplace_sub_component_type  = 'redshift' and '$templateData.input_data.intermediate_type$' = 'parquet') then ((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type'
        when (b.dataplace_sub_component_type  = 'snowflake') then ((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type'
        else null
    end as  file_system_type,
case
   when ((b.dataplace_sub_component_type = 'hive')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
   when ((b.dataplace_sub_component_type  = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
   when ((b.dataplace_sub_component_type = 'redshift' and '$templateData.input_data.intermediate_type$' = 'parquet')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
   when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
   else null end as filesystem_credential_id,
case
   when ((b.dataplace_sub_component_type  = 'hive')) then cast(coalesce (fs.credential_type_id,'1') as text)
   when ((b.dataplace_sub_component_type = 'azure_synapse')) then cast(fs.credential_type_id as text)
   when ((b.dataplace_sub_component_type = 'redshift' and '$templateData.input_data.intermediate_type$' = 'parquet')) then cast(fs.credential_type_id as text)
   when ((b.dataplace_sub_component_type = 'snowflake')) then cast(fs.credential_type_id as text)
   else null end as filesystem_credential_type_id,
case
   when ((b.dataplace_sub_component_type = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'directory_name'
   when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'directory_name'
   else null
end as adls_gen2_directory_name,
case
    when ((b.dataplace_sub_component_type = 'redshift' and '$templateData.input_data.intermediate_type$' = 'parquet')) then ((dataplace_info->\>'filesystem_info')::json)->\>'bucket'
    when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'bucket'
    else null
end as s3_bucket,
case
   when ((b.dataplace_sub_component_type = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'container'
   when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'container'
   else null
end as adls_gen2_container,
case
   when ((b.dataplace_sub_component_type = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'accountName'
   when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'accountName'
   else null
end as storage_account,
case when (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')  then ((dataplace_info->\>'jdbc_info')::json)->\>'database_name'
else dataplace_info->\>'database_name' end as destination_db,
dataplace_info->'filesystem_info'->\>'directory' as hdfs_directory,
case when  (b.dataplace_sub_component_type='sas') then dataplace_info->\>'location_path' end as destination_directory,
case when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') and dataplace_info->\>'connection_mode' = 'Cluster mode') then true else false end as is_spark_hive,
lower(case when b.dataplace_sub_component_type  = 'cloudera_hive' then 'hive' else b.dataplace_sub_component_type end) as destination_type,
lower(case when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') and dataplace_info->\>'connection_mode' = 'Cluster mode') then (select coalesce ((data_movement_additional_info->\>'flow_details')::json->\>'destination_file_format','parquet') from nabu.data_movement_physical dmp where data_movement_id=$templateData.input_data.data_movement_id$) else b.dataplace_sub_component_type end) as target_type,
b.additional_info->\>'jdbc_driver' as destination_jdbc_driver,
b.dataplace_sub_component_id as destination_dataplace_sub_component_id,
a.credential_id as destination_credential_id,
additional_info->\>'default_credential' as dest_default_credential_exists,
(dataplace_info->\>'filesystem_info')::JSON->\>'endpoint' as destination_endpoint,
fs.credential_type as filesystem_credential_type,
fs.credential_epoch as filesystem_credential_epoch
from nabu.dataplace_physical a, nabu.dataplace_sub_component_lookup b, file_system_credentials fs
where a.dataplace_sub_component_id = b.dataplace_sub_component_id
and a.dataplace_id = fs.dataplace_id
and a.valid_to_ts = '9999-12-31'
and a.dataplace_id in (select destination_dataplace_id from data_movement_details)
)
--select * from dest_dataplace_details;
,
dest_credential_details as (
select a.*,b.credential_type_id as destination_credential_type_id,
case when (a.file_system_type = 's3a') then true else false end as is_file_system_type_s3a,
case when (a.filesystem_credential_type_id = '2' or a.filesystem_credential_id is null) then true else false end as is_filesystem_credential_type_aws,
case when (b.credential_type_id = 8 or b.credential_type_id =9) then true else false end as is_destination_credential_kerberos,
case when ( b.credential_type_id =9) then true else false end as is_apache_jdbc_driver,
ct.credential_type as destination_credential_type
from dest_dataplace_details a left outer join nabu.credential_info b
on a.destination_credential_id = b.credential_id
left join nabu.credential_type_lookup ct on b.credential_type_id = ct.credential_type_id
and b.valid_to_ts = '9999-12-31'
)
--select * from dest_credential_details;
,
datatypes_to_be_ignored as (
 select replace((json_array_elements(data_movement_additional_info->'flow_details'->'ignore_data_types'))::text,'"','') as ignore_data_types
 from nabu.data_movement_physical
 where data_movement_id = $templateData.input_data.data_movement_id$ and valid_to_ts='9999-12-31'
 union
 select data_type as ignore_data_types
 from nabu.advanced_options_unsupported_mappings
 where advanced_options_sub_type_id =16 and data_movement_id = $templateData.input_data.data_movement_id$ and valid_to_ts='9999-12-31'
),
column_metadata_details as (
-- get the Column Metadata details for each file aggregate into a JSON
select b.dataplace_id , b.file_id , b.directory_id , '{'||string_agg( concat('"',column_name,'"',':','"',data_type::text,'"'),','  order by ordinal_position) ||'}'as column_details
from  nabu.dataplace_semi_structured_column_metadata b inner join flow_files a
    on a.file_id = b.file_id
    and a.dataplace_id = b.dataplace_id
    and a.directory_id = b.directory_id
    and b.valid_to_ts = '9999-12-31'
    left outer join datatypes_to_be_ignored c
 on b.data_type = c.ignore_data_types
 where c.ignore_data_types is null
group by  b.dataplace_id , b.file_id , b.directory_id
),
directory_name_details as(
select directory_name from nabu.dataplace_file_system_component a,column_metadata_details b
where a.dataplace_id=b.dataplace_id and a.valid_to_ts='9999-12-31 00:00:00'
)
select a.dataplace_id as source_dataplace_id,a.directory_id,a.file_id,a.file_format,a.has_header,a.delimiter,a.escape_char,
a.file_relative_path,a.file_absolute_path,a.source_s3_bucket,a.source_credential_id,a.source_credential_type_id,
table_timestamp, table_suffix, h.schema_drift_flag,a.source_adls_gen2_container,a.source_storage_account,i.directory_name,
case when schema_drift_option = 'new_table' and is_empty_suffix= false and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix,'_' ,to_char(current_timestamp ,table_timestamp))
          when schema_drift_option = 'new_table' and is_empty_suffix= false and table_timestamp = '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix)
          when schema_drift_option = 'new_table' and is_empty_suffix= true and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',to_char(current_timestamp ,table_timestamp))
          else destination_table_name end as destination_table_name,
     case when schema_drift_option = 'bkp_table' and is_empty_suffix= false and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix,'_' ,to_char(current_timestamp ,table_timestamp))
          when schema_drift_option = 'bkp_table' and is_empty_suffix= false and table_timestamp = '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix)
          when schema_drift_option = 'bkp_table' and is_empty_suffix= true and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',to_char(current_timestamp ,table_timestamp))
          else destination_table_name end as destination_backup_table_name,project,a.file_name,source_type,
file_size,f.schema_name as destination_schema_name,a.destination_dataplace_id,destination_dataplace_sub_component_id,
column_details, a.verification_threshold, destination_url,$templateData.input_data.process_id$ as process_id,
$templateData.input_data.batch_id$ as batch_id,
'$templateData.input_data.batch_name$' as batch_name,$templateData.input_data.data_movement_id$ as data_movement_id,
'$templateData.input_data.where_condition$' as where_condition,
path,bucket_name,project,filesystem_endpoint,
case when ('$templateData.input_data.intermediate_type$' in ('default')) then true else false end as is_default,
file_system_type,destination_oracle_jdbc_url,dest_default_credential_exists::boolean,
is_spark_hive, destination_db,destination_directory, destination_type,target_type, destination_file_format,destination_jdbc_driver,$templateData.input_data.flow_number$ as flow_number,
destination_credential_id, destination_credential_type_id,is_destination_credential_kerberos,is_apache_jdbc_driver,ingestion_table_format,
case when schema_drift_option = 'bkp_table' and h.schema_drift_flag = true
then true END as create_backup_table, schema_drift_option,
e.filesystem_credential_id,e.filesystem_credential_type_id,e.filesystem_credential_type,e.is_filesystem_credential_type_aws,e.is_file_system_type_s3a,
e.adls_gen2_directory_name,e.adls_gen2_container,e.storage_account,e.s3_bucket,
'$templateData.input_data.jwt_token$' as jwt_token, '$templateData.input_data.end_point$' as end_point,
case when source_endpoint is null then '' else source_endpoint end as source_endpoint,
case when destination_endpoint is null then '' else destination_endpoint end as destination_endpoint,
case when (e.filesystem_credential_type = 'Azure Active Directory' or destination_credential_type ='Azure Active Directory') then true else false end as is_azure_oauth,
case when (e.filesystem_credential_type = 'AWS IAM Role' or destination_credential_type ='AWS IAM Role') then true else false end as is_aws_iam,
case when (source_credential_type ='Azure Active Directory') then true else false end as is_source_azure_oauth,
e.filesystem_credential_epoch
from source_level_details a, column_metadata_details b, dest_credential_details e, nabu.dataplace_relational_component_physical f,
flag_for_schema_drifted_files h,directory_name_details i
where a.file_id = b.file_id
    and a.dataplace_id = b.dataplace_id
    and a.directory_id = b.directory_id
    and a.destination_dataplace_id = e.destination_dataplace_id
    and a.data_movement_id = $templateData.input_data.data_movement_id$
    and f.dataplace_id = e.destination_dataplace_id
    and a.destination_schema_directory_id = f.schema_id
    and f.valid_to_ts = '9999-12-31'
    and a.file_id = h.file_id
>>

command_to_run_almaren_ingestion_job_old(templateData)::=<<
["sh","-l","-c","\$NABU_ROOT_INSTALLATION/botworks/scripts/ingestion/run_almaren_job.sh","$templateData.data.query_input.encrypted_script$","$templateData.data.query_input.base64_rsa_compressed_encrypted_creds$","$templateData.data.query_input.input_json_for_checkpoint_status$","$templateData.retry_attempt$"]
>>

command_to_run_spark_almaren_job(templateData)::=<<
["bash","-l","-c","\$NABU_ROOT_INSTALLATION/botworks/scripts/curation/spark-almaren.sh $templateData.ssh_username$ $templateData.ssh_host$ $templateData.project_name$ $templateData.git_url$ $templateData.git_branch_or_tag$ $templateData.git_file_path$ $templateData.spark_command$ $templateData.process_id$ $templateData.engine_path$ $templateData.retry_attempt$ $templateData.spark_location$ $templateData.compute_engine_info$ $templateData.job_name$"]
>>

command_to_run_cleanup_job(templateData)::=<<
["sh","-l","-c","\$NABU_ROOT_INSTALLATION/botworks/scripts/curation/run_cleanup.sh $templateData.ssh_username$ $templateData.ssh_host$ $templateData.project_name$ $templateData.engine_path$ $templateData.process_id$ $templateData.spark_location$ $templateData.compute_engine_info$"]
>>

command_to_run_almaren_ingestion_if_schema_drift_old(templateData)::=<<
["sh","-l","-c","\$NABU_ROOT_INSTALLATION/botworks/scripts/ingestion/run_almaren_job.sh $templateData.data.templateData.query_input.encrypted_script$ $templateData.data.templateData.query_input.base64_rsa_compressed_encrypted_creds$ $templateData.data.templateData.query_input.input_json_for_checkpoint_status$ $templateData.retry_attempt$"]
>>

command_to_run_almaren_ingestion_job(templateData)::=<<
["sh","../scripts/ingestion/run_almaren_job.sh","$templateData.data.query_input.encrypted_script$","$templateData.data.query_input.base64_rsa_compressed_encrypted_creds$","$templateData.data.query_input.input_json_for_checkpoint_status$","$templateData.retry_attempt$","$templateData.ssh_host$","$templateData.ssh_username$","$templateData.engine_path$"]
>>

command_to_run_almaren_ingestion_if_schema_drift(templateData)::=<<
["sh","-l","-c","\$NABU_ROOT_INSTALLATION/botworks/scripts/ingestion/run_almaren_job.sh $templateData.data.templateData.query_input.encrypted_script$ $templateData.data.templateData.query_input.base64_rsa_compressed_encrypted_creds$ $templateData.data.templateData.query_input.input_json_for_checkpoint_status$ $templateData.retry_attempt$ $templateData.ssh_host$ $templateData.ssh_username$ $templateData.engine_path$ $templateData.spark_location$ $templateData.compute_engine_info$ $templateData.source_endpoint$ $templateData.destination_endpoint$"]
>>

generate_ddl_and_drop_backup_table_if_exists(templateData)::=<<
drop table if exists $templateData.destination_schema_name$.`$templateData.destination_backup_table_name$`
>>

generate_ddl_and_create_backup_table(templateData)::=<<
create table $templateData.destination_schema_name$.`$templateData.destination_backup_table_name$` as select * from $templateData.destination_schema_name$.`$templateData.destination_table_name$`
>>

generate_ddl_and_drop_backup_table_if_exists_redshift(templateData)::=<<
drop table if exists $templateData.destination_backup_table_name$
>>

generate_ddl_and_create_backup_table_redshift(templateData)::=<<
create table $templateData.destination_backup_table_name$ as select * from $templateData.destination_schema_name$.$templateData.destination_table_name$
>>

generate_ddl_drop_and_create_backup_table(templateData)::=<<
IF EXISTS(select t.name, s.name from sys.tables as t inner join sys.schemas as s
on t.schema_id = s.schema_id
where s.name='$templateData.destination_schema_name$' and t.name='$templateData.destination_backup_table_name$')
BEGIN
drop table $templateData.destination_schema_name$.$templateData.destination_backup_table_name$;
create table $templateData.destination_schema_name$.$templateData.destination_backup_table_name$ with(HEAP, DISTRIBUTION = ROUND_ROBIN)
as select * from $templateData.destination_schema_name$.$templateData.destination_table_name$;
END
ELSE
BEGIN
create table $templateData.destination_schema_name$.$templateData.destination_backup_table_name$ with(HEAP, DISTRIBUTION = ROUND_ROBIN)
as select * from $templateData.destination_schema_name$.$templateData.destination_table_name$;
END
>>


insert_spark_job_result(templateData)::=<<
INSERT INTO nabu.spark_job_result
(process_id, application_id, error_details, status,exit_status)
VALUES($templateData.query_input.process_id$,
$if(templateData.query_input.data.query_input.metadata.botLogicOutputMap.applicationId)$'$templateData.query_input.data.query_input.metadata.botLogicOutputMap.applicationId$'$elseif(templateData.query_input.data.botLogicOutputMap.applicationId)$'$templateData.query_input.data.botLogicOutputMap.applicationId$'$else$null$endif$,
$if(templateData.query_input.data.query_input.metadata.botLogicOutputMap.error_details)$'$templateData.query_input.data.query_input.metadata.botLogicOutputMap.error_details$'$elseif(templateData.query_input.data.botLogicOutputMap.error_details)$'$templateData.query_input.data.botLogicOutputMap.error_details$'$else$null$endif$,
$if(templateData.query_input.data.query_input.metadata.botLogicOutputMap.status)$'$templateData.query_input.data.query_input.metadata.botLogicOutputMap.status$'$elseif(templateData.query_input.data.botLogicOutputMap.status)$'$templateData.query_input.data.botLogicOutputMap.status$'$else$null$endif$,
$if(templateData.query_input.data.query_input.metadata.botLogicOutputMap.exit_status)$'$templateData.query_input.data.query_input.metadata.botLogicOutputMap.exit_status$'$elseif(templateData.query_input.data.botLogicOutputMap.exit_status)$'$templateData.query_input.data.botLogicOutputMap.exit_status$'$else$null$endif$
);
>>

insert_into_spark_job_result(templateData)::=<<
INSERT INTO nabu.spark_job_result
(process_id, application_id, error_details, status,exit_status)
VALUES($templateData.input_data.process_id$,$if(templateData.input_data.applicationId)$'$templateData.input_data.applicationId$'$else$null$endif$,$if(templateData.input_data.error_details)$'$templateData.input_data.error_details$'$else$null$endif$, $templateData.input_data.status$,
$templateData.input_data.exit_status$);
>>

insert_curation_spark_script_bot_status(templateData)::=<<
INSERT INTO nabu.checkpoint_status
( data_movement_id, process_id, status, application_id, job_type, hostname,  start_time,valid_from_ts, valid_to_ts,table_id)
VALUES( $templateData.data.input_data.data_movement_id$,$templateData.process_id$,'$templateData.status$','$templateData.applicationId$','nabu-curation-job','$templateData.ssh_host$',now(),now(),'9999-12-31',$templateData.data.input_data.where_condition$);

>>

update_curation_spark_script_bot_status(templateData)::=<<
update nabu.checkpoint_status
set
status='$templateData.status$',end_time=now()
where data_movement_id=$templateData.data.input_data.data_movement_id$
and process_id=$templateData.process_id$

>>


update_checkpoint_after_latest_status_insert(templateData)::=<<
update nabu.checkpoint_status set valid_to_ts=now()
where data_movement_id=$templateData.data.input_data.data_movement_id$
and valid_to_ts='9999-12-31' and
job_type='nabu-curation-job' and
data_movement_id=$templateData.data.input_data.data_movement_id$ and
application_id <> '$templateData.applicationId$'
>>

insert_spark_application_details(templateData)::=<<
insert into nabu.spark_application_details
(application_id, process_id, additional_info, cru_ts)
values($if(templateData.applicationId)$'$templateData.applicationId$'$else$null$endif$,$templateData.process_id$,'{"job_id":$if(templateData.jobId)$"$templateData.jobId$"$else$ null $endif$ ,"job_name":$if(templateData.jobName)$"$templateData.jobName$"$else$ null $endif$ ,"run_id":$if(templateData.runId)$"$templateData.runId$"$else$ null $endif$}',now())
>>

number_of_tables_per_flow(templateData)::=<<
1
>>


get_ingestion_information_for_first_flow(templateData)::=<<
with flow_tables as (
-- Get all the tables that are part of a particular flow. This can return a batch of tables for a given flow.
-- 'Flow' is a task that is part of overall Ingestion Job. The FlowControllerBOT throttles and controls the overall Ingestion process
-- as series of 'Flow'. A given number of 'Flow' are executed concurrently and then once a Flow is completed, the FlowControllerBOT start the next Flow.
select table_id,schema_id,table_name,trim(schema_name) as schema_name,dataplace_id,
case when estimated_rows < 0 then 0 else estimated_rows end as estimated_rows
from nabu.dataplace_table_metadata_physical
where table_id in ($templateData.input_data.where_condition$)
and valid_to_ts = '9999-12-31'
 ),
source_dataplace_details as (
-- For the source dataplace, get it's details
select a.dataplace_id as source_dataplace_id, case when  (dataplace_info->\>'connection_type'='hive' or dataplace_info->\>'connection_type'='cloudera_hive') then ((dataplace_info->\>'jdbc_info')::json)->\>'jdbc_url'
else dataplace_info->\>'host_name' end as source_url,
case when (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')  then trim(((dataplace_info->\>'jdbc_info')::json)->\>'database_name')
else trim(dataplace_info->\>'database_name') end as source_db,
case when  (dataplace_info->\>'connection_type'='sas') then dataplace_info->\>'location_path' end as source_directory,
case when ((dataplace_info->\>'connection_type'='hive' or dataplace_info->\>'connection_type'='cloudera_hive') and dataplace_info->\>'connection_mode' = 'Cluster mode') then true else false end as is_source_spark_hive,
lower(case when b.dataplace_sub_component_type  = 'cloudera_hive' then 'hive' else b.dataplace_sub_component_type end) as source_type,
b.additional_info->\>'jdbc_driver' as source_jdbc_driver,
case when dataplace_info->\>'connection_type' = 'hive' or dataplace_info->\>'connection_type' = 'cloudera_hive' then
(dataplace_info->\>'jdbc_info')::json->\>'database_name'
else dataplace_info->\>'database_name' end as database_name,
c.credential_id as source_credential_id,
c.credential_type_id as source_credential_type_id
from nabu.dataplace_physical a, nabu.dataplace_sub_component_lookup b , nabu.credential_info c
where a.dataplace_sub_component_id = b.dataplace_sub_component_id
and a.valid_to_ts = '9999-12-31'
and a.dataplace_id in (select dataplace_id from flow_tables)
and a.credential_id = c.credential_id and c.valid_to_ts = '9999-12-31'
),
data_movement_details as (
select ingestion_table_format,valid_db_name,valid_schema_name,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value' || '_'
else '' end as ingestion_table_format,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'database_name')::boolean as valid_db_name,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'schema_name')::boolean as valid_schema_name
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id  from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
where valid_to_ts = '9999-12-31'
)a
)
--select * from data_movement_details
,
source_details as (
select a.*,c.*
from flow_tables a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)
--select * from source_details
,
source_level_details as (
select *,coalesce(ingestion_table_format,'') || coalesce(db_name,'') || coalesce(source_schema_name,'') || table_name as destination_table_name
from (
select b.*,a.*,
case when valid_db_name is true then database_name || '_' else '' end as db_name,
case when valid_schema_name is true then schema_name || '_' else '' end as source_schema_name
from source_dataplace_details a
inner join source_details b on a.source_dataplace_id = b.dataplace_id
)a
),
dest_dataplace_details as (
-- For the destination dataplace, get it's details
select a.dataplace_id as destination_dataplace_id, case when  (dataplace_info->\>'connection_type'='hive' or dataplace_info->\>'connection_type'='cloudera_hive') then ((dataplace_info->\>'jdbc_info')::json)->\>'jdbc_url'
else dataplace_info->\>'host_name' end as destination_url,
    case
        when ((dataplace_info->\>'connection_type'='hive'
        or dataplace_info->\>'connection_type'='cloudera_hive')
        and dataplace_info->\>'connection_mode' = 'Cluster mode') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
        else null
    end as path,
    case
        when ((dataplace_info->\>'connection_type'='hive'
        or dataplace_info->\>'connection_type'='cloudera_hive')
        and dataplace_info->\>'connection_mode' = 'Cluster mode') then lower(((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type')
        else null
    end as  file_system_type,
	case
	    when  ((dataplace_info->\>'connection_type'='hive'
		or dataplace_info->\>'connection_type'='cloudera_hive')
		and dataplace_info->\>'connection_mode'='Cluster mode') then ((dataplace_info->\>'filesystem_info')::json)->\>'endpoint' else null
    end as file_end_point,'file' as destination_type,
case when (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')  then ((dataplace_info->\>'jdbc_info')::json)->\>'database_name'
else dataplace_info->\>'database_name' end as destination_db,
case when  (dataplace_info->\>'connection_type'='sas') then dataplace_info->\>'location_path' end as destination_directory,
case when ((dataplace_info->\>'connection_type'='hive' or dataplace_info->\>'connection_type'='cloudera_hive') and dataplace_info->\>'connection_mode' = 'Cluster mode') then true else false end as is_spark_hive, b.additional_info->\>'jdbc_driver' as destination_jdbc_driver,
b.dataplace_sub_component_id as destination_dataplace_sub_component_id,
c.credential_id as destination_credential_id,
c.credential_type_id as destination_credential_type_id
from nabu.dataplace_physical a, nabu.dataplace_sub_component_lookup b , nabu.credential_info c
where a.dataplace_sub_component_id = b.dataplace_sub_component_id
and a.valid_to_ts = '9999-12-31'
and a.dataplace_id in (select destination_dataplace_id from data_movement_details)
and (((dataplace_info->\>'filesystem_info')::json)->\>'credential_id') ::int = c.credential_id and c.valid_to_ts = '9999-12-31' ),
column_metadata_details as (
-- get the Column Metadata details for each table aggregate into a JSON
select b.dataplace_id , b.table_id , b.schema_id , '{'||string_agg( concat('"',column_name,'"',':','"',data_type::text,'"'),','  order by ordinal_position) ||'}'as column_details
from  nabu.dataplace_column_metadata_physical b inner join flow_tables a
    on a.table_id = b.table_id
    and a.dataplace_id = b.dataplace_id
    and a.schema_id = b.schema_id
    and b.valid_to_ts = '9999-12-31'
group by  b.dataplace_id , b.table_id , b.schema_id )
select a.dataplace_id,a.schema_id,a.schema_name,a.table_id,
$replacing_special_characters("destination_table_name")$ as destination_table_name,
table_name,source_url,source_credential_id,source_credential_type_id,source_jdbc_driver,source_type,estimated_rows,f.schema_name as destination_schema_name,a.destination_dataplace_id,destination_dataplace_sub_component_id,
column_details,'1' as verification_threshold, destination_url,$templateData.input_data.process_id$ as process_id,$templateData.input_data.batch_id$ as batch_id,
'$templateData.input_data.batch_name$' as batch_name,$templateData.input_data.data_movement_id$ as data_movement_id,
'$templateData.input_data.where_condition$' as where_condition,
path,
file_system_type,file_end_point,
is_spark_hive, destination_db,destination_directory, destination_type, destination_jdbc_driver,$templateData.input_data.flow_number$ as flow_number,source_directory, destination_credential_id, destination_credential_type_id,ingestion_table_format,source_db,is_source_spark_hive
from source_level_details a, column_metadata_details b,  dest_dataplace_details e, nabu.dataplace_relational_component_physical f
where a.table_id = b.table_id
    and a.dataplace_id = b.dataplace_id
    and a.schema_id = b.schema_id
    and a.destination_dataplace_id = e.destination_dataplace_id
    and a.data_movement_id = $templateData.input_data.data_movement_id$
    and f.dataplace_id = e.destination_dataplace_id
    and a.destination_schema_directory_id = f.schema_id
    and f.valid_to_ts = '9999-12-31'
>>


replacing_special_characters(table_or_column_name)::=<<
lower(trim(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace($table_or_column_name$,'\$','_dlr_' ) ,'|','_pe_' ),'}','_rbr_' ),'{','_lbr_' ),'#','_hash_' ) ,'*','_astk_' ) ,'(','_op_' ) , ')','_cp_' ) ,'@','_at_' ),'.','_dot_' ),' ','_' ),'&','_amp_' ),'+','_pls_' ),'-','_hyp_' ),'?','_qtn_' ),':','_cln_' ),'/','_fs_' ),'''','_sq_' ),'%','_prst_' ),'!','_exp_')))
>>

replacing_special_characters_file(file_name)::=<<
lower(trim(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(a.file_name,'\$','_dlr_' ) ,'|','_pe_' ),'}','_rbr_' ),'{','_lbr_' ),'#','_hash_' ) ,'*','_astk_' ) ,'(','_op_' ) , ')','_cp_' ) ,'@','_at_' ),'.','_dot_' ),' ','_' ),'&','_amp_' ),'+','_pls_' ),'-','_hyp_' ),'?','_qtn_' ),':','_cln_' ),'/','_fs_' ),'''','_sq_' ),'%','_prst_' ),'!','_exp_')))
>>


verification_of_file_ingestion_and_getting_source_details(templateData)::=<<
{
  "input_data": $generate_required_input_for_verification(templateData.query_input)$,
   "sequential_templates":[
        {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "verification_of_file_ingestion",
        "query_output_key": "verification_of_file_ingestion",
        "query_type":"select"
      },
      {
        "query_template_group": "ingestion_templates|artifact_ingestion",
        "query_template_name": "generate_federated_ingestion",
        "query_output_key": "federated_ingestion_json",
        "query_type":"json",
        "encryption" : "Yes"
      }

    ],
  "output_keys":["federated_ingestion_json","verification_of_file_ingestion"]
}
>>

generate_required_input_for_verification(templateData)::=<<
{
"data_movement_id" : $templateData.data.data_movement_id$,
"flow_number" : $templateData.data.flow_number$,
"where_condition" : "$templateData.data.where_condition$",
"process_id" : $templateData.process_id$
}
>>

fetch_object_column_metadata(templateData)::=<<
$([templateData.input_data.metadata_category,"_type_column_metadata"])(templateData)$
>>

file_type_column_metadata(templateData)::=<<
with datatype_mapping as (
  	select
  		lower(source_type) as source_type ,
  		lower(intermediate_type) as intermediate_type,
  		lower(destination_type) as destination_type ,
  		lower(source_datatype_name) as source_datatype_name ,
  		source_stg_function ,intermediate_stg_function ,destination_datatype_name,
  		lower(datamovement_engine_type) as datamovement_engine_type
  	from nabu.datatype_mapping where lower(source_type) =lower('$first(templateData.table_metadata_output).file_format$') and lower(destination_type)=lower('$templateData.input_data.destination$') and
  	lower(intermediate_type) = lower('$templateData.input_data.intermediate_type$')
 ),
  column_metadata as (
  	select
  		column_name,data_length,ordinal_position,data_scale,data_precision,
  		is_nullable,column_id,file_id ,file_name,
  		lower(trim(replace(replace(replace( replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(column_name,'\$','_dlr_' ) ,'#','_hash_' ),'|','_pe_' ),'}','_rbr_' ),'{','_lbr_' ) ,'*','_astk_' ) ,'(','_op_' ) , ')','_cp_' ) ,'@','_at_' ),'.','_dot_' ),' ','_' ),'&','_amp_' ),'+','_pls_' ),'-','_hyp_' ),'?','_qtn_' ),':','_cln_' ),'/','_fs_' ),'''','_sq_' ),'%','_prst_' ))) as field_name,
  		lower(data_type) as data_type
  	from nabu.dataplace_semi_structured_column_metadata where file_id = $first(templateData.table_metadata_output).file_id$ AND valid_to_ts='9999-12-31'
  )
  select
  	a.column_name,a.data_type,a.data_length,a.ordinal_position,a.is_nullable,file_name,
  	source_type,b.source_datatype_name,source_stg_function,destination_datatype_name,
  	a.data_scale,a.data_precision,
  	case
        		when ( intermediate_stg_function = 'AsIs') then 'parquet_AsIs'
             	else intermediate_stg_function
        end as intermediate_stg_function,
  	a.field_name,
      	case when ( 'oracle' = lower('$templateData.input_data.source$')) and
          length(a.field_name)> 30
          then
          'c_'||a.column_id
          else
          a.field_name
          end
          as intermediate_field_name
  from column_metadata as a left join datatype_mapping as b
  on a.data_type = b.source_datatype_name
  where a.ordinal_position is not null
  and b.datamovement_engine_type = lower('$templateData.input_data.datamovement_engine_type$')
  order by ordinal_position
>>

relational_type_column_metadata(templateData)::=<<
with datatype_mapping as (
  	$if(templateData.input_data.is_create_table)$
  	select
  		lower(source_type) as source_type ,
  		lower(intermediate_type) as intermediate_type ,
  		$templateData.input_data.is_create_table$ as is_create_table,
  		lower(destination_type) as destination_type ,
  		lower(source_datatype_name) as source_datatype_name ,
  		intermediate_datatype_name , destination_datatype_name ,source_cast_type,
  		intermediate_cast_type ,source_stg_function ,intermediate_stg_function ,
  		lower(datamovement_engine_type) as datamovement_engine_type
  	from nabu.datatype_mapping where lower(source_type) =lower('$templateData.input_data.source$') and lower(destination_type)=lower('$templateData.input_data.destination$') and
  	lower(intermediate_type) = lower('$templateData.input_data.intermediate_type$')

  	$else$
  		select
      		lower(source_type) as source_type ,
      		lower(intermediate_type) as intermediate_type ,
      		$templateData.input_data.is_create_table$ as is_create_table,
            lower(destination_type) as destination_type ,
      		lower(source_datatype_name) as source_datatype_name ,
      		intermediate_datatype_name ,source_cast_type,
      		intermediate_cast_type ,source_stg_function ,intermediate_stg_function ,
      		lower(datamovement_engine_type) as datamovement_engine_type
      	from(
         select *,row_number() over (partition by source_type ,source_datatype_name ) as rno
         from nabu.datatype_mapping dmt
      	 where lower(source_type) =lower('$templateData.input_data.source$') and
      	 lower(intermediate_type) = lower('$templateData.input_data.intermediate_type$')
      	 )a
      	 where rno=1
    $endif$

  ),
  ignore_datatypes as(
       select coalesce (lower(ignore_data_types),' ') as data_types from (
       select replace((json_array_elements(data_movement_additional_info->'flow_details'->'ignore_data_types'))::text,'"','') as ignore_data_types
       from nabu.data_movement_physical
       where data_movement_id = $templateData.input_data.data_movement_id$ and valid_to_ts='9999-12-31'
       union
      select source_datatype_name as ignore_data_types
      from nabu.user_defined_trans_for_unsupported_datatype
      where advanced_options_sub_type_id =16 and data_movement_id = $templateData.input_data.data_movement_id$ and valid_to_ts='9999-12-31'
       )a
  ),
  column_metadata as (
  select a.*,
          b.advanced_options_sub_type_id,
          b.additional_info
   from
    (
  	select
  		cmp.column_name,data_length,cmp.ordinal_position,data_scale,data_precision,
  		data_type as metadata_datatype,
  		cmp.is_nullable,cmp.column_id,cmp.table_id,cmp.table_name,cmp.schema_name,
  		case
  		    when ('oracle' = lower('$templateData.input_data.source$')) and length(lower(trim(replace(replace(replace(replace(replace( replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(column_name,'\$','_dlr_' ) ,'|','_pe_' ),'}','_rbr_' ),'{','_lbr_' ) ,'#','_hash_' ) ,'*','_astk_' ) ,'(','_op_' ) , ')','_cp_' ) ,'@','_at_' ),'.','_dot_' ),' ','_' ),'&','_amp_' ),'+','_pls_' ),'-','_hyp_' ),'?','_qtn_' ),':','_cln_' ),'/','_fs_' ),'''','_sq_' ),'%','_prst_' ),'^','_cap_'),'!','_exm_')))) > 30
  		    then 'c_'||column_id
  		    else
  		    lower(trim(replace(replace(replace(replace(replace( replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(column_name,'\$','_dlr_' ) ,'|','_pe_' ),'}','_rbr_' ),'{','_lbr_' ),'#','_hash_' ) ,'*','_astk_' ) ,'(','_op_' ) , ')','_cp_' ) ,'@','_at_' ),'.','_dot_' ),' ','_' ),'&','_amp_' ),'+','_pls_' ),'-','_hyp_' ),'?','_qtn_' ),':','_cln_' ),'/','_fs_' ),'''','_sq_' ),'%','_prst_' ),'^','_cap_'),'!','_exm_')))
  		    end as field_name,
  		case
  			when data_type ~* 'INTERVAL' then regexp_replace(lower(data_type),'[^A-Za-z ]', '', 'g')
  			when data_type like 'array%' and dp.dataplace_sub_component_id =7  then regexp_replace(lower(data_type), '^(\w+)\W.*','\1','g')
            when data_type like 'struct%' and dp.dataplace_sub_component_id =7 then regexp_replace(lower(data_type), '^(\w+)\W.*','\1','g')
            when data_type like 'map%' and dp.dataplace_sub_component_id =7 then regexp_replace(lower(data_type), '^(\w+)\W.*','\1','g')
  			else lower(data_type)
  		end as data_type
  	from nabu.dataplace_column_metadata_physical cmp left outer join nabu.dataplace_physical dp on cmp.dataplace_id = dp.dataplace_id where cmp.table_id = $templateData.input_data.where_condition$  and cmp.valid_to_ts='9999-12-31'
  	and lower(data_type) not in (select lower(data_types) from ignore_datatypes)
    ) a
    left outer join
         (
         select data_movement_id,
                 lower(source_datatype_name) AS source_datatype,
                 advanced_options_sub_type_id,
                 additional_info
          from nabu.user_defined_trans_for_unsupported_datatype
          where data_movement_id = $templateData.input_data.data_movement_id$
            and valid_to_ts='9999-12-31'
        ) b on a.data_type = b.source_datatype
   ),
  initial_column_mapping as (
  select
  	a.column_name,a.data_type,a.data_length,a.ordinal_position,a.is_nullable,table_name,schema_name , a.metadata_datatype,
  	source_type,destination_type,intermediate_type,datamovement_engine_type ,b.source_datatype_name,source_cast_type,b.intermediate_datatype_name,
  	$([templateData.input_data.source,"_",templateData.input_data.intermediate_type,"_",templateData.input_data.destination,"_source_stg_function"])()$ as source_stg_function,
  	case when (a.advanced_options_sub_type_id = 17) then a.additional_info ->\> 'target_value' else null end as user_text,
  	a.data_scale,a.data_precision,
  	$if(templateData.input_data.is_create_table)$
  	$([templateData.input_data.source,"_",templateData.input_data.intermediate_type,"_",templateData.input_data.destination,"_destination_datatype_name"])()$ as destination_datatype_name,
  	$([templateData.input_data.source,"_",templateData.input_data.intermediate_type,"_",templateData.input_data.destination,"_intermediate_stg_function"])()$ as intermediate_stg_function,
	$([templateData.input_data.source,"_",templateData.input_data.intermediate_type,"_",templateData.input_data.destination,"_inconsistent_datatype"])()$ as inconsistent_datatype,
  	$else$$endif$
  	a.field_name,
      	case when ( 'oracle' = lower('$templateData.input_data.source$')) and
          length(a.field_name)> 30
          then
          'c_'||a.column_id
          else
          a.field_name
        end as intermediate_field_name
  from column_metadata as a left join datatype_mapping as b
  on a.data_type = b.source_datatype_name
  where a.ordinal_position is not null
 -- and b.datamovement_engine_type = lower('$templateData.input_data.datamovement_engine_type$')
  )
  select a.column_name,a.data_type,a.data_length,a.ordinal_position,a.is_nullable,table_name,schema_name ,$first(templateData.table_metadata_output).is_mysql$ as is_mysql,$first(templateData.table_metadata_output).is_source_hive$ as is_source_hive,
	a.source_type,a.source_datatype_name,source_cast_type,a.intermediate_datatype_name,
	coalesce(c.source_stg_function,a.source_stg_function) as source_stg_function,
	a.data_scale,a.data_precision,
	$if(templateData.input_data.is_create_table)$
	coalesce(c.destination_datatype_name,a.destination_datatype_name) as destination_datatype_name,
	case when coalesce(c.intermediate_stg_function,a.intermediate_stg_function) = 'AsIs' then lower('$templateData.input_data.intermediate_type$')||'_AsIs' else coalesce(c.intermediate_stg_function,a.intermediate_stg_function) end as intermediate_stg_function,
	a.inconsistent_datatype,
	$else$$endif$
	a.field_name,
	case
		when ('$templateData.input_data.source$' = 'oracle') then
        (
            case
                when data_type ~* 'time zone' and c.option_number = 0 then true
                else false
            end
        )
		when ('$templateData.input_data.source$' = 'postgres') then
        (
        	case
                when data_type ~* 'timestamp with time zone' and c.option_number = 0 then true
                else false
            end
        )
        when ('$templateData.input_data.source$' = 'sql_server') then
        (
        	case
                when data_type = 'datetimeoffset' and c.option_number = 0 then true
                else false
            end
        )
        else false
	end as isTimeZone , user_text
  from initial_column_mapping a left outer join (select a.*,lower(b.engine_type) as datamovement_engine_type from nabu.user_defined_trans_for_inconsistent_datatype a inner join nabu.engine_lookup b on a.engine_id = b.engine_id where data_movement_id = $templateData.input_data.data_movement_id$ and valid_to='9999-12-31' ) c
  on a.source_type = c.source_type
  and a.intermediate_type = c.intermediate_type
  and a.destination_type = c.destination_type
  and a.datamovement_engine_type = c.datamovement_engine_type
  $if(templateData.input_data.is_create_table)$
  and a.inconsistent_datatype = c.inconsistent_datatype
  $else$$endif$
  order by ordinal_position
>>

hive_parquet_hive_inconsistent_datatype() ::=<<
case
	when (b.source_datatype_name = 'date') then 'date_to_string'
	else null
end
>>

oracle_parquet_hive_inconsistent_datatype() ::=<<
case
	when (b.source_datatype_name in ('number','decimal')) then
	(
		case
			when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
			else 'number_to_string'
		end
	)
	when (b.source_datatype_name = 'float') then
	(
		case
			when (a.data_precision <=53) then null
			else 'float_to_string'
		end
	)
	when (b.source_datatype_name ~* 'INTERVAL') then 'interval_to_string'
	when (b.source_datatype_name in ('rowid','urowid','xmltype','float'))then concat(b.source_datatype_name,'_to_string')
	when (b.source_datatype_name ~* 'LOCAL TIME ZONE') then
	(
		case
			when (a.data_scale <= 6) then 'timestamp_with_local_time_zone_to_timestamp'
			else 'timestamp_with_local_time_zone_to_string'
		end
	)
	when (b.source_datatype_name ~* 'TIME ZONE') then
	(
		case
			when (a.data_scale <= 6) then 'timestamp_with_time_zone_to_timestamp'
			else 'timestamp_with_time_zone_to_string'
		end
	)
	when (b.source_datatype_name ~* 'TIMESTAMP') then
	(
		case
			when (a.data_scale <= 6) then null
			else 'timestamp_to_string'
		end
	)
	else null
end
>>

oracle_parquet_azure_synapse_inconsistent_datatype() ::=<<
case
    when (data_type in ('number','decimal')) then
    (
        case
            when (data_scale <= 37 and (data_precision <= 38)) then null
            else 'number_to_varchar'
        end
    )
    when (data_type ~* 'interval year') then 'interval_year_to_month_to_varchar'
    when (data_type ~* 'interval day') then 'interval_day_to_second_to_varchar'
    when (data_type in ('rowid','urowid','xmltype','binary_float','binary_double','date'))then concat(data_type,'_to_varchar')
    when (data_type = 'float') then
    (
        case
            when (data_precision <=53) then null
            else 'float_to_varchar'
        end
    )
    when (data_type ~* 'local time zone') then 'timestamp_with_local_time_zone_to_varchar'
    when (data_type ~* 'time zone') then 'timestamp_with_time_zone_to_varchar'
    when (data_type ~* 'timestamp') then 'timestamp_to_varchar'
    else null
end
>>

sql_server_parquet_hive_inconsistent_datatype() ::=<<
case
	when (b.source_datatype_name in ('time','date','bit')) then concat(b.source_datatype_name,'_to_string')
	when (b.source_datatype_name in ('datetime2')) then
	(
		case
			when (a.data_precision <= 6) then null
			else 'datetime2_to_string'
		end
	)
	when (b.source_datatype_name = 'datetimeoffset') then
	(
		case
			when (a.data_precision <= 6) then 'datetimeoffset_to_timestamp'
			else 'datetimeoffset_to_string'
		end
	)
	else null
end
>>

sql_server_parquet_azure_synapse_inconsistent_datatype() ::=<<
case
	when (data_type in ('time','date','bit','datetime2','datetimeoffset','datetime','smalldatetime')) then concat(data_type,'_to_varchar')
    else null
end
>>

postgres_parquet_hive_inconsistent_datatype() ::=<<
case
	when (b.source_datatype_name = 'numeric') then
    (
		case
			when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
            else 'numeric_to_string'
        end
    )
    when (b.source_datatype_name = 'bit varying') then 'bit_varying_to_string'
    when (b.source_datatype_name = 'time without time zone') then 'time_without_time_zone_to_string'
    when (b.source_datatype_name = 'time with time zone') then 'time_with_time_zone_to_string'
    when (b.source_datatype_name = 'timestamp with time zone') then 'timestamp_with_time_zone_to_string'
    when (b.source_datatype_name in ('money','bit','date')) then concat(b.source_datatype_name,'_to_string')
    else null
end
>>

postgres_parquet_azure_synapse_inconsistent_datatype() ::=<<
case
	when (data_type = 'numeric') then
    (
		case
            when (data_scale <= 37 and (data_precision <= 38)) then null
            else 'numeric_to_varchar'
        end
    )
	when (data_type in ('bit varying','bit')) then 'bit_to_varchar'
    when (data_type = 'time without time zone') then 'time_without_time_zone_to_varchar'
    when (data_type = 'time with time zone') then 'time_with_time_zone_to_varchar'
    when (data_type = 'timestamp with time zone') then 'timestamp_with_time_zone_to_varchar'
    when (data_type = 'timestamp without time zone') then 'timestamp_without_time_zone_to_varchar'
    when (data_type in ('money','date','boolean','abstime','real')) then concat(data_type,'_to_varchar')
	when (data_type = 'double precision') then 'double_to_varchar'
    else null
end
>>

mysql_parquet_hive_inconsistent_datatype() ::=<<
case
    when (b.source_datatype_name = 'decimal') then
    (
        case
            when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
            else 'decimal_to_string'
        end
    )
    when (b.source_datatype_name in ('year','time','date','bit'))then concat(b.source_datatype_name,'_to_string')
    else null
end
>>

mysql_parquet_azure_synapse_inconsistent_datatype() ::=<<
case
    when (data_type in ('decimal')) then
    (
    	case
            when (data_scale <= 37 and (data_precision <= 38)) then null
            else 'decimal_to_varchar'
        end
    )
    when (data_type in ('year','date','bit','time','timestamp','datetime')) then concat(data_type,'_to_varchar')
    else null
end
>>

oracle_parquet_hive_intermediate_stg_function()::=<<
case
	when (b.source_datatype_name = 'number') then
	(
		CASE
			when (data_scale=0 and (data_precision between 0 and 2)) then 'ParquetStringToTinyIntConvert'
            when (data_scale=0 and (data_precision between 3 and 4)) then 'ParquetStringToSmallIntConvert'
            when (data_scale=0 and (data_precision between 5 and 9)) then 'ParquetStringToIntConvert'
            when (data_scale=0 and (data_precision between 10 and 18)) then 'ParquetStringToBigIntConvert'
            when (data_scale=0 and (data_precision between 18 and 38)) then 'ParquetStringToDecimalConvert'
            when (data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function else 'AsIs'
        END
	)
	when (b.source_datatype_name = 'float') then
    (
  	    CASE
   	   	  	when (a.data_precision between 0 and 23) then 'ParquetStringToFloatConvert'
   	   	  	when (a.data_precision between 24 and 53) then 'ParquetStringToDoubleConvert'
   	   	  	else 'AsIs'
   	   	END
   	)
   	when(a.advanced_options_sub_type_id in (14,15,17)) then 'AsIs'
	else b.intermediate_stg_function
end
>>

oracle_parquet_hive_destination_datatype_name()::=<<
case
   when (b.source_datatype_name = 'number') then
   (
      CASE
         when (a.data_scale = 0 and (a.data_precision between 0 and 2)) then 'TINYINT'
         when (a.data_scale = 0 and (a.data_precision between 3 and 4)) then 'SMALLINT'
         when (a.data_scale = 0 and (a.data_precision between 5 and 9)) then 'INT'
         when (a.data_scale = 0 and (a.data_precision between 10 and 18)) then 'BIGINT'
         when (a.data_scale = 0 and (a.data_precision between 18 and 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         else 'STRING'
      END
   )
   when (b.source_datatype_name = 'float') then
   (
   	  case
   	   	 when (a.data_precision between 0 and 23) then 'FLOAT'
   	   	 when (a.data_precision between 24 and 53) then 'DOUBLE'
   	   	 else 'STRING'
   	  END
   )
   when(b.source_datatype_name = 'varchar2' or b.source_datatype_name = 'nvarchar2') then
   (
      CASE
         when (a.data_length between 1 and 65535) then concat(b.destination_datatype_name,'(',a.data_length,')')
         else 'STRING'
      END
   )
   when(b.source_datatype_name = 'char' or b.source_datatype_name = 'nchar') then
   (
      CASE
         when (a.data_length between 1 and 255) then concat(b.destination_datatype_name,'(',a.data_length,')')
         else 'STRING'
      END
   )
   when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
   else b.destination_datatype_name
end
>>

oracle_default_postgres_intermediate_stg_function()::=<<
case
	when (b.source_datatype_name = 'number') then
	(
		CASE
            when (data_scale=0 and (data_precision between 0 and 5)) then 'ParquetStringToSmallIntConvert'
            when (data_scale=0 and (data_precision between 6 and 10)) then 'ParquetStringToIntConvert'
            when (data_scale=0 and (data_precision between 11 and 19)) then 'ParquetStringToBigIntConvert'
            when (data_scale=0 and (data_precision between 20 and 38)) then 'ParquetStringToDecimalConvert'
            when (data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function else 'AsIs'
        END
	)
	else b.intermediate_stg_function
end
>>

oracle_default_postgres_destination_datatype_name()::=<<
case
   when (b.source_datatype_name = 'number') then
   (
      CASE
         when (a.data_scale = 0 and (a.data_precision between 0 and 5)) then 'SMALLINT'
         when (a.data_scale = 0 and (a.data_precision between 6 and 10)) then 'INT'
         when (a.data_scale = 0 and (a.data_precision between 11 and 19)) then 'BIGINT'
         when (a.data_scale = 0 and (a.data_precision between 20 and 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         else 'text'
      END
   )

   when(b.source_datatype_name = 'varchar2' or b.source_datatype_name = 'nvarchar2') then
   (
      CASE
         when (a.data_length between 1 and 65535) then concat(b.destination_datatype_name,'(',a.data_length,')')
         else 'text'
      END
   )
   when(b.source_datatype_name = 'char' or b.source_datatype_name = 'nchar') then
   (
      CASE
         when (a.data_length between 1 and 255) then concat(b.destination_datatype_name,'(',a.data_length,')')
         else 'text'
      END
   )
   when(a.advanced_options_sub_type_id in (14,15,17)) then 'text'
   else b.destination_datatype_name
end
>>

mysql_default_postgres_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
		CASE
			when (a.data_length between 1 and 255) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when (b.source_datatype_name='varchar') then
    (
		CASE
			when (a.data_length between 1 and 65535) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when (b.source_datatype_name='decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AsIs'
    else b.intermediate_stg_function
END
>>

mysql_default_postgres_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
		CASE
			when (a.data_length between 1 and 255) then concat('CHAR','(',a.data_length,')')
			else 'text'
		END
    )
    when (b.source_datatype_name = 'varchar') then
    (
		CASE
			when (a.data_length between 1 and 65535) then concat('VARCHAR','(',a.data_length,')')
			else 'text'
		END
    )
    when (b.source_datatype_name = 'bit') then concat('VARCHAR','(',a.data_precision,')')
    when (b.source_datatype_name = 'decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then concat('DECIMAL','(',a.data_precision,',',a.data_scale,')')
			else 'text'
		END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.destination_datatype_name
END
>>

mysql_default_redshift_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
		CASE
			when (a.data_length between 1 and 255) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when (b.source_datatype_name='varchar') then
    (
		CASE
			when (a.data_length between 1 and 65535) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when (b.source_datatype_name='decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AsIs'
    else b.intermediate_stg_function
END
>>

mysql_default_redshift_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
		CASE
			when (a.data_length between 1 and 255) then concat('CHAR','(',a.data_length,')')
			else 'varchar(max)'
		END
    )
    when (b.source_datatype_name = 'varchar') then
    (
		CASE
			when (a.data_length between 1 and 65535) then concat('VARCHAR','(',a.data_length,')')
			else 'varchar(max)'
		END
    )
    when (b.source_datatype_name = 'bit') then concat('VARCHAR','(',a.data_precision,')')
    when (b.source_datatype_name = 'decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then concat('DECIMAL','(',a.data_precision,',',a.data_scale,')')
			else 'varchar(max)'
		END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.destination_datatype_name
END
>>

sql_server_default_postgres_intermediate_stg_function()::=<<
case
	when (b.source_datatype_name = 'decimal') then
	(
		CASE
			 when (data_scale=0 and (data_precision between 0 and 5)) then 'ParquetStringToSmallIntConvert'
            when (data_scale=0 and (data_precision between 6 and 10)) then 'ParquetStringToIntConvert'
            when (data_scale=0 and (data_precision between 11 and 19)) then 'ParquetStringToBigIntConvert'
            when (data_scale=0 and (data_precision between 20 and 38)) then 'ParquetStringToDecimalConvert'
            when (data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function else 'AsIs'
		END
	)
	when (b.source_datatype_name = 'numeric') then
	(
		CASE
			 when (data_scale=0 and (data_precision between 0 and 5)) then 'ParquetStringToSmallIntConvert'
            when (data_scale=0 and (data_precision between 6 and 10)) then 'ParquetStringToIntConvert'
            when (data_scale=0 and (data_precision between 11 and 19)) then 'ParquetStringToBigIntConvert'
            when (data_scale=0 and (data_precision between 20 and 38)) then 'ParquetStringToDecimalConvert'
            when (data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function else 'AsIs'
		END
	)
	when (b.source_datatype_name = 'datetime2') then
	(
		CASE
			when data_precision <6 then b.intermediate_stg_function
			else 'AsIs'
		END
	)
	when(a.advanced_options_sub_type_id in (14,15,17)) then 'AsIs'
	else b.intermediate_stg_function
END
>>


sql_server_default_postgres_destination_datatype_name()::=<<
CASE
	when (b.source_datatype_name = 'char') then
	(
		CASE
			when (a.data_length <= 255) then concat('char','(', a.data_length,')')
			when (a.data_length between 256 and 8000) then concat('varchar','(', a.data_length,')')
			else 'text'
		END
	)
	when (b.source_datatype_name = 'varchar') then
	(
		CASE
			when (a.data_length = -1) then 'text'
			else concat('varchar','(', a.data_length,')')
		END
	)
	when (b.source_datatype_name = 'nchar') then
	(
		CASE
			when (a.data_length <= 255) then concat('char', '(', a.data_length, ')')
			when (a.data_length <= 4000) then concat('varchar', '(', a.data_length, ')')
			else 'text'
		END
	)
	when (b.source_datatype_name = 'nvarchar') then
	(
		CASE
			when (a.data_length = -1) then 'text'
			when (a.data_length <= 255) then concat('char', '(', a.data_length, ')')
			else concat('varchar', '(', a.data_length, ')')
		END
	)
	when (b.source_datatype_name = 'decimal') then
	(
		CASE
			when (a.data_scale = 0 and (a.data_precision between 0 and 5)) then 'SMALLINT'
         when (a.data_scale = 0 and (a.data_precision between 6 and 10)) then 'INT'
         when (a.data_scale = 0 and (a.data_precision between 11 and 19)) then 'BIGINT'
         when (a.data_scale = 0 and (a.data_precision between 20 and 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         else 'text'
		END
	)
	when (b.source_datatype_name = 'numeric') then
	(
		CASE
			when (a.data_scale = 0 and (a.data_precision between 0 and 5)) then 'SMALLINT'
         when (a.data_scale = 0 and (a.data_precision between 6 and 10)) then 'INT'
         when (a.data_scale = 0 and (a.data_precision between 11 and 19)) then 'BIGINT'
         when (a.data_scale = 0 and (a.data_precision between 20 and 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         else 'text'
		END
	)
	when (b.source_datatype_name = 'datetime2') then
	(
		CASE
			when data_precision <14 then b.destination_datatype_name
			else 'text'
		END
	)
	when (b.source_datatype_name = 'smallmoney' or b.source_datatype_name = 'money') then
    (
        concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
	else b.destination_datatype_name
END
>>

sql_server_default_redshift_intermediate_stg_function()::=<<
case
	when (b.source_datatype_name = 'decimal') then
	(
		CASE
			 when (data_scale=0 and (data_precision between 0 and 5)) then 'ParquetStringToSmallIntConvert'
            when (data_scale=0 and (data_precision between 6 and 10)) then 'ParquetStringToIntConvert'
            when (data_scale=0 and (data_precision between 11 and 19)) then 'ParquetStringToBigIntConvert'
            when (data_scale=0 and (data_precision between 20 and 38)) then 'ParquetStringToDecimalConvert'
            when (data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function else 'AsIs'
		END
	)
	when (b.source_datatype_name = 'numeric') then
	(
		CASE
			 when (data_scale=0 and (data_precision between 0 and 5)) then 'ParquetStringToSmallIntConvert'
            when (data_scale=0 and (data_precision between 6 and 10)) then 'ParquetStringToIntConvert'
            when (data_scale=0 and (data_precision between 11 and 19)) then 'ParquetStringToBigIntConvert'
            when (data_scale=0 and (data_precision between 20 and 38)) then 'ParquetStringToDecimalConvert'
            when (data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function else 'AsIs'
		END
	)
	when (b.source_datatype_name = 'datetime2') then
	(
		CASE
			when data_precision <6 then b.intermediate_stg_function
			else 'AsIs'
		END
	)
	when(a.advanced_options_sub_type_id in (14,15,17)) then 'AsIs'
	else b.intermediate_stg_function
END
>>


sql_server_default_redshift_destination_datatype_name()::=<<
CASE
	when (b.source_datatype_name = 'char') then
	(
		CASE
			when (a.data_length <= 255) then concat('char','(', a.data_length,')')
			when (a.data_length between 256 and 8000) then concat('varchar','(', a.data_length,')')
			else 'varchar(max)'
		END
	)
	when (b.source_datatype_name = 'varchar') then
	(
		CASE
			when (a.data_length = -1) then 'varchar(max)'
			else concat('varchar','(', a.data_length,')')
		END
	)
	when (b.source_datatype_name = 'nchar') then
	(
		CASE
			when (a.data_length <= 255) then concat('char', '(', a.data_length, ')')
			when (a.data_length <= 4000) then concat('varchar', '(', a.data_length, ')')
			else 'varchar(max)'
		END
	)
	when (b.source_datatype_name = 'nvarchar') then
	(
		CASE
			when (a.data_length = -1) then 'varchar(max)'
			when (a.data_length <= 255) then concat('char', '(', a.data_length, ')')
			else concat('varchar', '(', a.data_length, ')')
		END
	)
	when (b.source_datatype_name = 'decimal') then
	(
		CASE
			when (a.data_scale = 0 and (a.data_precision between 0 and 5)) then 'SMALLINT'
         when (a.data_scale = 0 and (a.data_precision between 6 and 10)) then 'INT'
         when (a.data_scale = 0 and (a.data_precision between 11 and 19)) then 'BIGINT'
         when (a.data_scale = 0 and (a.data_precision between 20 and 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         else 'varchar(max)'
		END
	)
	when (b.source_datatype_name = 'numeric') then
	(
		CASE
			when (a.data_scale = 0 and (a.data_precision between 0 and 5)) then 'SMALLINT'
         when (a.data_scale = 0 and (a.data_precision between 6 and 10)) then 'INT'
         when (a.data_scale = 0 and (a.data_precision between 11 and 19)) then 'BIGINT'
         when (a.data_scale = 0 and (a.data_precision between 20 and 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         else 'varchar(max)'
		END
	)
	when (b.source_datatype_name = 'datetime2') then
	(
		CASE
			when data_precision <14 then b.destination_datatype_name
			else 'varchar(max)'
		END
	)
	when (b.source_datatype_name = 'smallmoney' or b.source_datatype_name = 'money') then
    (
        concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
	else b.destination_datatype_name
END
>>


postgres_default_postgres_destination_datatype_name()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.destination_datatype_name
END
>>

postgres_default_postgres_intermediate_stg_function()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.intermediate_stg_function
END
>>

redshift_default_redshift_destination_datatype_name()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.destination_datatype_name
END
>>

redshift_default_redshift_intermediate_stg_function()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.intermediate_stg_function
END
>>

postgres_default_redshift_destination_datatype_name()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.destination_datatype_name
END
>>

postgres_default_redshift_intermediate_stg_function()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.intermediate_stg_function
END
>>

redshift_default_postgres_intermediate_stg_function()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.intermediate_stg_function
END
>>

redshift_default_postgres_destination_datatype_name()::=<<
CASE
when(b.source_datatype_name = 'char') then
   (
      CASE
         when (a.data_length between 1 and 255) then concat(b.destination_datatype_name,'(',a.data_length,')')
         else 'text'
      END
   )
END
>>

redshift_default_postgres_inconsistent_datatype() ::=<<
null
>>


oracle_default_postgres_inconsistent_datatype() ::=<<
null
>>

postgres_default_postgres_inconsistent_datatype() ::=<<
null
>>

redshift_default_redshift_inconsistent_datatype() ::=<<
null
>>

postgres_default_redshift_inconsistent_datatype() ::=<<
null
>>

mysql_default_postgres_inconsistent_datatype() ::=<<
null
>>

mysql_default_redshift_inconsistent_datatype() ::=<<
null
>>

sql_server_default_postgres_inconsistent_datatype() ::=<<
null
>>

sql_server_default_redshift_inconsistent_datatype() ::=<<
null
>>

oracle_parquet_bigquery_inconsistent_datatype() ::=<<
null
>>
postgres_parquet_bigquery_inconsistent_datatype() ::=<<
null
>>

sql_server_parquet_bigquery_inconsistent_datatype() ::=<<
null
>>


oracle_parquet_redshift_inconsistent_datatype()::=<<
case
	when (b.source_datatype_name in ('number','decimal')) then
	(
		case
			when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
			else 'number_to_varchar'
		end
	)
	when (b.source_datatype_name ~* 'INTERVAL YEAR TO MONTH') then 'interval_year_to_month_to_varchar'
	when (b.source_datatype_name ~* 'INTERVAL DAY TO SECOND') then 'interval_day_to_second_to_varchar'
	when (b.source_datatype_name in ('rowid','urowid','xmltype','timestamp','date')) then concat(b.source_datatype_name,'_to_varchar')
	when (b.source_datatype_name = 'float') then
	(
		case
			when (a.data_precision <= 53) then null
			else 'float_to_varchar'
		end
	)
	when (b.source_datatype_name ~* 'LOCAL TIME ZONE') then 'timestamp_with_local_time_zone_to_varchar'
	when (b.source_datatype_name ~* 'TIME ZONE') then 'timestamp_with_time_zone_to_varchar'
	else null
end
>>

mysql_parquet_redshift_inconsistent_datatype() ::=<<
case
    when (b.source_datatype_name = 'decimal') then
    (
    case
        when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
        else 'decimal_to_varchar'
    end
    )
    when (b.source_datatype_name in ('datetime','bit','timestamp','time','year','blob','mediumblob','longblob','tinyblob','binary','varbinary')) then        concat(b.source_datatype_name,'_to_varchar')
    else null
end
>>

postgres_parquet_redshift_inconsistent_datatype() ::=<<
case
    when (b.source_datatype_name = 'numeric') then
    (
        case
            when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
            else 'numeric_to_string'
        end
    )
    when (b.source_datatype_name = 'bit varying') then 'bit_varying_to_string'
    when (b.source_datatype_name = 'time without time zone') then 'time_without_time_zone_to_string'
    when (b.source_datatype_name = 'time with time zone') then 'time_with_time_zone_to_string'
    when (b.source_datatype_name = 'timestamp without time zone') then 'timestamp_without_time_zone_to_string'
    when (b.source_datatype_name = 'timestamp with time zone') then 'timestamp_with_time_zone_to_string'
    when (b.source_datatype_name in ('money','bit','abstime')) then concat(b.source_datatype_name,'_to_string')
    else null
end
>>

sql_server_parquet_redshift_inconsistent_datatype() ::=<<
case
    when (b.source_datatype_name in ('bit','time','timestamp','datetime','datetime2','datetimeoffset','smalldatetime','binary','varbinary')) then concat(b.source_datatype_name,'_to_varchar')
    else null
end
>>

oracle_parquet_redshift_destination_datatype_name()::=<<
CASE
when (b.source_datatype_name = 'char') then
(
    case
        when (a.data_length = 0 OR a.data_length = null) then 'char(1)'
        when (a.data_length between 1 and 2000) then concat('char(',a.data_length,')')
    END
)
when (b.source_datatype_name = 'varchar' or b.source_datatype_name = 'varchar2') THEN concat('varchar(',a.data_length,')')
when (b.source_datatype_name = 'nchar') THEN concat('nchar(',a.data_length,')')
when (b.source_datatype_name = 'nvarchar2') THEN concat('nvarchar(',a.data_length,')')
when (b.source_datatype_name = 'raw') THEN concat('varchar(',a.data_length*2,')')
when (b.source_datatype_name = 'urowid' OR b.source_datatype_name = 'xmltype') THEN concat('varchar(',a.data_length,')')
when (b.source_datatype_name = 'long' or b.source_datatype_name = 'clob' or b.source_datatype_name = 'nclob' OR b.source_datatype_name = 'blob' OR b.source_datatype_name = 'xmltype') THEN
CASE
WHEN (a.data_length =0 OR a.data_length = null) THEN 'varchar(5000)'
else
concat('varchar(',a.data_length,')')
end
when (b.source_datatype_name = 'decimal' or b.source_datatype_name = 'number') then
(
CASE
when (data_scale <= 37 and (data_precision <= 38))
then concat('decimal','(',a.data_precision,',',a.data_scale,')')
else 'varchar(255)' END
)
when (b.source_datatype_name~*'timestamp') then 'varchar(100)'
when(a.advanced_options_sub_type_id in (14,15,17)) then 'VARCHAR(40)'
else b.destination_datatype_name END
>>

oracle_parquet_redshift_intermediate_stg_function()::=<<
case
when (b.source_datatype_name = 'decimal' or b.source_datatype_name = 'number')
then
(
CASE
when (data_scale <= 37 and (data_precision <= 38)) then 'ParquetStringToDecimalConvert'
else  'AsIs'
END
)
when(a.advanced_options_sub_type_id in (14,15,17)) then 'AsIs'
else b.intermediate_stg_function
end
>>


get_source_stg_function()::=<<
case
           when(a.advanced_options_sub_type_id = 17) then 'UserText'
           when(a.advanced_options_sub_type_id = 14) then 'AsIs'
           when(a.advanced_options_sub_type_id = 15) then 'ReplaceWithNull'
else b.source_stg_function
end
>>

get_Hive_source_stg_function()::=<<
case
           when(a.advanced_options_sub_type_id = 17) then 'UserText'
           when(a.advanced_options_sub_type_id = 14) then 'AsIs'
           when(a.advanced_options_sub_type_id = 15) then 'ReplaceWithNullHive'
else b.source_stg_function
end
>>

get_Mysql_source_stg_function()::=<<
case
           when(a.advanced_options_sub_type_id = 17) then 'UserText'
           when(a.advanced_options_sub_type_id = 14) then 'AsIs'
           when(a.advanced_options_sub_type_id = 15) then 'ReplaceWithNullMysql'
else b.source_stg_function
end
>>

redshift_parquet_hive_destination_datatype_name()::=<<
CASE
	when (b.source_datatype_name = 'character') then
	(
		CASE
			when (a.data_length between 1 and 255) then concat(b.destination_datatype_name,'(',a.data_length,')')
			else 'string'
		END
	)
	when (b.source_datatype_name = 'character varying') then
	(
		CASE
			when (a.data_length between 1 and 65535) then concat(b.destination_datatype_name,'(',a.data_length,')')
			else 'string'
		END
	)
	when (b.source_datatype_name = 'numeric') then
	(
		CASE

			when (a.data_precision between 1 and 38) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
			else 'string'
	END
	)
	when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
	else b.destination_datatype_name
END
>>

redshift_parquet_hive_intermediate_stg_function()::=<<
CASE
	when (b.source_datatype_name = 'numeric') then
	(
		CASE

			when (a.data_precision between 1 and 38) then b.intermediate_stg_function
			else 'AsIs'
		END
	)
	when(a.advanced_options_sub_type_id in (14,15,17)) then 'AsIs'
	else b.intermediate_stg_function
END
>>

redshift_parquet_hive_source_stg_function()::=<<
$get_source_stg_function()$
>>

redshift_parquet_hive_inconsistent_datatype() ::=<<
null
>>

hive_parquet_hive_source_stg_function()::=<<
$get_Hive_source_stg_function()$
>>

oracle_parquet_hive_source_stg_function()::=<<
$get_source_stg_function()$
>>

oracle_default_postgres_source_stg_function()::=<<
$get_source_stg_function()$
>>

redshift_default_postgres_source_stg_function()::=<<
$get_source_stg_function()$
>>

postgres_default_postgres_source_stg_function()::=<<
$get_source_stg_function()$
>>

redshift_default_redshift_source_stg_function()::=<<
$get_source_stg_function()$
>>


postgres_default_redshift_source_stg_function()::=<<
$get_source_stg_function()$
>>

mysql_default_postgres_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

mysql_default_redshift_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

sql_server_default_postgres_source_stg_function()::=<<
$get_source_stg_function()$
>>

sql_server_default_redshift_source_stg_function()::=<<
$get_source_stg_function()$
>>

oracle_parquet_redshift_source_stg_function()::=<<
$get_source_stg_function()$
>>

postgres_parquet_redshift_source_stg_function()::=<<
$get_source_stg_function()$
>>

postgres_parquet_adls_gen2_source_stg_function()::=<<
$get_source_stg_function()$
>>

mysql_parquet_redshift_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

sql_server_parquet_redshift_source_stg_function()::=<<
$get_source_stg_function()$
>>

oracle_parquet_s3_source_stg_function()::=<<
$get_source_stg_function()$
>>

mysql_parquet_s3_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

mysql_parquet_adls_gen2_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

sql_server_parquet_s3_source_stg_function()::=<<
$get_source_stg_function()$
>>

sql_server_parquet_adls_gen2_source_stg_function()::=<<
$get_source_stg_function()$
>>

hive_parquet_hive_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'char' or b.source_datatype_name = 'varchar') then concat(b.destination_datatype_name ,'(',a.data_length,')')
    when (b.source_datatype_name = 'decimal') then concat(b.destination_datatype_name ,'(',a.data_precision,',',a.data_scale,')')
    when (b.source_datatype_name = 'array' or b.source_datatype_name = 'struct' or b.source_datatype_name = 'map') then a.metadata_datatype
    else b.destination_datatype_name
END
>>

hive_parquet_hive_intermediate_stg_function()::=<<
b.intermediate_stg_function

>>

postgres_parquet_redshift_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'character' or b.source_datatype_name = 'bit') then(
    CASE
        when (a.data_length between 1 and 4096) then concat(b.destination_datatype_name ,'(',a.data_length,')')
        else 'VARCHAR(MAX)'
    END)
    when (b.source_datatype_name = 'character varying' or b.source_datatype_name = 'bit varying') then(
    CASE
        when (a.data_length between 1 and 65535) then concat(b.destination_datatype_name ,'(',a.data_length,')')
        else 'VARCHAR(MAX)'
    END)
    when (b.source_datatype_name = 'numeric') then(
    CASE
        when (a.data_precision between 1 and 38) then concat(b.destination_datatype_name ,'(',a.data_precision,',',a.data_scale,')')
        else 'VARCHAR(MAX)'
    END)
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'VARCHAR(40)'
    else b.destination_datatype_name
END
>>

postgres_parquet_redshift_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name = 'character') then(
    CASE
        when (a.data_length between 1 and 4096) then b.intermediate_stg_function
        else 'AsIs'
    END)
    when (b.source_datatype_name = 'character varying') then(
    CASE
        when (a.data_length between 1 and 65535) then b.intermediate_stg_function
        else 'AsIs'
    END)
    when (b.source_datatype_name = 'numeric') then(
    CASE
        when (a.data_precision between 1 and 38) then b.intermediate_stg_function
        else 'AsIs'
    END)
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AsIs'
    else b.intermediate_stg_function
END
>>

mysql_parquet_redshift_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'char' or b.source_datatype_name='binary') then(
    case
        when (a.data_length = 0) then 'VARCHAR(1)'
        when (a.data_length between 1 and 4096) then concat('CHAR','(',a.data_length,')')
        else 'VARCHAR(MAX)'
    END)
    when (b.source_datatype_name = 'varchar' or b.source_datatype_name='blob' or b.source_datatype_name='mediumblob' or b.source_datatype_name='longblob' or b.source_datatype_name='enum' or b.source_datatype_name='set' or b.source_datatype_name='text' or b.source_datatype_name='mediumtext' or b.source_datatype_name='tinytext' or b.source_datatype_name='longtext') then(
    case
        when (a.data_length = 0) then 'VARCHAR(1)'
        when (a.data_length between 1 and 65535) then concat('VARCHAR','(',a.data_length,')')
        else 'VARCHAR(MAX)'
    END)
    when (b.source_datatype_name='bit') then concat(b.destination_datatype_name ,'(',a.data_precision,')')
    when (b.source_datatype_name = 'decimal') then(
    CASE
        when (a.data_precision between 1 and 38) then concat('DECIMAL','(',a.data_precision,',',a.data_scale,')')
        else 'VARCHAR'
    END)
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'VARCHAR(40)'
    else b.destination_datatype_name
END
>>

mysql_parquet_redshift_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name = 'char') then(
    CASE
        when (a.data_length between 1 and 4096) then b.intermediate_stg_function
        else 'AsIs'
    END)
    when (b.source_datatype_name = 'varchar') then(
    CASE
        when (a.data_length between 1 and 65535) then b.intermediate_stg_function
        else 'AsIs'
    END)
    when (b.source_datatype_name = 'decimal') then(
    CASE
        when (a.data_precision between 1 and 38) then b.intermediate_stg_function
        else 'AsIs'
    END)
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AsIs'
    else b.intermediate_stg_function
END
>>

sql_server_parquet_redshift_destination_datatype_name()::=<<
CASE
when (b.source_datatype_name = 'char') then
(
CASE
when (a.data_length between 1 and 4096) then concat('char','(',a.data_length,')')
else 'VARCHAR(MAX)' END
)
when (b.source_datatype_name = 'varchar') then
(
CASE
when (a.data_length between 1 and 65535) then concat('varchar','(',a.data_length,')')
else 'VARCHAR(MAX)' END
)
when (b.source_datatype_name = 'nchar') then
(
CASE
when (a.data_length between 1 and 4096) then concat('nchar','(',a.data_length,')')
else 'NVARCHAR(MAX)' END
)
when (b.source_datatype_name = 'nvarchar') then
(
CASE
when (a.data_length between 1 and 65535) then concat('nvarchar','(',a.data_length,')')
else 'NVARCHAR(MAX)' END
)
when (b.source_datatype_name = 'decimal') then
(
CASE
when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat('decimal','(',a.data_precision,',',a.data_scale,')')
else 'VARCHAR(MAX)' END
)
when (b.source_datatype_name = 'numeric') then
(
CASE
when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat('numeric','(',a.data_precision,',',a.data_scale,')')
else 'VARCHAR(MAX)' END
)
when (b.source_datatype_name = 'binary') then
    (
        case
            when (a.data_length = 0) then 'varchar(1)'
            when (a.data_length between 1 and 8000) then concat(b.destination_datatype_name ,'(',a.data_length,')')
        else 'varchar(MAX)' END
    )
    when (b.source_datatype_name = 'varbinary') then
    (
        case
            when (a.data_length = 0) then 'varchar(1)'
            when (a.data_length between 1 and 8000) then concat(b.destination_datatype_name ,'(',a.data_length,')')
        else 'varchar(max)' END
    )
when(a.advanced_options_sub_type_id in (14,15,17)) then 'VARCHAR(40)'
else b.destination_datatype_name END
>>

sql_server_parquet_redshift_intermediate_stg_function()::=<<
CASE
when (b.source_datatype_name = 'char') then
(
CASE
when (a.data_length between 1 and 4096) then b.intermediate_stg_function
else 'AsIs' END
)
when (b.source_datatype_name = 'varchar') then
(
CASE
when (a.data_length between 1 and 65535) then b.intermediate_stg_function
else 'AsIs' END
)
when (b.source_datatype_name = 'nchar') then
(
CASE
when (a.data_length between 1 and 4096) then b.intermediate_stg_function
else 'AsIs' END
)
when (b.source_datatype_name = 'nvarchar') then
(
CASE
when (a.data_length between 1 and 65535) then b.intermediate_stg_function
else 'AsIs' end
)
when (b.source_datatype_name = 'decimal') then
(
CASE
when (a.data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function
else 'AsIs' END
)
when (b.source_datatype_name = 'numeric') then
(
CASE
when (a.data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function
else 'AsIs' END
)
when(a.advanced_options_sub_type_id in (14,15,17)) then 'AsIs'
else b.intermediate_stg_function END
>>

oracle_parquet_azure_synapse_intermediate_stg_function()::=<<
case
    when (b.source_datatype_name = 'decimal' or b.source_datatype_name = 'number') then
    (
        CASE
            when (data_scale = 0 and (data_precision between 0 and 2)) then 'ParquetStringToTinyIntConvert'
            when (data_scale = 0 and (data_precision between 3 and 4)) then 'ParquetStringToSmallIntConvert'
            when (data_scale = 0 and (data_precision between 5 and 9)) then 'ParquetStringToIntConvert'
            when (data_scale = 0 and (data_precision between 10 and 18)) then 'ParquetStringToBigIntConvert'
            when (data_scale = 0 and (data_precision between 19 and 38)) then b.intermediate_stg_function
            when (data_scale <= 37 and (data_precision <= 38)) then b.intermediate_stg_function
            else 'AsIs'
        END
    )
    when (b.source_datatype_name = 'float') then
    (
   	    CASE
   	   	  	when (a.data_precision between 0 and 53) then 'ParquetStringToDoubleConvert'
   	   	  	else 'AsIs'
   	   	END
   	)
   	when(a.advanced_options_sub_type_id in (14,15,17)) then 'AsIs'
    else b.intermediate_stg_function
END
>>

oracle_parquet_azure_synapse_destination_datatype_name()::=<<
case
    when (b.source_datatype_name = 'decimal' or b.source_datatype_name = 'number') then
    (
        CASE
            when (data_scale = 0 and (data_precision between 0 and 2)) then 'TINYINT'
            when (data_scale = 0 and (data_precision between 3 and 4)) then 'SMALLINT'
            when (data_scale = 0 and (data_precision between 5 and 9)) then 'INT'
            when (data_scale = 0 and (data_precision between 10 and 18)) then 'BIGINT'
            when (data_scale = 0 and (data_precision between 19 and 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
            when (data_scale <= 37 and (data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
            else 'VARCHAR(255)'
        END
    )
    when (b.source_datatype_name = 'float') then
    (
        case
       	  	when (a.data_precision between 0 and 53) then 'FLOAT'
   	   	  	else 'VARCHAR(255)'
       	END
    )
	when (b.source_datatype_name in ('char','nchar','varchar2','nvarchar2','raw')) then concat(b.destination_datatype_name,'(',a.data_length,')')
	when(a.advanced_options_sub_type_id in (14,15,17)) then 'VARCHAR(8000)'
    else b.destination_datatype_name
end
>>

oracle_parquet_azure_synapse_source_stg_function()::=<<
$get_source_stg_function()$
>>

oracle_parquet_adls_gen2_source_stg_function()::=<<
$get_source_stg_function()$
>>

mysql_parquet_hive_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
		CASE
			when (a.data_length between 1 and 255) then concat('CHAR','(',a.data_length,')')
			else 'STRING'
		END
    )
    when (b.source_datatype_name = 'varchar') then
    (
		CASE
			when (a.data_length between 1 and 65535) then concat('VARCHAR','(',a.data_length,')')
			else 'STRING'
		END
    )
    when (b.source_datatype_name = 'bit') then concat('VARCHAR','(',a.data_precision,')')
    when (b.source_datatype_name = 'decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then concat('DECIMAL','(',a.data_precision,',',a.data_scale,')')
			else 'STRING'
		END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.destination_datatype_name
END
>>

mysql_parquet_hive_intermediate_stg_function_old()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
		CASE
			when (a.data_length between 1 and 255) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when (b.source_datatype_name='varchar') then
    (
		CASE
			when (a.data_length between 1 and 65535) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when (b.source_datatype_name='decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AsIs'
    else b.intermediate_stg_function
END
>>

mysql_parquet_hive_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name='decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AsIs'
    else b.intermediate_stg_function
END
>>

mysql_parquet_hive_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

mysql_parquet_azure_synapse_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
        case
            when (a.data_length = 0) then 'VARCHAR(1)'
            when (a.data_length between 1 and 8000) then concat(b.destination_datatype_name ,'(',a.data_length,')')
        else 'VARCHAR(MAX)'  END
    )
    when (b.source_datatype_name = 'varchar' or b.source_datatype_name = 'enum' or b.source_datatype_name = 'set') then
    (
        case
            when (a.data_length = 0) then 'VARCHAR(1)'
            when (a.data_length between 1 and 8000) then concat(b.destination_datatype_name ,'(',a.data_length,')')
        else 'VARCHAR(MAX)' END
    )
    when (b.source_datatype_name = 'binary') then
    (
        case
            when (a.data_length = 0) then 'VARBINARY(1)'
            when (a.data_length between 1 and 8000) then concat(b.destination_datatype_name ,'(',a.data_length,')')
        else 'VARBINARY(MAX)' END
    )
    when (b.source_datatype_name = 'varbinary') then
    (
        case
            when (a.data_length = 0) then 'VARBINARY(1)'
            when (a.data_length between 1 and 8000) then concat(b.destination_datatype_name ,'(',a.data_length,')')
        else 'VARBINARY(MAX)' END
    )
    when (b.source_datatype_name='bit') then concat(b.destination_datatype_name ,'(',a.data_precision,')')
    when (b.source_datatype_name = 'decimal') then
    (
        CASE
            when (a.data_precision between 1 and 38) then concat(b.destination_datatype_name ,'(',a.data_precision,',',a.data_scale,')')
        else 'VARCHAR(255)' END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'VARCHAR(8000)'
    else b.destination_datatype_name
END
>>

mysql_parquet_azure_synapse_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name='decimal') then
    (
        CASE
            when (data_scale <= 37 and (data_precision <= 38)) then b.intermediate_stg_function
            else 'AsIs'
        END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AsIs'
    else b.intermediate_stg_function
END
>>

mysql_parquet_azure_synapse_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

mysql_parquet_adls_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

sql_server_parquet_hive_destination_datatype_name()::=<<
CASE
	when (b.source_datatype_name = 'char') then
	(
		CASE
			when (a.data_length <= 255) then concat('char','(', a.data_length,')')
			when (a.data_length between 256 and 8000) then concat('varchar','(', a.data_length,')')
			else 'STRING'
		END
	)
	when (b.source_datatype_name = 'varchar') then
	(
		CASE
			when (a.data_length = -1) then 'string'
			else concat('varchar','(', a.data_length,')')
		END
	)
	when (b.source_datatype_name = 'nchar') then
	(
		CASE
			when (a.data_length <= 255) then concat('char', '(', a.data_length, ')')
			when (a.data_length <= 4000) then concat('varchar', '(', a.data_length, ')')
			else 'STRING'
		END
	)
	when (b.source_datatype_name = 'nvarchar') then
	(
		CASE
			when (a.data_length = -1) then 'STRING'
			when (a.data_length <= 255) then concat('char', '(', a.data_length, ')')
			else concat('varchar', '(', a.data_length, ')')
		END
	)
	when (b.source_datatype_name = 'decimal') then
	(
		CASE
			when (data_scale = 0 and (data_precision between 0 and 2)) then 'TINYINT'
			when (data_scale = 0 and (data_precision between 3 and 4)) then 'SMALLINT'
			when (data_scale = 0 and (data_precision between 5 and 9)) then 'INT'
			when (data_scale = 0 and (data_precision between 10 and 18)) then 'BIGINT'
			when (data_scale = 0 and (data_precision between 19 and 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
			when (data_scale <= 37 and (data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
			else 'STRING'
		END
	)
	when (b.source_datatype_name = 'numeric') then
	(
		CASE
			when (data_scale = 0 and (data_precision between 0 and 2)) then 'TINYINT'
			when (data_scale = 0 and (data_precision between 3 and 4)) then 'SMALLINT'
			when (data_scale = 0 and (data_precision between 5 and 9)) then 'INT'
			when (data_scale = 0 and (data_precision between 10 and 18)) then 'BIGINT'
			when (data_scale = 0 and (data_precision between 19 and 38)) then b.destination_datatype_name
			when (data_scale <= 37 and (data_precision <= 38)) then b.destination_datatype_name
			else 'STRING'
		END
	)
	when (b.source_datatype_name = 'datetime2') then
	(
		CASE
			when data_precision <7 then b.destination_datatype_name
			else 'STRING'
		END
	)
	when (b.source_datatype_name = 'smallmoney' or b.source_datatype_name = 'money') then
    (
        concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
	else b.destination_datatype_name
END
>>

sql_server_parquet_hive_intermediate_stg_function()::=<<
case
	when (b.source_datatype_name = 'decimal') then
	(
		CASE
			when (data_scale = 0 and (data_precision between 0 and 2)) then 'ParquetStringToTinyIntConvert'
			when (data_scale = 0 and (data_precision between 3 and 4)) then 'ParquetStringToSmallIntConvert'
			when (data_scale = 0 and (data_precision between 5 and 9)) then 'ParquetStringToIntConvert'
			when (data_scale = 0 and (data_precision between 10 and 18)) then 'ParquetStringToBigIntConvert'
			when (data_scale = 0 and (data_precision between 19 and 38)) then b.intermediate_stg_function
			when (data_scale <= 37 and (data_precision <= 38)) then b.intermediate_stg_function
			else 'AsIs'
		END
	)
	when (b.source_datatype_name = 'numeric') then
	(
		CASE
			when (data_scale = 0 and (data_precision between 0 and 2)) then 'ParquetStringToTinyIntConvert'
			when (data_scale = 0 and (data_precision between 3 and 4)) then 'ParquetStringToSmallIntConvert'
			when (data_scale = 0 and (data_precision between 5 and 9)) then 'ParquetStringToIntConvert'
			when (data_scale = 0 and (data_precision between 10 and 18)) then 'ParquetStringToBigIntConvert'
			when (data_scale = 0 and (data_precision between 19 and 38)) then b.intermediate_stg_function
			when (data_scale <= 37 and (data_precision <= 38)) then b.intermediate_stg_function
			else 'AsIs'
		END
	)
	when (b.source_datatype_name = 'datetime2') then
	(
		CASE
			when data_precision <7 then b.intermediate_stg_function
			else 'AsIs'
		END
	)
	when(a.advanced_options_sub_type_id in (14,15,17)) then 'AsIs'
	else b.intermediate_stg_function
END
>>

sql_server_parquet_hive_source_stg_function()::=<<
$get_source_stg_function()$
>>

postgres_parquet_hive_destination_datatype_name()::=<<
CASE
	when (b.source_datatype_name = 'character') then
	(
		CASE
			when (a.data_length between 1 and 255) then concat('char','(',a.data_length,')')
			else 'string'
		END
	)
	when (b.source_datatype_name = 'character varying' or b.source_datatype_name = 'bit' or b.source_datatype_name = 'bit varying') then
	(
		CASE
			when (a.data_length between 1 and 65535) then concat('varchar','(',a.data_length,')')
			else 'string'
		END
	)
	when (b.source_datatype_name = 'numeric') then
	(
		CASE

			when (a.data_precision between 1 and 38) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
			else 'string'
	END
	)
   when (b.source_datatype_name = 'varchar') then
    (
		CASE
			when (a.data_length between 1 and 65535) then concat('VARCHAR','(',a.data_length,')')
		else 'STRING'
		END
    )
	when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
	else b.destination_datatype_name
END
>>

postgres_parquet_hive_intermediate_stg_function()::=<<
CASE
	when (b.source_datatype_name = 'numeric') then
	(
		CASE

			when (a.data_precision between 1 and 38) then b.intermediate_stg_function
			else 'AsIs'
		END
	)
	when(a.advanced_options_sub_type_id in (14,15,17)) then 'AsIs'
	else b.intermediate_stg_function
END
>>

postgres_parquet_hive_source_stg_function()::=<<
CASE
	when (b.source_datatype_name = 'timestamp without time zone') then (
		CASE
			when (a.data_precision is null) then 'PostgresTimestampOffsetZeroToStringConvert'
			else b.source_stg_function
		END
	)
	when (b.source_datatype_name = 'timestamp with time zone') then
	(
		CASE
			when (a.data_precision is null) then 'PostgresTimestamptzOffsetZeroToStringConvert'
			else b.source_stg_function
		END
	)
	when (b.source_datatype_name = 'time without time zone') then
	(
		CASE
			when (a.data_precision is null) then 'PostgresTimeOffsetZeroToStringConvert'
			else b.source_stg_function
		END
	)
    when(a.advanced_options_sub_type_id = 17) then 'UserText'
    when(a.advanced_options_sub_type_id = 14) then 'AsIs'
    when(a.advanced_options_sub_type_id = 15) then 'ReplaceWithNull'
	else b.source_stg_function
END
>>

oracle_parquet_bigquery_source_stg_function()::=<<
$get_source_stg_function()$
>>

oracle_parquet_bigquery_intermediate_stg_function()::=<<
case
    when (b.source_datatype_name in ('number','decimal')) then
    (
        CASE
            when (data_scale=0 and (data_precision between 0 and 2)) then 'ParquetStringToTinyIntConvert'
            when (data_scale=0 and (data_precision between 3 and 4)) then 'ParquetStringToSmallIntConvert'
            when (data_scale=0 and (data_precision between 5 and 9)) then 'ParquetStringToIntConvert'
            when (data_scale=0 and (data_precision between 10 and 18)) then 'ParquetStringToBigIntConvert'
            when (data_scale=0 and (data_precision between 18 and 38)) then 'ParquetStringToDecimalConvert'
            when (data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function
            else 'AsIs'
        END
    )
    when (b.source_datatype_name = 'float') then
    (
        CASE
                 when (a.data_precision between 0 and 23) then 'ParquetStringToFloatConvert'
                 when (a.data_precision between 24 and 53) then 'ParquetStringToDoubleConvert'
                 else 'AsIs'
           END
    )
    else b.intermediate_stg_function
end
>>



oracle_parquet_bigquery_destination_datatype_name()::=<<
case
    when (b.source_datatype_name = 'number') then
    (
      CASE
         when (a.data_scale = 0 and (a.data_precision between 0 and 2)) then 'TINYINT'
         when (a.data_scale = 0 and (a.data_precision between 3 and 4)) then 'SMALLINT'
         when (a.data_scale = 0 and (a.data_precision between 5 and 9)) then 'INT'
         when (a.data_scale = 0 and (a.data_precision between 10 and 18)) then 'BIGINT'
         when (a.data_scale = 0 and (a.data_precision between 18 and 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         else 'STRING'
      END
    )
    when (b.source_datatype_name = 'float') then
    (
           case
                 when (a.data_precision between 0 and 53) then 'FLOAT'
                 else 'STRING'
           END
    )
    else b.destination_datatype_name
end
>>

postgres_parquet_bigquery_source_stg_function()::=<<
$get_source_stg_function()$
>>

postgres_parquet_bigquery_intermediate_stg_function()::=<<

CASE
    when (b.source_datatype_name = 'NUMERIC') then(
    CASE
        when ((a.data_precision between 1 and 38) and (a.data_scale between 1 and 9)) then 'AsIS'
        else 'PostgresStringConvert'
    END)
    else b.intermediate_stg_function
END


>>
postgres_parquet_bigquery_destination_datatype_name()::=<<

CASE
    when (b.source_datatype_name = 'NUMERIC') then(
    CASE
        when ((a.data_precision between 1 and 38) and (a.data_scale between 1 and 9)) then concat('NUMERIC','(',a.data_precision,',',a.data_scale,')')
        else 'STRING'
    END)
    else b.destination_datatype_name
END

>>

sql_server_parquet_bigquery_source_stg_function()::=<<
$get_source_stg_function()$
>>

sql_server_parquet_bigquery_intermediate_stg_function()::=<<

CASE
    when (b.source_datatype_name = 'NUMERIC') then(
    CASE
        when ((a.data_precision between 1 and 38) and (a.data_scale between 1 and 9)) then 'AsIS'
        else 'SQLServerStringConvert'
    END)
    else b.intermediate_stg_function
END


>>
sql_server_parquet_bigquery_destination_datatype_name()::=<<

CASE
    when (b.source_datatype_name = 'NUMERIC') then(
    CASE
        when ((a.data_precision between 1 and 38) and (a.data_scale between 1 and 9)) then concat('NUMERIC','(',a.data_precision,',',a.data_scale,')')
        else 'STRING'
    END)
    else b.destination_datatype_name
END

>>

postgres_parquet_s3_source_stg_function()::=<<
CASE
	when (b.source_datatype_name = 'timestamp without time zone') then(
		CASE
			when (a.data_precision is null) then 'PostgresTimestampOffsetZeroToStringConvert'
			else b.source_stg_function
		END
	)
	when (b.source_datatype_name = 'timestamp with time zone') then
	(
		CASE
			when (a.data_precision is null) then 'PostgresTimestamptzOffsetZeroToStringConvert'
			else b.source_stg_function
		END
	)
	when (b.source_datatype_name = 'time without time zone') then
	(
		CASE
			when (a.data_precision is null) then 'PostgresTimeOffsetZeroToStringConvert'
			else b.source_stg_function
		END
	)
	else
	b.source_stg_function
END
>>

sql_server_parquet_azure_synapse_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
        CASE
            when (a.data_length between 1 and 8000) then concat('char','(',a.data_length,')')
            else 'VARCHAR(MAX)'
        END
    )
    when (b.source_datatype_name = 'varchar') then
    (
        CASE
            when (a.data_length between 1 and 8000) then concat('varchar','(',a.data_length,')')
            else 'VARCHAR(MAX)'
        END
    )
    when (b.source_datatype_name = 'nchar') then
    (
        CASE
            when (a.data_length between 1 and 4000) then concat('nchar','(',a.data_length,')')
            else 'NVARCHAR(MAX)'
        END
    )
    when (b.source_datatype_name = 'nvarchar') then
    (
        CASE
            when (a.data_length between 1 and 4000) then concat('nvarchar','(',a.data_length,')')
            else 'NVARCHAR(MAX)'
        END
    )
    when (b.source_datatype_name = 'binary') then
    (
        CASE
            when (a.data_length between 1 and 8000) then concat('binary','(',a.data_length,')')
            else 'VARBINARY(MAX)'
        END
    )
    when (b.source_datatype_name = 'varbinary') then
    (
        CASE
            when (a.data_length between 1 and 8000) then concat('varbinary','(',a.data_length,')')
            else 'VARBINARY(MAX)'
        END
    )
    when (b.source_datatype_name = 'decimal' or b.source_datatype_name = 'numeric') then
    (
        CASE
            when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat('decimal','(',a.data_precision,',',a.data_scale,')')
            else 'VARCHAR(255)'
        END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'VARCHAR(8000)'
    else b.destination_datatype_name
END
>>

sql_server_parquet_azure_synapse_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name = 'decimal' or b.source_datatype_name = 'numeric') then
    (
        CASE
            when (a.data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function
            else 'AsIs'
        END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AsIs'
    else b.intermediate_stg_function
END
>>

sql_server_parquet_azure_synapse_source_stg_function()::=<<
$get_source_stg_function()$
>>

sql_server_parquet_adls_source_stg_function()::=<<
$get_source_stg_function()$
>>

postgres_parquet_azure_synapse_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'character' or b.source_datatype_name = 'bit') then
    (
       case
          when (a.data_length between 1 and 8000) then concat(b.destination_datatype_name ,'(',a.data_length,')')
          else 'VARCHAR(MAX)'
       END
    )
    when (b.source_datatype_name = 'character varying' or b.source_datatype_name = 'bit varying') then
    (
       case
          when (a.data_length between 1 and 8000) then concat(b.destination_datatype_name ,'(',a.data_length,')')
          else 'VARCHAR(MAX)'
       END
    )
    when (b.source_datatype_name = 'numeric') then
    (
       CASE
          when (a.data_precision between 1 and 38) then concat(b.destination_datatype_name ,'(',a.data_precision,',',a.data_scale,')')
          else 'VARCHAR(max)'
       END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'VARCHAR(8000)'
    else b.destination_datatype_name
END
>>

postgres_parquet_azure_synapse_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name='numeric') then
    (
       CASE
          when (a.data_precision between 1 and 38) then b.intermediate_stg_function
          else 'AsIs'
       END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AsIs'
    else b.intermediate_stg_function
END
>>

postgres_parquet_azure_synapse_source_stg_function()::=<<
$get_source_stg_function()$
>>

postgres_parquet_adls_source_stg_function()::=<<
$get_source_stg_function()$
>>

generate_credential_json(templateData)::=<<
{
  "sourceCredentials": {
    "token": "$templateData.input_data.jwt_token$",
    "credential_id": $if(first(templateData.table_metadata_output).source_credential_id)$$first(templateData.table_metadata_output).source_credential_id$$else$0$endif$ ,
    "credential_type_id": $if(first(templateData.table_metadata_output).source_credential_type_id)$$first(templateData.table_metadata_output).source_credential_type_id$$else$0$endif$ ,
    "end_point": "$templateData.input_data.end_point$"
  },
  "targetCredentials":$if(first(templateData.table_metadata_output).filesystem_credential_id)$ {
    "token": "$templateData.input_data.jwt_token$",
    "credential_id":  $first(templateData.table_metadata_output).filesystem_credential_id$,
    "credential_type_id":  $first(templateData.table_metadata_output).filesystem_credential_type_id$,
    "end_point": "$templateData.input_data.end_point$"
  }
  $else${
    "token": "$templateData.input_data.jwt_token$",
    "credential_id": $if(first(templateData.table_metadata_output).destination_credential_id)$$first(templateData.table_metadata_output).destination_credential_id$$else$0$endif$ ,
    "credential_type_id": $if(first(templateData.table_metadata_output).destination_credential_type_id)$$first(templateData.table_metadata_output).destination_credential_type_id$$else$0$endif$ ,
    "end_point": "$templateData.input_data.end_point$"
  }  $endif$
}
>>

convert_base64_encrypted_json_creds_to_rsa(templateData)::=<<
$templateData.base64_encrypted_json_creds$
>>

generate_source_input_json_for_checkpoint(templateData)::=<<
$(["generate_source_input_json_for_checkpoint_",templateData.input_data.metadata_category])(templateData)$
>>

generate_source_input_json_for_checkpoint_file(templateData)::=<<
{
"processId":$templateData.input_data.process_id$,
"dataplaceId":$first(templateData.table_metadata_output).source_dataplace_id$,
"tableId":$first(templateData.table_metadata_output).file_id$,
"datamovementId":$templateData.input_data.data_movement_id$
}
>>

generate_source_input_json_for_checkpoint_relational(templateData)::=<<
{
"processId":$templateData.input_data.process_id$,
"dataplaceId":$first(templateData.table_metadata_output).dataplace_id$,
"tableId":$first(templateData.table_metadata_output).table_id$,
"datamovementId":$templateData.input_data.data_movement_id$
}
>>

generate_almaren_artifact(templateData)::=<<
$if(first(templateData.table_partition_output) && (first(templateData.table_adv_opt_details_partition_details).is_parallel_ingestion))$
$(["generate_almaren_artifact_for_jdbcParallel_",templateData.input_data.destination])(templateData)$
$else$
$(["generate_almaren_artifact_for_",templateData.input_data.metadata_category])(templateData)$
$endif$
>>

generate_almaren_artifact_for_file(templateData)::=<<
$if(first(templateData.table_metadata_output).is_default)$
$(["generate_almaren_artifact_file_default","_",templateData.input_data.destination])(templateData)$
$else$
$(["generate_almaren_artifact_file","_",templateData.input_data.destination])(templateData)$
$endif$
>>

generate_almaren_artifact_for_relational(templateData)::=<<
$if(first(templateData.table_metadata_output).is_default)$
$(["generate_almaren_artifact_default_",templateData.input_data.destination])(templateData)$
$else$
$(["generate_almaren_artifact_",templateData.input_data.destination])(templateData)$
$endif$
>>

generate_almaren_artifact_for_jdbcParallel_adls_gen2(templateData)::=<<
import scala.sys.process._
import scala.util.Try
import scala.util.{Failure, Success}
import org.apache.log4j.{Level, Logger, LogManager}

val logger = LogManager.getLogger("com.Artifact")
logger.setLevel(Level.INFO)

Try {
import com.modak.common.token.Token
import com.modak.checkpoint.Checkpoint
import com.modak.common.Constants

val inputJson = Token.parsedInputJson
val checkpoint = Checkpoint(inputJson.processId,inputJson.dataplaceId, None, None,  Some(inputJson.datamovementId))
checkpoint.startDataMovement(inputJson.tableId)

Try {
 import com.github.music.of.the.ainur.almaren.builder.Core.Implicit
 import com.github.music.of.the.ainur.almaren.Almaren
 import com.modak.common.token.Token
 import com.modak.common.token.Token.Ldap
 import com.modak.common.token.Token.AzureGen2
 import com.modak.encryption.MessageEncryptionUtil
 import com.github.music.of.the.ainur.almaren.jdbcparallel.SourceJdbcParallelConn.SourceJdbcParallelImplicit
 import com.modak.common.Util._
 import org.apache.spark._
 import org.apache.spark.sql._


 val almaren = Almaren("adls_gen2_parallel_ingestion")
 val spark = almaren.spark.getOrCreate()
 spark.conf.set(s"fs.azure.account.key.\${AzureGen2.target.accountname}.dfs.core.windows.net", AzureGen2.target.accountkey)

 almaren.builder
    .sourceJdbcParallel($([first(templateData.table_metadata_output).source_type,"_source"])(templateData)$,
      List($([first(templateData.table_metadata_output).source_type,"_jdbcParallelQueries"])(templateData)$),
      $first(templateData.table_adv_opt_details_partition_details).maximum_parallel_connections$,
      Some(Ldap.source.username),Some(Ldap.source.password),Map("fetchsize" -> Constants.DB.fetchSize))
    .batch.write.format("$templateData.input_data.intermediate_type$").mode("overwrite").$([first(templateData
    .table_metadata_output).destination_type,"_destination_YYYY_MM_DD_HH_mm_SS_format"])(templateData)$
 } match {
   case Success(s) =>
     logger.info(s"Success \${inputJson.tableId}")
     checkpoint.endDatamovementWithSuccess(inputJson.tableId, verificationStatus = true)

   case Failure(f) =>
     logger.error(s"Failed \${inputJson.tableId}")
     logger.error(s"Error while ingestion", f)
     checkpoint.endDatamovementWithError(inputJson.tableId, f.getLocalizedMessage, verificationStatus = false)
     throw f
 } } match {
    case Success(s) => {
      logger.info(s"Ingestion Success")

        }
        case Failure (f) => {
        logger.error (s"Error while ingestion", f)
        throw f
    }
 }
>>

generate_almaren_artifact_file_hive(templateData)::=<<
import scala.sys.process._
import scala.util.Try
import scala.util.{Failure, Success}
import org.apache.log4j.{Level, Logger, LogManager}

val logger = LogManager.getLogger("com.Artifact")
logger.setLevel(Level.INFO)

Try {
import com.modak.common.token.Token
import com.modak.checkpoint.Checkpoint
import com.modak.common.Constants

val inputJson = Token.parsedInputJson
val checkpoint = Checkpoint(inputJson.processId,inputJson.dataplaceId, None, None,  Some(inputJson.datamovementId))
checkpoint.startDataMovement(inputJson.tableId)

Try {
 import com.github.music.of.the.ainur.almaren.builder.Core.Implicit
 import com.github.music.of.the.ainur.almaren.Almaren
 import com.modak.common.token.Token.AzureGen2
 import com.modak.encryption.MessageEncryptionUtil
 import com.modak.common.Util._
 import com.modak.common.token.Token.Aws
 import com.modak.common.token.Token.Gcp

 val almaren = Almaren("nabu-sparkbot-ingestion") ;
 val spark = almaren.spark.getOrCreate() ;
 $(["Configuration_",templateData.input_data.source])(templateData)$
 almaren.builder
 .sourceFile("$first(templateData.table_metadata_output).file_format$","$([templateData.input_data.source,"_source"])(templateData)$",$file_options(first(templateData.table_metadata_output))$)
 .sql("""$(["destination_transformation_query_for_",templateData.input_data.metadata_category])(templateData)$""".stripMargin)
 .batch.write.format("$templateData.input_data.intermediate_type$").mode("overwrite").$([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$;

 } match {
   case Success(s) =>
     logger.info(s"Success \${inputJson.tableId}")
     checkpoint.endDatamovementWithSuccess(inputJson.tableId, verificationStatus = true)

   case Failure(f) =>
     logger.error(s"Failed \${inputJson.tableId}")
     logger.error(s"Error while ingestion", f)
     checkpoint.endDatamovementWithError(inputJson.tableId, f.getLocalizedMessage, verificationStatus = false)
     throw f
 } } match {
    case Success(s) => {
      logger.info(s"Ingestion Success")

        }
        case Failure (f) => {
        logger.error (s"Error while ingestion", f)
        throw f
    }
 }

>>

postgres_destination(templateData)::=<<
"jdbc:postgresql://$first(templateData.table_metadata_output).destination_url$/$first(templateData.table_metadata_output).destination_db$","$first(templateData.table_metadata_output).destination_jdbc_driver$"
>>

redshift_target_destination(templateData)::=<<
"jdbc:redshift://$first(templateData.table_metadata_output).destination_url$/$first(templateData.table_metadata_output).destination_db$","$first(templateData.table_metadata_output).destination_jdbc_driver$"
>>

generate_almaren_artifact_default_postgres(templateData)::=<<
import scala.sys.process._
import scala.util.Try
import scala.util.{Failure, Success}
import org.apache.log4j.{Level, Logger, LogManager}

val logger = LogManager.getLogger("com.Artifact")
logger.setLevel(Level.INFO)

Try {
import com.modak.common.token.Token
import com.modak.checkpoint.Checkpoint
import com.modak.common.Constants

val inputJson = Token.parsedInputJson
val checkpoint = Checkpoint(inputJson.processId,inputJson.dataplaceId, None, None,  Some(inputJson.datamovementId))
checkpoint.startDataMovement(inputJson.tableId)

Try {
 import com.github.music.of.the.ainur.almaren.builder.Core.Implicit
 import com.github.music.of.the.ainur.almaren.Almaren
 import com.modak.common.token.Token.Ldap
 import com.modak.encryption.MessageEncryptionUtil
 import com.modak.common.Util._
 import org.apache.spark.sql.SaveMode

 val almaren = Almaren("nabu-sparkbot-ingestion") ;
 val spark = almaren.spark.getOrCreate() ;
 almaren.builder
 .sourceJdbc($([first(templateData.table_metadata_output).source_type,"_source"])(templateData)$,
     """$([first(templateData.table_metadata_output).source_type,"_jdbcQuery"])(templateData)$""".stripMargin,
     Some(Ldap.source.username),Some(Ldap.source.password),Map("fetchsize" -> Constants.DB.fetchSize))
.sql(
     """$destination_transformation_query(templateData)$""".stripMargin
   ).targetJdbc($([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$,"$first(templateData.table_metadata_output).destination_schema_name$.$first(templateData.table_metadata_output).destination_table_name$",SaveMode.Overwrite,Some(Ldap.target.username),Some(Ldap.target.password),Map()).batch;
} match {
   case Success(s) =>
     logger.info(s"Success \${inputJson.tableId}")
     checkpoint.endDatamovementWithSuccess(inputJson.tableId, verificationStatus = true)

   case Failure(f) =>
     logger.error(s"Failed \${inputJson.tableId}")
     logger.error(s"Error while ingestion", f)
     checkpoint.endDatamovementWithError(inputJson.tableId, f.getLocalizedMessage, verificationStatus = false)
     throw f
 } } match {
    case Success(s) => {
      logger.info(s"Ingestion Success")

        }
        case Failure (f) => {
        logger.error (s"Error while ingestion", f)
        throw f
    }
 }

>>

generate_almaren_artifact_default_redshift(templateData)::=<<
import scala.sys.process._
import scala.util.Try
import scala.util.{Failure, Success}
import org.apache.log4j.{Level, Logger, LogManager}

val logger = LogManager.getLogger("com.Artifact")
logger.setLevel(Level.INFO)

Try {
import com.modak.common.token.Token
import com.modak.checkpoint.Checkpoint
import com.modak.common.Constants

val inputJson = Token.parsedInputJson
val checkpoint = Checkpoint(inputJson.processId,inputJson.dataplaceId, None, None,  Some(inputJson.datamovementId))
checkpoint.startDataMovement(inputJson.tableId)

Try {
 import com.github.music.of.the.ainur.almaren.builder.Core.Implicit
 import com.github.music.of.the.ainur.almaren.Almaren
 import com.modak.common.token.Token.Ldap
 import com.modak.encryption.MessageEncryptionUtil
 import com.modak.common.Util._
 import org.apache.spark.sql.SaveMode

 val almaren = Almaren("nabu-sparkbot-ingestion") ;
 val spark = almaren.spark.getOrCreate() ;
 almaren.builder
 .sourceJdbc($([first(templateData.table_metadata_output).source_type,"_source"])(templateData)$,
     """$([first(templateData.table_metadata_output).source_type,"_jdbcQuery"])(templateData)$""".stripMargin,
     Some(Ldap.source.username),Some(Ldap.source.password),Map("fetchsize" -> Constants.DB.fetchSize))
.sql(
     """$destination_transformation_query(templateData)$""".stripMargin
   ).targetJdbc($([first(templateData.table_metadata_output).destination_type,"_target_destination"])(templateData)$,"$first(templateData.table_metadata_output).destination_schema_name$.$first(templateData.table_metadata_output).destination_table_name$",SaveMode.Overwrite,Some(Ldap.target.username),Some(Ldap.target.password),Map()).batch;
} match {
   case Success(s) =>
     logger.info(s"Success \${inputJson.tableId}")
     checkpoint.endDatamovementWithSuccess(inputJson.tableId, verificationStatus = true)

   case Failure(f) =>
     logger.error(s"Failed \${inputJson.tableId}")
     logger.error(s"Error while ingestion", f)
     checkpoint.endDatamovementWithError(inputJson.tableId, f.getLocalizedMessage, verificationStatus = false)
     throw f
 } } match {
    case Success(s) => {
      logger.info(s"Ingestion Success")

        }
        case Failure (f) => {
        logger.error (s"Error while ingestion", f)
        throw f
    }
 }

>>

generate_almaren_artifact_file_default_postgres(templateData)::=<<
import scala.sys.process._
import scala.util.Try
import scala.util.{Failure, Success}
import org.apache.log4j.{Level, Logger, LogManager}

val logger = LogManager.getLogger("com.Artifact")
logger.setLevel(Level.INFO)

Try {
import com.modak.common.token.Token
import com.modak.checkpoint.Checkpoint
import com.modak.common.Constants

val inputJson = Token.parsedInputJson
val checkpoint = Checkpoint(inputJson.processId,inputJson.dataplaceId, None, None,  Some(inputJson.datamovementId))
checkpoint.startDataMovement(inputJson.tableId)

Try {
 import com.github.music.of.the.ainur.almaren.builder.Core.Implicit
 import com.github.music.of.the.ainur.almaren.Almaren
 import com.modak.common.token.Token.Ldap
 import com.modak.encryption.MessageEncryptionUtil
 import com.modak.common.Util._
 import org.apache.spark.sql.SaveMode
 import com.modak.common.token.Token.Aws

 val almaren = Almaren("nabu-sparkbot-ingestion") ;
 val spark = almaren.spark.getOrCreate() ;
 spark.conf.set("spark.hadoop.fs.s3a.endpoint","")
 spark.conf.set("spark.hadoop.fs.s3a.access.key", Aws.source.access_id)
 spark.conf.set("spark.hadoop.fs.s3a.secret.key", Aws.source.secret_access_key)
 spark.conf.set("spark.hadoop.fs.s3a.impl","org.apache.hadoop.fs.s3a.S3AFileSystem")
 almaren.builder
 .sourceFile("$first(templateData.table_metadata_output).file_format$","$([templateData.input_data.source,"_source"])(templateData)$",$file_options(first(templateData.table_metadata_output))$).sql("""$(["destination_transformation_query_for_",templateData.input_data.metadata_category])(templateData)$""".stripMargin).targetJdbc($([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$,"$first(templateData.table_metadata_output).destination_schema_name$.$first(templateData.table_metadata_output).destination_table_name$",SaveMode.Overwrite,Some(Ldap.target.username),Some(Ldap.target.password),Map()).batch;
} match {
   case Success(s) =>
     logger.info(s"Success \${inputJson.tableId}")
     checkpoint.endDatamovementWithSuccess(inputJson.tableId, verificationStatus = true)

   case Failure(f) =>
     logger.error(s"Failed \${inputJson.tableId}")
     logger.error(s"Error while ingestion", f)
     checkpoint.endDatamovementWithError(inputJson.tableId, f.getLocalizedMessage, verificationStatus = false)
     throw f
 } } match {
    case Success(s) => {
      logger.info(s"Ingestion Success")

        }
        case Failure (f) => {
        logger.error (s"Error while ingestion", f)
        throw f
    }
 }

>>

generate_almaren_artifact_file_default_redshift(templateData)::=<<
import scala.sys.process._
import scala.util.Try
import scala.util.{Failure, Success}
import org.apache.log4j.{Level, Logger, LogManager}

val logger = LogManager.getLogger("com.Artifact")
logger.setLevel(Level.INFO)

Try {
import com.modak.common.token.Token
import com.modak.checkpoint.Checkpoint
import com.modak.common.Constants

val inputJson = Token.parsedInputJson
val checkpoint = Checkpoint(inputJson.processId,inputJson.dataplaceId, None, None,  Some(inputJson.datamovementId))
checkpoint.startDataMovement(inputJson.tableId)

Try {
 import com.github.music.of.the.ainur.almaren.builder.Core.Implicit
 import com.github.music.of.the.ainur.almaren.Almaren
 import com.modak.common.token.Token.Ldap
 import com.modak.encryption.MessageEncryptionUtil
 import com.modak.common.Util._
 import org.apache.spark.sql.SaveMode
 import com.modak.common.token.Token.Aws

 val almaren = Almaren("nabu-sparkbot-ingestion") ;
 val spark = almaren.spark.getOrCreate() ;
 spark.conf.set("spark.hadoop.fs.s3a.endpoint","")
 spark.conf.set("spark.hadoop.fs.s3a.access.key", Aws.source.access_id)
 spark.conf.set("spark.hadoop.fs.s3a.secret.key", Aws.source.secret_access_key)
 spark.conf.set("spark.hadoop.fs.s3a.impl","org.apache.hadoop.fs.s3a.S3AFileSystem")
 almaren.builder
 .sourceFile("$first(templateData.table_metadata_output).file_format$","$([templateData.input_data.source,"_source"])(templateData)$",$file_options(first(templateData.table_metadata_output))$).sql("""$(["destination_transformation_query_for_",templateData.input_data.metadata_category])(templateData)$""".stripMargin).targetJdbc($([first(templateData.table_metadata_output).destination_type,"_target_destination"])(templateData)$,"$first(templateData.table_metadata_output).destination_schema_name$.$first(templateData.table_metadata_output).destination_table_name$",SaveMode.Overwrite,Some(Ldap.target.username),Some(Ldap.target.password),Map()).batch;
} match {
   case Success(s) =>
     logger.info(s"Success \${inputJson.tableId}")
     checkpoint.endDatamovementWithSuccess(inputJson.tableId, verificationStatus = true)

   case Failure(f) =>
     logger.error(s"Failed \${inputJson.tableId}")
     logger.error(s"Error while ingestion", f)
     checkpoint.endDatamovementWithError(inputJson.tableId, f.getLocalizedMessage, verificationStatus = false)
     throw f
 } } match {
    case Success(s) => {
      logger.info(s"Ingestion Success")

        }
        case Failure (f) => {
        logger.error (s"Error while ingestion", f)
        throw f
    }
 }

>>



generate_almaren_artifact_file_redshift(templateData)::=<<
import scala.sys.process._
import scala.util.Try
import scala.util.{Failure, Success}
import org.apache.log4j.{Level, Logger, LogManager}

val logger = LogManager.getLogger("com.Artifact")
logger.setLevel(Level.INFO)

Try {
import com.modak.common.token.Token
import com.modak.checkpoint.Checkpoint
import com.modak.common.Constants

val inputJson = Token.parsedInputJson
val checkpoint = Checkpoint(inputJson.processId,inputJson.dataplaceId, None, None,  Some(inputJson.datamovementId))
checkpoint.startDataMovement(inputJson.tableId)

Try {
 import com.github.music.of.the.ainur.almaren.builder.Core.Implicit
 import com.github.music.of.the.ainur.almaren.Almaren
 import com.modak.common.token.Token.AzureGen2
 import com.modak.encryption.MessageEncryptionUtil
 import com.modak.common.Util._
 import com.modak.common.token.Token.Aws
 import com.modak.common.token.Token.Gcp

 val almaren = Almaren("nabu-sparkbot-ingestion") ;
 val spark = almaren.spark.getOrCreate() ;
 $(["Configuration_",templateData.input_data.source])(templateData)$
 spark.conf.set("spark.hadoop.fs.s3a.access.key", Aws.target.access_id)
 spark.conf.set("spark.hadoop.fs.s3a.secret.key", Aws.target.secret_access_key)
 almaren.builder
 .sourceFile("$first(templateData.table_metadata_output).file_format$","$([templateData.input_data.source,"_source"])(templateData)$",$file_options(first(templateData.table_metadata_output))$)
 .sql("""$(["destination_transformation_query_for_",templateData.input_data.metadata_category])(templateData)$""".stripMargin)
 .batch.write.format("$templateData.input_data.intermediate_type$").mode("overwrite").$([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$;

 } match {
   case Success(s) =>
     logger.info(s"Success \${inputJson.tableId}")
     checkpoint.endDatamovementWithSuccess(inputJson.tableId, verificationStatus = true)

   case Failure(f) =>
     logger.error(s"Failed \${inputJson.tableId}")
     logger.error(s"Error while ingestion", f)
     checkpoint.endDatamovementWithError(inputJson.tableId, f.getLocalizedMessage, verificationStatus = false)
     throw f
 } } match {
    case Success(s) => {
      logger.info(s"Ingestion Success")

        }
        case Failure (f) => {
        logger.error (s"Error while ingestion", f)
        throw f
    }
 }

>>

generate_almaren_artifact_file_azure_synapse(templateData)::=<<
import scala.sys.process._
import scala.util.Try
import scala.util.{Failure, Success}
import org.apache.log4j.{Level, Logger, LogManager}

val logger = LogManager.getLogger("com.Artifact")
logger.setLevel(Level.INFO)

Try {
import com.modak.common.token.Token
import com.modak.checkpoint.Checkpoint
import com.modak.common.Constants

val inputJson = Token.parsedInputJson
val checkpoint = Checkpoint(inputJson.processId,inputJson.dataplaceId, None, None,  Some(inputJson.datamovementId))
checkpoint.startDataMovement(inputJson.tableId)

Try {
 import com.github.music.of.the.ainur.almaren.builder.Core.Implicit
 import com.github.music.of.the.ainur.almaren.Almaren
 import com.modak.common.token.Token.AzureGen2
 import com.modak.common.token.Token.AzureOAuth
 import com.modak.encryption.MessageEncryptionUtil
 import com.modak.common.Util._
 import com.modak.common.token.Token.Aws
 import com.modak.common.token.Token.Gcp


 val almaren = Almaren("nabu-sparkbot-ingestion") ;
 val spark = almaren.spark.getOrCreate() ;
 $(["Configuration_",templateData.input_data.source])(templateData)$
val df = almaren.builder
 .sourceFile("$first(templateData.table_metadata_output).file_format$","$([templateData.input_data.source,"_source"])(templateData)$",$file_options(first(templateData.table_metadata_output))$)
 .sql("""$(["destination_transformation_query_for_",templateData.input_data.metadata_category])(templateData)$""".stripMargin).batch;

 $if(first(templateData.table_metadata_output).is_azure_oauth)$
	spark.conf.set(s"fs.azure.account.auth.type.\${AzureOAuth.target.accountname}.dfs.core.windows.net", "OAuth")
	spark.conf.set(s"fs.azure.account.oauth.provider.type.\${AzureOAuth.target.accountname}.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
	spark.conf.set(s"fs.azure.account.oauth2.client.id.\${AzureOAuth.target.accountname}.dfs.core.windows.net", s"\${AzureOAuth.target.client_id}")
	spark.conf.set(s"fs.azure.account.oauth2.client.secret.\${AzureOAuth.target.accountname}.dfs.core.windows.net", s"\${AzureOAuth.target.client_secret}")
	spark.conf.set(s"fs.azure.account.oauth2.client.endpoint.\${AzureOAuth.target.accountname}.dfs.core.windows.net", s"https://login.microsoftonline.com/\${AzureOAuth.target.tenant_id}/oauth2/token")
 $else$
	spark.conf.set(s"fs.azure.account.key.\${AzureGen2.target.accountname}.dfs.core.windows.net", AzureGen2.target.accountkey)
 $endif$

 df.write.format("$templateData.input_data.intermediate_type$").mode("overwrite").$([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$;

 } match {
   case Success(s) =>
     logger.info(s"Success \${inputJson.tableId}")
     checkpoint.endDatamovementWithSuccess(inputJson.tableId, verificationStatus = true)

   case Failure(f) =>
     logger.error(s"Failed \${inputJson.tableId}")
     logger.error(s"Error while ingestion", f)
     checkpoint.endDatamovementWithError(inputJson.tableId, f.getLocalizedMessage, verificationStatus = false)
     throw f
 } } match {
    case Success(s) => {
      logger.info(s"Ingestion Success")

        }
        case Failure (f) => {
        logger.error (s"Error while ingestion", f)
        throw f
    }
 }

>>

Configuration_s3(templateData)::=<<
spark.conf.set("spark.hadoop.fs.s3a.access.key", Aws.source.access_id)
spark.conf.set("spark.hadoop.fs.s3a.secret.key", Aws.source.secret_access_key)
>>

Configuration_adls_gen2(templateData)::=<<
 $if(first(templateData.table_metadata_output).is_source_azure_oauth)$
	spark.conf.set(s"fs.azure.account.auth.type.\${AzureOAuth.source.accountname}.dfs.core.windows.net", "OAuth")
	spark.conf.set(s"fs.azure.account.oauth.provider.type.\${AzureOAuth.source.accountname}.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
	spark.conf.set(s"fs.azure.account.oauth2.client.id.\${AzureOAuth.source.accountname}.dfs.core.windows.net", s"\${AzureOAuth.source.client_id}")
	spark.conf.set(s"fs.azure.account.oauth2.client.secret.\${AzureOAuth.source.accountname}.dfs.core.windows.net", s"\${AzureOAuth.source.client_secret}")
	spark.conf.set(s"fs.azure.account.oauth2.client.endpoint.\${AzureOAuth.source.accountname}.dfs.core.windows.net", s"https://login.microsoftonline.com/\${AzureOAuth.source.tenant_id}/oauth2/token")
 $else$
	spark.conf.set(s"fs.azure.account.key.\${AzureGen2.source.accountname}.dfs.core.windows.net", AzureGen2.source.accountkey)
 $endif$
>>

Configuration_gcs(templateData)::=<<
import io.circe.generic.auto._
import io.circe.syntax._

spark.conf.set("credentials", Util.encodeBase64(Gcp.source.asJson.noSpaces))
spark.conf.set("spark.hadoop.fs.gs.impl","com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem")
spark.conf.set("spark.hadoop.fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS")
spark.conf.set("spark.hadoop.fs.gs.auth.service.account.email", Gcp.source.client_email)
spark.conf.set("spark.hadoop.fs.gs.auth.service.account.private.key.id", Gcp.source.private_key_id)
spark.conf.set("spark.hadoop.fs.gs.auth.service.account.private.key", Gcp.source.private_key)
>>

Configuration_unix(templateData)::=<<

>>

file_options(templateData)::=<<
Map("header" -> "$templateData.has_header$", "delimiter" -> "$templateData.delimiter$", "quote" -> "\\$templateData.escape_char$","escape" -> "\\$templateData.escape_char$","lineSep" -> "\\r\\n")
>>

s3_source(templateData)::=<<
s3a://$first(templateData.table_metadata_output).source_s3_bucket$/$first(templateData.table_metadata_output).file_relative_path$
>>

adls_gen2_source(templateData)::=<<
abfss://$first(templateData.table_metadata_output).source_adls_gen2_container$@$first(templateData.table_metadata_output).source_storage_account$.dfs.core.windows.net/$first(templateData.table_metadata_output).file_relative_path$
>>

unix_source(templateData)::=<<
file://$first(templateData.table_metadata_output).file_absolute_path$
>>

gcs_source(templateData)::=<<
$first(templateData.table_metadata_output).file_absolute_path$
>>

file_azure_synapse_destination(templateData)::=<<
option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).adls_gen2_container$@$first(templateData.table_metadata_output).storage_account$.dfs.core.windows.net/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).destination_table_name$.parquet").save()
>>

generate_almaren_artifact_hive(templateData)::=<<
$if(first(templateData.table_metadata_output).is_source_spark_hive)$
$generate_almaren_artifact_hive_cluster(templateData)$
$else$
$generate_almaren_artifact_hive_jdbc(templateData)$
$endif$
>>

generate_almaren_artifact_hive_jdbc(templateData)::=<<
import scala.sys.process._
import scala.util.Try
import scala.util.{Failure, Success}
import org.apache.log4j.{Level, Logger, LogManager}

val logger = LogManager.getLogger("com.Artifact")
logger.setLevel(Level.INFO)

Try {
import com.modak.common.token.Token
import com.modak.checkpoint.Checkpoint
import com.modak.common.Constants

val inputJson = Token.parsedInputJson
val checkpoint = Checkpoint(inputJson.processId,inputJson.dataplaceId, None, None,  Some(inputJson.datamovementId))
checkpoint.startDataMovement(inputJson.tableId)
Try {
  import com.github.music.of.the.ainur.almaren.builder.Core.Implicit;
  import com.github.music.of.the.ainur.almaren.Almaren;
  import com.modak.common.token.Token.Ldap
  import com.modak.common.token.Token.Aws
  import com.modak.encryption.MessageEncryptionUtil
  import com.modak.common.Util._

  val almaren = Almaren("nabu-sparkbot-ingestion") ;
  val spark = almaren.spark.getOrCreate() ;
  $(["Configuration_",first(templateData.table_metadata_output).file_system_type])(templateData)$

  almaren.builder
   .sourceJdbc($([first(templateData.table_metadata_output).source_type,"_source"])(templateData)$,
     """$([templateData.input_data.source,"_jdbcQuery"])(templateData)$""".stripMargin,
     Some(Ldap.source.username),Some(Ldap.source.password),Map("fetchsize" -> Constants.DB.fetchSize))
     .sql(
     """$destination_transformation_query(templateData)$""".stripMargin
   ).batch.write.format("$templateData.input_data.intermediate_type$").mode("overwrite").$([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$;
 } match {
   case Success(s) =>
     logger.info(s"Success \${inputJson.tableId}")
     checkpoint.endDatamovementWithSuccess(inputJson.tableId, verificationStatus = true)

   case Failure(f) =>
     logger.error(s"Failed \${inputJson.tableId}")
     logger.error(s"Error while ingestion", f)
     checkpoint.endDatamovementWithError(inputJson.tableId, f.getLocalizedMessage, verificationStatus = false)
     throw f
 } } match {
    case Success(s) => {
      logger.info(s"Ingestion Success")

        }
        case Failure (f) => {
        logger.error (s"Error while ingestion", f)
        throw f
    }
 }
>>

generate_almaren_artifact_hive_cluster(templateData)::=<<
import scala.sys.process._
import scala.util.Try
import scala.util.{Failure, Success}
import org.apache.log4j.{Level, Logger, LogManager}

val logger = LogManager.getLogger("com.Artifact")
logger.setLevel(Level.INFO)

Try {
import com.modak.common.token.Token
import com.modak.checkpoint.Checkpoint
import com.modak.common.Constants

val inputJson = Token.parsedInputJson
val checkpoint = Checkpoint(inputJson.processId,inputJson.dataplaceId, None, None,  Some(inputJson.datamovementId))
checkpoint.startDataMovement(inputJson.tableId)

Try {
  import com.github.music.of.the.ainur.almaren.builder.Core.Implicit;
  import com.github.music.of.the.ainur.almaren.Almaren;
  import com.modak.common.token.Token.Ldap
  import com.modak.common.token.Token.Aws
  import com.modak.encryption.MessageEncryptionUtil
  import com.modak.common.Util._

  val almaren = Almaren("nabu-sparkbot-ingestion") ;
  val spark = almaren.spark.getOrCreate() ;

   $if((first(templateData.table_metadata_output).is_file_system_type_s3a) && (first(templateData.table_metadata_output).is_filesystem_credential_type_aws))$
   $Configuration_s3a(templateData)$
   $endif$

  almaren.builder
   .sourceSql("""$([templateData.input_data.source,"_clusterQuery"])(templateData)$""".stripMargin)
     .sql(
     """$destination_transformation_query(templateData)$""".stripMargin
   ).batch.write.format("$templateData.input_data.intermediate_type$").mode("overwrite").$([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$;
 } match {
   case Success(s) =>
     logger.info(s"Success \${inputJson.tableId}")
     checkpoint.endDatamovementWithSuccess(inputJson.tableId, verificationStatus = true)

   case Failure(f) =>
     logger.error(s"Failed \${inputJson.tableId}")
     logger.error(s"Error while ingestion", f)
     checkpoint.endDatamovementWithError(inputJson.tableId, f.getLocalizedMessage, verificationStatus = false)
     throw f
 } } match {
    case Success(s) => {
      logger.info(s"Ingestion Success")

        }
        case Failure (f) => {
        logger.error (s"Error while ingestion", f)
        throw f
    }
 }
>>

generate_almaren_artifact_hdfs(templateData)::=<<
import scala.sys.process._
import scala.util.Try
import scala.util.{Failure, Success}
import org.apache.log4j.{Level, Logger, LogManager}

val logger = LogManager.getLogger("com.Artifact")
logger.setLevel(Level.INFO)

Try {
import com.modak.common.token.Token
import com.modak.checkpoint.Checkpoint
import com.modak.common.Constants

val inputJson = Token.parsedInputJson
val checkpoint = Checkpoint(inputJson.processId,inputJson.dataplaceId, None, None,  Some(inputJson.datamovementId))
checkpoint.startDataMovement(inputJson.tableId)

Try {
 import com.github.music.of.the.ainur.almaren.builder.Core.Implicit;
 import com.github.music.of.the.ainur.almaren.Almaren;
 import com.modak.common.token.Token.Ldap
 import com.modak.encryption.MessageEncryptionUtil
 import com.modak.common.Util._
 val almaren = Almaren("nabu-sparkbot-ingestion") ;
 val spark = almaren.spark.getOrCreate() ;
 almaren.builder
   .sourceJdbc($([first(templateData.table_metadata_output).source_type,"_source"])(templateData)$,
     """$([templateData.input_data.source,"_jdbcQuery"])(templateData)$""",
     Some(Ldap.source.username),Some(Ldap.source.password),Map("fetchsize" -> Constants.DB.fetchSize))
   .batch.write.format("$templateData.input_data.intermediate_type$").mode("overwrite").$([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$;
 } match {
   case Success(s) =>
     logger.info(s"Success \${inputJson.tableId}")
     checkpoint.endDatamovementWithSuccess(inputJson.tableId, verificationStatus = true)

   case Failure(f) =>
     logger.error(s"Failed \${inputJson.tableId}")
     logger.error(s"Error while ingestion", f)
     checkpoint.endDatamovementWithError(inputJson.tableId, f.getLocalizedMessage, verificationStatus = false)
     throw f
}}
match {
    case Success(s) => {
      logger.info(s"Ingestion Success")

}
        case Failure (f) => {
        logger.error (s"Error while ingestion", f)
        throw f
    }
 }


>>

generate_almaren_artifact_redshift(templateData)::=<<
import scala.sys.process._
import scala.util.Try
import scala.util.{Failure, Success}
import org.apache.log4j.{Level, Logger, LogManager}

val logger = LogManager.getLogger("com.Artifact")
logger.setLevel(Level.INFO)

Try {
  import com.modak.common.token.Token
  import com.modak.checkpoint.Checkpoint
  import com.modak.common.Constants

  val inputJson = Token.parsedInputJson
  val checkpoint = Checkpoint(inputJson.processId, inputJson.dataplaceId, None, None, Some(inputJson.datamovementId))
  checkpoint.startDataMovement(inputJson.tableId)
  Try {
    import com.github.music.of.the.ainur.almaren.builder.Core.Implicit;
    import com.github.music.of.the.ainur.almaren.Almaren;
    import com.modak.common.token.Token.Ldap
    import com.modak.common.token.Token.Aws
    import com.modak.encryption.MessageEncryptionUtil
    import com.modak.common.Util._

    val almaren = Almaren("nabu-sparkbot-ingestion");
    val spark = almaren.spark.getOrCreate() ;
    spark.conf.set("spark.hadoop.fs.s3a.access.key", Aws.target.access_id)
    spark.conf.set("spark.hadoop.fs.s3a.secret.key", Aws.target.secret_access_key)

    almaren.builder
      .sourceJdbc($([first(templateData.table_metadata_output).source_type,"_source"])(templateData)$,
        """$([templateData.input_data.source,"_jdbcQuery"])(templateData)$""".stripMargin,
        Some(Ldap.source.username),Some(Ldap.source.password),Map("fetchsize" -> Constants.DB.fetchSize))
      .sql(
        """$destination_transformation_query(templateData)$""".stripMargin
      ).batch.write.format("$templateData.input_data.intermediate_type$").mode("overwrite").$([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$;
  } match {
    case Success(s) =>
      logger.info(s"Success \${inputJson.tableId}")
      checkpoint.endDatamovementWithSuccess(inputJson.tableId, verificationStatus = true)

    case Failure(f) =>
      logger.error(s"Failed \${inputJson.tableId}")
      logger.error(s"Error while ingestion", f)
      checkpoint.endDatamovementWithError(inputJson.tableId, f.getLocalizedMessage, verificationStatus = false)
      throw f
  }
} match {
  case Success(s) => {
    logger.info(s"Ingestion Success")

  }
  case Failure(f) => {
    logger.error(s"Error while ingestion", f)
    throw f
  }
}

>>

generate_almaren_artifact_s3(templateData)::=<<
import scala.sys.process._
import scala.util.Try
import scala.util.{Failure, Success}
import org.apache.log4j.{Level, Logger, LogManager}

val logger = LogManager.getLogger("com.Artifact")
logger.setLevel(Level.INFO)

Try {
import com.modak.common.token.Token
import com.modak.checkpoint.Checkpoint
import com.modak.common.Constants

val inputJson = Token.parsedInputJson
val checkpoint = Checkpoint(inputJson.processId,inputJson.dataplaceId, None, None,  Some(inputJson.datamovementId))
checkpoint.startDataMovement(inputJson.tableId)

Try {
 import com.github.music.of.the.ainur.almaren.builder.Core.Implicit
 import com.github.music.of.the.ainur.almaren.Almaren
 import com.modak.common.token.Token
 import com.modak.common.token.Token.Ldap
 import com.modak.common.token.Token.Aws
 import com.modak.encryption.MessageEncryptionUtil
 import com.modak.common.Util._


 val almaren = Almaren("nabu-sparkbot-ingestion") ;
 val spark = almaren.spark.getOrCreate() ;
 spark.conf.set("spark.hadoop.fs.s3a.access.key", Aws.target.access_id)
 spark.conf.set("spark.hadoop.fs.s3a.secret.key", Aws.target.secret_access_key)

 almaren.builder
   .sourceJdbc($([first(templateData.table_metadata_output).source_type,"_source"])(templateData)$,
     """$([templateData.input_data.source,"_jdbcQuery"])(templateData)$""".stripMargin,
     Some(Ldap.source.username),Some(Ldap.source.password),Map("fetchsize" -> Constants.DB.fetchSize))
   .batch.write.format("$templateData.input_data.intermediate_type$").mode("overwrite").$([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$;
 } match {
   case Success(s) =>
     logger.info(s"Success \${inputJson.tableId}")
     checkpoint.endDatamovementWithSuccess(inputJson.tableId, verificationStatus = true)

   case Failure(f) =>
     logger.error(s"Failed \${inputJson.tableId}")
     logger.error(s"Error while ingestion", f)
     checkpoint.endDatamovementWithError(inputJson.tableId, f.getLocalizedMessage, verificationStatus = false)
     throw f
 } } match {
    case Success(s) => {
      logger.info(s"Ingestion Success")

        }
        case Failure (f) => {
        logger.error (s"Error while ingestion", f)
        throw f
    }
 }
>>

generate_almaren_artifact_bigquery(templateData)::=<<
import scala.sys.process._
import scala.util.Try
import io.circe.generic.auto._
import io.circe.syntax._
import com.modak.common.token.Token
import scala.util.{Failure, Success}
import com.modak.checkpoint.Checkpoint
import org.apache.log4j.{Level, Logger, LogManager}
import com.modak.common.Constants

val logger = LogManager.getLogger("com.Artifact")
logger.setLevel(Level.INFO)

Try {

val inputJson = Token.parsedInputJson
val checkpoint = Checkpoint(inputJson.processId,inputJson.dataplaceId, None, None,  Some(inputJson.datamovementId))
checkpoint.startDataMovement(inputJson.tableId)
Try {
 import com.github.music.of.the.ainur.almaren.builder.Core.Implicit
 import com.github.music.of.the.ainur.almaren.Almaren
 import com.modak.common.token.Token
 import com.modak.common.token.Token.Ldap
 import com.modak.common.token.Token.Gcp
 import com.modak.encryption.MessageEncryptionUtil
 import com.modak.common.Util._

 val almaren = Almaren("nabu-sparkbot-ingestion");
 val spark = almaren.spark.getOrCreate() ;

 spark.conf.set("credentials", Util.encodeBase64(Gcp.target.asJson.noSpaces))
 spark.conf.set("spark.hadoop.fs.gs.impl","com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem")
 spark.conf.set("spark.hadoop.fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS")
 spark.conf.set("spark.hadoop.fs.gs.auth.service.account.email",Gcp.target.client_email)
 spark.conf.set("spark.hadoop.fs.gs.auth.service.account.private.key.id",Gcp.target.private_key_id)
 spark.conf.set("spark.hadoop.fs.gs.auth.service.account.private.key",Gcp.target.private_key)

 almaren.builder
   .sourceJdbc($([first(templateData.table_metadata_output).source_type,"_source"])(templateData)$,
     """$([first(templateData.table_metadata_output).source_type,"_jdbcQuery"])(templateData)$""".stripMargin,
     Some(Ldap.source.username),Some(Ldap.source.password),Map("fetchsize" -> Constants.DB.fetchSize))
   .sql(
     """$destination_transformation_query(templateData)$""".stripMargin
   ).batch.write.format("$templateData.input_data.intermediate_type$").mode("overwrite").$([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$;
 } match {
   case Success(s) =>
     logger.info(s"Success \${inputJson.tableId}")
     checkpoint.endDatamovementWithSuccess(inputJson.tableId, verificationStatus = true)
   case Failure(f) =>
     logger.error(s"Failed \${inputJson.tableId}")
     logger.error(s"Error while ingestion", f)
     checkpoint.endDatamovementWithError(inputJson.tableId, f.getLocalizedMessage, verificationStatus = false)
     throw f
 }}match {
    case Success(s) => {
      logger.info(s"Ingestion Success")

        }
        case Failure (f) => {
        logger.error (s"Error while ingestion", f)
        throw f
    }
 }

>>


generate_almaren_artifact_azure_synapse(templateData)::=<<
import scala.sys.process._
import scala.util.Try
import scala.util.{Failure, Success}
import org.apache.log4j.{Level, Logger, LogManager}

val logger = LogManager.getLogger("com.Artifact")
logger.setLevel(Level.INFO)

Try {
  import com.modak.common.token.Token
  import com.modak.checkpoint.Checkpoint
  import com.modak.common.Constants

  val inputJson = Token.parsedInputJson
  val checkpoint = Checkpoint(inputJson.processId, inputJson.dataplaceId, None, None, Some(inputJson.datamovementId))
  checkpoint.startDataMovement(inputJson.tableId)
  Try {
    import com.github.music.of.the.ainur.almaren.builder.Core.Implicit;
    import com.github.music.of.the.ainur.almaren.Almaren;
    import com.modak.common.token.Token.Ldap
    import com.modak.common.token.Token.AzureGen2
	import com.modak.common.token.Token.AzureOAuth
    import com.modak.encryption.MessageEncryptionUtil
    import com.modak.common.Util._

    val almaren = Almaren("nabu-sparkbot-ingestion");
    val spark = almaren.spark.getOrCreate() ;

    $if(first(templateData.table_metadata_output).is_azure_oauth)$
        spark.conf.set(s"fs.azure.account.auth.type.\${AzureOAuth.target.accountname}.dfs.core.windows.net", "OAuth")
        spark.conf.set(s"fs.azure.account.oauth.provider.type.\${AzureOAuth.target.accountname}.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
        spark.conf.set(s"fs.azure.account.oauth2.client.id.\${AzureOAuth.target.accountname}.dfs.core.windows.net", s"\${AzureOAuth.target.client_id}")
        spark.conf.set(s"fs.azure.account.oauth2.client.secret.\${AzureOAuth.target.accountname}.dfs.core.windows.net", s"\${AzureOAuth.target.client_secret}")
        spark.conf.set(s"fs.azure.account.oauth2.client.endpoint.\${AzureOAuth.target.accountname}.dfs.core.windows.net", s"https://login.microsoftonline.com/\${AzureOAuth.target.tenant_id}/oauth2/token")
    $else$
        spark.conf.set(s"fs.azure.account.key.\${AzureGen2.target.accountname}.dfs.core.windows.net", AzureGen2.target.accountkey)
    $endif$



    almaren.builder
      .sourceJdbc($([first(templateData.table_metadata_output).source_type,"_source"])(templateData)$,
        """$([templateData.input_data.source,"_jdbcQuery"])(templateData)$""".stripMargin,
        Some(Ldap.source.username),Some(Ldap.source.password),Map("fetchsize" -> Constants.DB.fetchSize))
      .sql(
        """$destination_transformation_query(templateData)$""".stripMargin
      ).batch.write.format("$templateData.input_data.intermediate_type$").mode("overwrite").$([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$;
  } match {
    case Success(s) =>
      logger.info(s"Success \${inputJson.tableId}")
      checkpoint.endDatamovementWithSuccess(inputJson.tableId, verificationStatus = true)

    case Failure(f) =>
      logger.error(s"Failed \${inputJson.tableId}")
      logger.error(s"Error while ingestion", f)
      checkpoint.endDatamovementWithError(inputJson.tableId, f.getLocalizedMessage, verificationStatus = false)
      throw f
  }
} match {
  case Success(s) => {
    logger.info(s"Ingestion Success")

  }
  case Failure(f) => {
    logger.error(s"Error while ingestion", f)
    throw f
  }
}

>>

generate_almaren_artifact_adls_gen2(templateData)::=<<
import scala.sys.process._
import scala.util.Try
import scala.util.{Failure, Success}
import org.apache.log4j.{Level, Logger, LogManager}

val logger = LogManager.getLogger("com.Artifact")
logger.setLevel(Level.INFO)

Try {
  import com.modak.common.token.Token
  import com.modak.checkpoint.Checkpoint
  import com.modak.common.Constants

  val inputJson = Token.parsedInputJson
  val checkpoint = Checkpoint(inputJson.processId, inputJson.dataplaceId, None, None, Some(inputJson.datamovementId))
  checkpoint.startDataMovement(inputJson.tableId)
  Try {
 import com.github.music.of.the.ainur.almaren.builder.Core.Implicit
 import com.github.music.of.the.ainur.almaren.Almaren
 import com.modak.common.token.Token
    import com.modak.common.token.Token.Ldap
    import com.modak.common.token.Token
    import com.modak.common.token.Token.AzureOAuth
    import com.modak.common.token.Token.AzureGen2
    import com.modak.encryption.MessageEncryptionUtil
    import com.modak.common.Util._

    val almaren = Almaren("nabu-sparkbot-ingestion");
    val spark = almaren.spark.getOrCreate() ;

    $if(first(templateData.table_metadata_output).is_azure_oauth)$
        spark.conf.set(s"fs.azure.account.auth.type.\${AzureOAuth.target.accountname}.dfs.core.windows.net", "OAuth")
        spark.conf.set(s"fs.azure.account.oauth.provider.type.\${AzureOAuth.target.accountname}.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
        spark.conf.set(s"fs.azure.account.oauth2.client.id.\${AzureOAuth.target.accountname}.dfs.core.windows.net", s"\${AzureOAuth.target.client_id}")
        spark.conf.set(s"fs.azure.account.oauth2.client.secret.\${AzureOAuth.target.accountname}.dfs.core.windows.net", s"\${AzureOAuth.target.client_secret}")
        spark.conf.set(s"fs.azure.account.oauth2.client.endpoint.\${AzureOAuth.target.accountname}.dfs.core.windows.net", s"https://login.microsoftonline.com/\${AzureOAuth.target.tenant_id}/oauth2/token")
    $else$
        spark.conf.set(s"fs.azure.account.key.\${AzureGen2.target.accountname}.dfs.core.windows.net", AzureGen2.target.accountkey)
    $endif$
    almaren.builder
      .sourceJdbc($([first(templateData.table_metadata_output).source_type,"_source"])(templateData)$,
        """$([templateData.input_data.source,"_jdbcQuery"])(templateData)$""".stripMargin,
        Some(Ldap.source.username),Some(Ldap.source.password),Map("fetchsize" -> Constants.DB.fetchSize))
      .batch.write.format("$templateData.input_data.intermediate_type$").mode("overwrite").$([first(templateData.table_metadata_output).destination_type,"_destination_YYYY_MM_DD_HH_mm_SS_format"])(templateData)$;
  } match {
    case Success(s) =>
      logger.info(s"Success \${inputJson.tableId}")
      checkpoint.endDatamovementWithSuccess(inputJson.tableId, verificationStatus = true)

    case Failure(f) =>
      logger.error(s"Failed \${inputJson.tableId}")
      logger.error(s"Error while ingestion", f)
      checkpoint.endDatamovementWithError(inputJson.tableId, f.getLocalizedMessage, verificationStatus = false)
     throw f
 } } match {
  case Success(s) => {
    logger.info(s"Ingestion Success")

  }
  case Failure(f) => {
    logger.error(s"Error while ingestion", f)
        throw f
  }
}

>>

hive_source(templateData)::=<<
"$first(templateData.table_metadata_output).source_url$/$first(templateData.table_metadata_output).source_db$","$first(templateData.table_metadata_output).source_jdbc_driver$"
>>

oracle_source(templateData)::=<<
"jdbc:oracle:thin:@$first(templateData.table_metadata_output).source_url$:$first(templateData.table_metadata_output).source_db$","$first(templateData.table_metadata_output).source_jdbc_driver$"
>>

mysql_source(templateData)::=<<
"jdbc:mysql://$first(templateData.table_metadata_output).source_url$/$first(templateData.table_metadata_output).source_db$","$first(templateData.table_metadata_output).source_jdbc_driver$"
>>

postgres_source(templateData)::=<<
"jdbc:postgresql://$first(templateData.table_metadata_output).source_url$/$first(templateData.table_metadata_output).source_db$","$first(templateData.table_metadata_output).source_jdbc_driver$"
>>

redshift_source(templateData)::=<<
"jdbc:redshift://$first(templateData.table_metadata_output).source_url$/$first(templateData.table_metadata_output).source_db$","$first(templateData.table_metadata_output).source_jdbc_driver$"
>>

sql_server_source(templateData)::=<<
"jdbc:jtds:sqlserver://$first(templateData.table_metadata_output).source_url$;databaseName=$first(templateData.table_metadata_output).source_db$","$first(templateData.table_metadata_output).source_jdbc_driver$"
>>

hive_destination(templateData)::=<<
$if(first(templateData.table_metadata_output).source_db)$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).path$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).destination_table_name$").save()$else$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).path$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).source_dataplace_id$/$first(templateData.table_metadata_output).directory_name$/$first(templateData.table_metadata_output).destination_table_name$").save()$endif$
>>


hdfs_destination(templateData)::=<<
$if(first(templateData.table_metadata_output).source_db)$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).path$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).table_name$").save()$else$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).path$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).source_dataplace_id$/$first(templateData.table_metadata_output).directory_name$/$first(templateData.table_metadata_output).table_name$").save()$endif$
>>

bigquery_destination(templateData)::=<<
$if(first(templateData.table_metadata_output).source_db)$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$/$first(templateData.table_metadata_output).gcs_bucket$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).destination_table_name$").save()$else$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$/$first(templateData.table_metadata_output).gcs_bucket$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).directory_name$/$first(templateData.table_metadata_output).destination_table_name$").save()$endif$
>>

redshift_destination(templateData)::=<<
$if(first(templateData.table_metadata_output).source_db)$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$/$first(templateData.table_metadata_output).s3_bucket$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).destination_table_name$").save()$else$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$/$first(templateData.table_metadata_output).s3_bucket$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).source_dataplace_id$/$first(templateData.table_metadata_output).directory_name$/$first(templateData.table_metadata_output).destination_table_name$").save()$endif$
>>

s3_destination(templateData)::=<<
$if(first(templateData.table_metadata_output).source_db)$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$/$first(templateData.table_metadata_output).s3_bucket$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).table_name$").save()$else$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$/$first(templateData.table_metadata_output).s3_bucket$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).source_dataplace_id$/$first(templateData.table_metadata_output).directory_name$/$first(templateData.table_metadata_output).table_name$").save()$endif$
>>

azure_synapse_destination(templateData)::=<<
$if(first(templateData.table_metadata_output).source_db)$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).adls_gen2_container$@$first(templateData.table_metadata_output).storage_account$.dfs.core.windows.net/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).destination_table_name$.parquet").save()$else$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).adls_gen2_container$@$first(templateData.table_metadata_output).storage_account$.dfs.core.windows.net/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).source_dataplace_id$/$first(templateData.table_metadata_output).directory_name$/$first(templateData.table_metadata_output).destination_table_name$.parquet").save()$endif$
>>

adls_gen2_destination(templateData)::=<<
$if(first(templateData.table_metadata_output).source_db)$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).adls_gen2_container$@$first(templateData.table_metadata_output).storage_account$.dfs.core.windows.net/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).destination_table_name$.parquet").save()$else$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).adls_gen2_container$@$first(templateData.table_metadata_output).storage_account$.dfs.core.windows.net/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).source_dataplace_id$/$first(templateData.table_metadata_output).directory_name$/$first(templateData.table_metadata_output).destination_table_name$.parquet").save()$endif$
>>

adls_gen2_destination_YYYY_MM_DD_HH_mm_SS_format(templateData)::=<<
$if(first(templateData.table_metadata_output).source_db)$option("path","$([first(templateData.table_metadata_output)
.destination_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output)
.adls_gen2_container$@$first(templateData.table_metadata_output).storage_account$.dfs.core.windows.net/$first(templateData.table_metadata_output).upper_case_destination_table_name$/$first
(templateData.table_metadata_output).destination_folder_format$/$first(templateData.table_metadata_output)
.destination_table_name$.parquet").save()$else$option("path","$([first(templateData.table_metadata_output)
.file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output)
.adls_gen2_container$@$first(templateData.table_metadata_output).storage_account$.dfs.core.windows.net/$first(templateData.table_metadata_output).upper_case_destination_table_name$/$
(templateData.table_metadata_output).destination_folder_format$/$first(templateData.table_metadata_output).destination_table_name$.parquet").save()$endif$
>>


drop_hive_table(templateData)::=<<
DROP TABLE IF EXISTS $first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.`$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$`
>>


drop_hive_table_if_schema_drift(templateData)::=<<
DROP TABLE IF EXISTS $first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$.`$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$`
>>

drop_redshift_table_if_schema_drift(templateData)::=<<
DROP TABLE IF EXISTS $first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$
>>


create_backup_hive_table(templateData)::=<<
CREATE TABLE IF NOT EXISTS `$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_backup_table_name$`
as SELECT * from `$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$`
>>

create_bigquery_table(templateData)::=<<
CREATE OR REPLACE EXTERNAL TABLE $first(templateData.query_input.metadata.data.query_input.table_metadata_output).
destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$
OPTIONS(
format = 'parquet',
$uris(templateData)$
)
>>

uris(templateData)::=<<
uris=['$([first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_type,"_location"])(templateData)$']
>>

create_bigquery_bkp_table(templateData)::=<<
CREATE OR REPLACE TABLE $templateData.destination_schema_name$.$templateData.destination_backup_table_name$ as select * from $templateData.destination_schema_name$.$templateData.destination_table_name$;
>>

create_bigquery_table_if_schema_drift(templateData)::=<<
CREATE OR REPLACE EXTERNAL TABLE
$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$.
$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$
OPTIONS(
format = 'parquet',
uris=['$([first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_type,"_location_drift"])(templateData)$']

)
>>

uris_drift(templateData)::=<<
uris=['$([first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_type,"_location_drift"])(templateData)$']
>>


drop_redshift_table(templateData)::=<<
DROP TABLE IF EXISTS $first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$

>>

create_redshift_table(templateData)::=<<
$(["create_redshift_table_for_",templateData.query_input.metadata.data.query_input.input_data.metadata_category])(templateData)$
>>

create_redshift_table_for_relational(templateData)::=<<
CREATE EXTERNAL TABLE $first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$
(
$templateData.query_input.metadata.data.query_input.column_metadata_output:getColumnsRedshift();separator = ","$
)
stored as $templateData.query_input.metadata.data.query_input.input_data.intermediate_type$
$location(templateData)$

>>

create_redshift_table_for_file(templateData)::=<<
CREATE EXTERNAL TABLE $first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$
(
$templateData.query_input.metadata.data.query_input.column_metadata_output:getColumnsRedshift();separator = ","$
)
stored as $templateData.query_input.metadata.data.query_input.input_data.intermediate_type$
$location(templateData)$

>>

create_redshift_table_if_schema_drift(templateData)::=<<
$(["create_redshift_table_if_schema_drift_",templateData.query_input.metadata.data.templateData.query_input.input_data.metadata_category])(templateData)$
>>

create_redshift_table_if_schema_drift_relational(templateData)::=<<
CREATE EXTERNAL TABLE $first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$
(
$templateData.query_input.metadata.data.templateData.query_input.column_metadata_output:getColumnsRedshift();separator = ","$
)
stored as $templateData.query_input.metadata.data.templateData.query_input.input_data.intermediate_type$
$location_drift(templateData)$

>>

create_redshift_table_if_schema_drift_file(templateData)::=<<
CREATE EXTERNAL TABLE $first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$
(
$templateData.query_input.metadata.data.templateData.query_input.column_metadata_output:getColumnsRedshift();separator = ","$
)
stored as $templateData.query_input.metadata.data.templateData.query_input.input_data.intermediate_type$
$location_drift(templateData)$

>>

location(templateData)::=<<
LOCATION
  '$([first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_type,"_location"])(templateData)$'
>>

location_nw(templateData)::=<<
LOCATION
  '$([first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_type,"_location"])(templateData)$'
>>

location_drift(templateData)::=<<
LOCATION
  '$([first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_type,"_location_drift"])(templateData)$'
>>

create_hive_table(templateData)::=<<
$(["create_hive_table_for_",templateData.query_input.metadata.data.query_input.input_data.metadata_category])(templateData)$
>>

create_hive_table_for_file(templateData)::=<<
CREATE EXTERNAL TABLE if not exists $first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.`$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$`
(
$templateData.query_input.metadata.data.query_input.column_metadata_output:getColumns();separator = ","$
)
$(templateData.query_input.metadata.data.query_input.input_data.intermediate_type)(templateData)$

>>

create_hive_table_for_relational(templateData)::=<<
$if((first(templateData.query_input.metadata.data.query_input.table_metadata_output).is_source_hive) && (!first(templateData.query_input.metadata.data.query_input.table_metadata_output).is_source_spark_hive))$
$create_hive_table_for_relational_hive_source_jdbc_mode(templateData)$
$else$
$create_hive_table_for_relational_all_sources(templateData)$
$endif$
>>

create_hive_table_for_relational_hive_source_jdbc_mode(templateData)::=<<
CREATE EXTERNAL TABLE if not exists $first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.`$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$`
(
 $templateData.query_input.metadata.data.query_input.column_metadata_output:getColumns();separator = ","$
)
$(templateData.query_input.metadata.data.query_input.input_data.intermediate_type)(templateData)$
>>

create_hive_table_for_relational_all_sources(templateData)::=<<
CREATE EXTERNAL TABLE if not exists $first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.`$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$`
(
 $templateData.query_input.metadata.data.query_input.column_metadata_output:getColumns();separator = ","$
)
$(templateData.query_input.metadata.data.query_input.input_data.intermediate_type)(templateData)$
>>

create_hive_table_if_schema_drift(templateData)::=<<
$(["create_hive_table_if_schema_drift_",templateData.query_input.metadata.data.templateData.query_input.input_data.metadata_category])(templateData)$
>>


create_hive_table_if_schema_drift_relational(templateData)::=<<
$if((first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).is_source_hive) && (!first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).is_source_spark_hive))$
$create_hive_table_if_schema_drift_hive_source_jdbc_mode(templateData)$
$else$
$create_hive_table_if_schema_drift_for_all_sources(templateData)$
$endif$
>>

create_hive_table_if_schema_drift_hive_source_jdbc_mode(templateData)::=<<
CREATE EXTERNAL TABLE if not exists $first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$
(
 $templateData.query_input.metadata.data.templateData.query_input.column_metadata_output:getColumns();separator = ","$
)
$([templateData.query_input.metadata.data.templateData.query_input.input_data.intermediate_type,"_drift"])(templateData)$

>>


create_hive_table_if_schema_drift_for_all_sources(templateData)::=<<
CREATE EXTERNAL TABLE if not exists $first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$
(
 $templateData.query_input.metadata.data.templateData.query_input.column_metadata_output:getColumns();separator = ","$
)
$([templateData.query_input.metadata.data.templateData.query_input.input_data.intermediate_type,"_drift"])(templateData)$

>>

create_hive_table_if_schema_drift_file(templateData)::=<<
CREATE EXTERNAL TABLE if not exists $first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$.`$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$`
(
 $templateData.query_input.metadata.data.templateData.query_input.column_metadata_output:getColumns();separator = ","$
)
$([templateData.query_input.metadata.data.templateData.query_input.input_data.intermediate_type,"_drift"])(templateData)$
>>

parquet(templateData)::=<<
ROW FORMAT SERDE
  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
STORED AS INPUTFORMAT
  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
LOCATION
  '$([first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_type,"_location"])(templateData)$'
>>


parquet_drift(templateData)::=<<
ROW FORMAT SERDE
  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
STORED AS INPUTFORMAT
  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
LOCATION
  '$([first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_type,"_location_drift"])(templateData)$'
>>

avro(templateData)::=<<
ROW FORMAT SERDE
    'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS INPUTFORMAT
    'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT
    'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
LOCATION
  '$([first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_type,"_location"])(templateData)$'
>>

avro_drift(templateData)::=<<
ROW FORMAT SERDE
    'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS INPUTFORMAT
    'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT
    'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
LOCATION
  '$([first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_type,"_location_drift"])(templateData)$'
>>




getColumns(columnMap)::=<<
$if(columnMap.istimezone)$
`$columnMap.field_name$` STRING, `$columnMap.field_name$_utc` $columnMap.destination_datatype_name$
$else$
`$columnMap.field_name$` $columnMap.destination_datatype_name$
$endif$
>>

redshift_location(templateData)::=<<
$if(first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_db)$$([first(templateData.query_input.metadata.data.query_input.table_metadata_output).file_system_type,"_file_system_type_destination"])(templateData)$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).s3_bucket$/$templateData.query_input.metadata.data.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$/$else$$([first(templateData.query_input.metadata.data.query_input.table_metadata_output).file_system_type,"_file_system_type_destination"])(templateData)$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).s3_bucket$/$templateData.query_input.metadata.data.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$/$endif$
>>

redshift_location_drift(templateData)::=<<
$if(first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).source_db)$$([first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type_destination"])(templateData)$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).s3_bucket$/$templateData.query_input.metadata.data.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$/$else$$([first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type_destination"])(templateData)$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).s3_bucket$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).data_movement_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$/$endif$
>>


s3_location(templateData)::=<<
$if(first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_db)$$([first(templateData.query_input.metadata.data.query_input.table_metadata_output).file_system_type,"_file_system_type_destination"])(templateData)$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).s3_bucket$/$templateData.query_input.metadata.data.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$/$else$$([first(templateData.query_input.metadata.data.query_input.table_metadata_output).file_system_type,"_file_system_type_destination"])(templateData)$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).s3_bucket$/$templateData.query_input.metadata.data.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$/$endif$
>>

hive_location(templateData)::=<<
$if(first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_db)$$([first(templateData.query_input.metadata.data.query_input.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).path$/$templateData.query_input.metadata.data.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$/$else$$([first(templateData.query_input.metadata.data.query_input.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).path$/$templateData.query_input.metadata.data.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$/$endif$
>>

hdfs_location(templateData)::=<<
$if(first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_db)$$([first(templateData.query_input.metadata.data.query_input.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).path$/$templateData.query_input.metadata.data.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$/$else$$([first(templateData.query_input.metadata.data.query_input.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).path$/$templateData.query_input.metadata.data.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$/$endif$
>>

bigquery_location(templateData)::=<<
$if(first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_db)$$([first(templateData.query_input.metadata.data.query_input.table_metadata_output).file_system_type,"_file_system_type_destination"])(templateData)$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).gcs_bucket$/$templateData.query_input.metadata.data.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$/*.parquet$else$$([first(templateData.query_input.metadata.data.query_input.table_metadata_output).file_system_type,"_file_system_type_destination"])(templateData)$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).gcs_bucket$/$templateData.query_input.metadata.data.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$/*.parquet$endif$
>>

bigquery_location_drift(templateData)::=<<
$if(first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).source_db)$$([first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type_destination"])(templateData)$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).gcs_bucket$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).data_movement_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$/*.parquet$else$$([first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type_destination"])(templateData)$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).gcs_bucket$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).data_movement_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$/*.parquet$endif$
>>


hive_location_drift(templateData)::=<<
$if(first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).source_db)$$([first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).path$/$templateData.query_input.metadata.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$/$else$$([first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).path$/$templateData.query_input.metadata.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$/$endif$
>>

hdfs_location_drift(templateData)::=<<
$if(first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).source_db)$$([first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).path$/$templateData.query_input.metadata.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).table_name$/$else$$([first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).path$/$templateData.query_input.metadata.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).table_name$/$endif$
>>



hdfs_file_system_type(templateData)::=<<
hdfs://
>>

abfss_file_system_type(templateData)::=<<
abfss://
>>

s3a_file_system_type(templateData)::=<<
s3a:/
>>

S3A_file_system_type(templateData)::=<<
s3a:/
>>

s3_file_system_type(templateData)::=<<
s3a:/
>>

S3A_file_system_type_destination(templateData)::=<<
s3:/
>>

s3_file_system_type_destination(templateData)::=<<
s3:/
>>

adls_gen2_file_system_type(templateData)::=<<
abfss://
>>

gcs_file_system_type(templateData)::=<<
gs:/
>>

gcs_file_system_type_destination(templateData)::=<<
gs:/
>>

redshift_database_config(templateData)::=<<
{
	"jwt_token":"$templateData.query_input.input_data.jwt_token$",
    "authentication_type":"auth_token",
	"truststore":false,
	"end_point": "$templateData.query_input.input_data.end_point$",
	"jdbc_driver": "$first(templateData.query_input.table_metadata_output).destination_jdbc_driver$",
	"maxconnectionsperpartition": "1",
	"minconnectionsperpartition" : "0",
	"jdbc_url": "jdbc:redshift://$first(templateData.query_input.table_metadata_output).destination_url$/$first(templateData.query_input.table_metadata_output).destination_db$",
	"credential_id": "$first(templateData.query_input.table_metadata_output).destination_credential_id$",
	"credential_type_id" : "$first(templateData.query_input.table_metadata_output).destination_credential_type_id$",
	"file_system_info": {
		"file_system_type" : "$first(templateData.query_input.table_metadata_output).file_system_type$",
		"bucket": "$first(templateData.query_input.table_metadata_output).s3_bucket$",
		"credential_id" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_id$",
		"credential_type_id" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type_id$",
		"credential_type" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type$"
    }
}
>>

azure_synapse_database_config(templateData)::=<<
{
	"jwt_token":"$templateData.query_input.input_data.jwt_token$",
    "authentication_type":"auth_token",
	"end_point": "$templateData.query_input.input_data.end_point$",
    "jdbc_driver": "$first(templateData.query_input.table_metadata_output).destination_jdbc_driver$",
	"maxconnectionsperpartition": "1",
	"minconnectionsperpartition" : "0",
    "jdbc_url": "jdbc:sqlserver://$first(templateData.query_input.table_metadata_output).destination_url$;database=$first(templateData.query_input.table_metadata_output).destination_db$;",
    "credential_id": "$first(templateData.query_input.table_metadata_output).destination_credential_id$",
    "credential_type_id" : "$first(templateData.query_input.table_metadata_output).destination_credential_type_id$",
	"file_system_info" : {
        "file_system_type" : "ADLS_GEN2",
        "credential_id": "$first(templateData.query_input.table_metadata_output).filesystem_credential_id$",
        "credential_type_id" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type_id$",
		"credential_type" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type$"
    }
}
>>

azure_synapse_scoped_credential_create(templateData)::=<<
$if(first(templateData.query_input.metadata.data.query_input.table_metadata_output).is_azure_oauth)$
IF NOT EXISTS (select top 1 * from sys.database_scoped_credentials where credential_identity='<client_id>@https://login.microsoftonline.com/<tenant_id>/oauth2/token'
and name = '$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_credential')
BEGIN
    CREATE DATABASE SCOPED CREDENTIAL [$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_credential]
    WITH IDENTITY = '<client_id>@https://login.microsoftonline.com/<tenant_id>/oauth2/token',
    SECRET = '<client_secret>'
END;
$else$
IF NOT EXISTS (select top 1 * from sys.database_scoped_credentials where credential_identity='$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$' and
name = '$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_credential')
BEGIN
    CREATE DATABASE SCOPED CREDENTIAL [$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_credential]
    WITH IDENTITY = '$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$',
    SECRET = '<access_key>'
END;
$endif$
>>

azure_synapse_scoped_credential_create_if_schema_drift(templateData)::=<<
$if(first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).is_azure_oauth)$
IF NOT EXISTS (select top 1 * from sys.database_scoped_credentials
where credential_identity='<client_id>@https://login.microsoftonline.com/<tenant_id>/oauth2/token'
and name = '$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_credential')
BEGIN
    CREATE DATABASE SCOPED CREDENTIAL [$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_credential]
    WITH IDENTITY = '<client_id>@https://login.microsoftonline.com/<tenant_id>/oauth2/token',
    SECRET = '<client_secret>'
END;
$else$
IF NOT EXISTS (select top 1 * from sys.database_scoped_credentials
where credential_identity='$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$'
and name = '$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_credential')
BEGIN
    CREATE DATABASE SCOPED CREDENTIAL [$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_credential]
    WITH IDENTITY = '$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$',
    SECRET = '<access_key>'
END;
$endif$
>>

azure_synapse_external_data_source_create(templateData)::=<<
$if(first(templateData.query_input.metadata.data.query_input.table_metadata_output).is_azure_oauth)$
IF NOT EXISTS (select top 1 * from sys.external_data_sources eds
inner join sys.database_scoped_credentials dsc on eds.credential_id = dsc.credential_id
where location ='abfss://$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$@$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$.dfs.core.windows.net' and credential_identity='<client_id>@https://login.microsoftonline.com/<tenant_id>/oauth2/token' and eds.name = '$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source')
BEGIN
CREATE EXTERNAL DATA SOURCE  [$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source]
WITH
(
	LOCATION = 'abfss://$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$@$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$.dfs.core.windows.net',
	CREDENTIAL = [$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_credential],
	TYPE = HADOOP
)
END;
$else$
IF NOT EXISTS (select top 1 * from sys.external_data_sources eds
inner join sys.database_scoped_credentials dsc on eds.credential_id = dsc.credential_id
where location ='abfss://$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$@$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$.dfs.core.windows.net'
and credential_identity='$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$' and eds.name = '$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_data_source')
BEGIN
CREATE EXTERNAL DATA SOURCE  [$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_data_source]
WITH
(
	LOCATION = 'abfss://$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$@$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$.dfs.core.windows.net',
	CREDENTIAL = [$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_credential],
	TYPE = HADOOP
)
END;
$endif$
>>

azure_synapse_external_data_source_create_if_schema_drift(templateData)::=<<
$if(first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).is_azure_oauth)$
IF NOT EXISTS (select top 1 * from sys.external_data_sources eds
inner join sys.database_scoped_credentials dsc on eds.credential_id = dsc.credential_id
where location ='abfss://$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$@$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$.dfs.core.windows.net'
and credential_identity='<client_id>@https://login.microsoftonline.com/<tenant_id>/oauth2/token'
and eds.name = '$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source')
BEGIN
CREATE EXTERNAL DATA SOURCE
[$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source]
WITH
(
	LOCATION = 'abfss://$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$@$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$.dfs.core.windows.net',
	CREDENTIAL = [$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_credential],
	TYPE = HADOOP
)
$else$
IF NOT EXISTS (select top 1 * from sys.external_data_sources eds
inner join sys.database_scoped_credentials dsc on eds.credential_id = dsc.credential_id
where location ='abfss://$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$@$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$.dfs.core.windows.net'
and credential_identity='$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$' and
eds.name = '$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_data_source')
BEGIN
CREATE EXTERNAL DATA SOURCE [$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_data_source]
WITH
(
	LOCATION = 'abfss://$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$@$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$.dfs.core.windows.net',
	CREDENTIAL = [$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_credential],
	TYPE = HADOOP
)
END;
$endif$
>>

azure_synapse_external_file_format_create(templateData)::=<<
IF NOT EXISTS (select top 1 * from sys.external_file_formats where name = 'parquetFileFormat' and data_compression = 'org.apache.hadoop.io.compress.SnappyCodec' and format_type = 'PARQUET')
BEGIN
CREATE EXTERNAL FILE FORMAT parquetFileFormat
WITH
(
    FORMAT_TYPE = PARQUET,
    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'
)
END;
>>

azure_synapse_external_table_create(templateData)::=<<
$(["azure_synapse_external_table_create_for_",templateData.query_input.metadata.data.query_input.input_data.metadata_category])(templateData)$
>>

azure_synapse_external_table_create_for_file(templateData)::=<<
IF EXISTS (SELECT top 1 * FROM sys.external_tables et
INNER JOIN sys.external_data_sources ds
ON et.data_source_id = ds.data_source_id
INNER JOIN sys.external_file_formats ff
ON et.file_format_id = ff.file_format_id
INNER JOIN sys.schemas s
ON s.schema_id = et.schema_id
where et.name = '$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$' and
ds.name = '$if(first(templateData.query_input.metadata.data.query_input.table_metadata_output).is_azure_oauth)$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source$else$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_data_source$endif$'
and ff.name = 'parquetFileFormat'
    and s.name = '$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$')
BEGIN
DROP EXTERNAL TABLE $first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$
CREATE EXTERNAL TABLE [$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$].[$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$](
$templateData.query_input.metadata.data.query_input.column_metadata_output:getColumnsAzureSynapse();separator = ",\n"$
)
WITH (
	LOCATION = '$templateData.query_input.metadata.data.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$.parquet',
    DATA_SOURCE = [$if(first(templateData.query_input.metadata.data.query_input.table_metadata_output).is_azure_oauth)$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source$else$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_data_source$endif$],
    FILE_FORMAT = parquetFileFormat,
    REJECT_TYPE = VALUE,
    REJECT_VALUE = 0
)
END
ELSE
BEGIN
CREATE EXTERNAL TABLE [$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$].[$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$](
$templateData.query_input.metadata.data.query_input.column_metadata_output:getColumnsAzureSynapse();separator = ",\n"$
)
WITH (
	LOCATION = '$templateData.query_input.metadata.data.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$.parquet',
    DATA_SOURCE = [$if(first(templateData.query_input.metadata.data.query_input.table_metadata_output).is_azure_oauth)$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source$else$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_data_source$endif$],
    FILE_FORMAT = parquetFileFormat,
    REJECT_TYPE = VALUE,
    REJECT_VALUE = 0
)
END;
>>

azure_synapse_external_table_create_for_relational(templateData)::=<<
IF EXISTS (SELECT top 1 * FROM sys.external_tables et
INNER JOIN sys.external_data_sources ds
ON et.data_source_id = ds.data_source_id
INNER JOIN sys.external_file_formats ff
ON et.file_format_id = ff.file_format_id
INNER JOIN sys.schemas s
ON s.schema_id = et.schema_id
where et.name = '$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$'
and ds.name = '$if(first(templateData.query_input.metadata.data.query_input.table_metadata_output).is_azure_oauth)$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source$else$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_data_source$endif$'
and ff.name = 'parquetFileFormat'
    and s.name = '$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$')
BEGIN
DROP EXTERNAL TABLE $first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$
CREATE EXTERNAL TABLE [$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$].[$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$](
$templateData.query_input.metadata.data.query_input.column_metadata_output:getColumnsAzureSynapse();separator = ",\n"$
)
WITH (
	LOCATION = '$templateData.query_input.metadata.data.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$.parquet',
    DATA_SOURCE = [$if(first(templateData.query_input.metadata.data.query_input.table_metadata_output).is_azure_oauth)$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source$else$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_data_source$endif$],
    FILE_FORMAT = parquetFileFormat,
    REJECT_TYPE = VALUE,
    REJECT_VALUE = 0
)
END
ELSE
BEGIN
CREATE EXTERNAL TABLE [$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$].[$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$](
$templateData.query_input.metadata.data.query_input.column_metadata_output:getColumnsAzureSynapse();separator = ",\n"$
)
WITH (
	LOCATION = '$templateData.query_input.metadata.data.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$.parquet',
    DATA_SOURCE = [$if(first(templateData.query_input.metadata.data.query_input.table_metadata_output).is_azure_oauth)$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source$else$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).filesystem_credential_epoch$_data_source$endif$],
    FILE_FORMAT = parquetFileFormat,
    REJECT_TYPE = VALUE,
    REJECT_VALUE = 0
)
END;
>>

azure_synapse_external_table_create_if_schema_drift(templateData)::=<<
$(["azure_synapse_external_table_create_if_schema_drift_",templateData.query_input.metadata.data.templateData.query_input.input_data.metadata_category])(templateData)$
>>


azure_synapse_external_table_create_if_schema_drift_relational(templateData)::=<<
IF EXISTS (SELECT top 1 * FROM sys.external_tables et
INNER JOIN sys.external_data_sources ds
ON et.data_source_id = ds.data_source_id
INNER JOIN sys.external_file_formats ff
ON et.file_format_id = ff.file_format_id
INNER JOIN sys.schemas s
ON s.schema_id = et.schema_id
where et.name = '$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$' and
ds.name = '$if(first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).is_azure_oauth)$$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source$else$$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_data_source$endif$'
and ff.name = 'parquetFileFormat'
and s.name = '$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$')
BEGIN
DROP EXTERNAL TABLE $first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$
CREATE EXTERNAL TABLE [$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$].[$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$](
$templateData.query_input.metadata.data.templateData.query_input.column_metadata_output:getColumnsAzureSynapse();separator = ",\n"$
)
WITH (
	LOCATION = '$templateData.query_input.metadata.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$.parquet',
    DATA_SOURCE = [$if(first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).is_azure_oauth)$$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source$else$$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_data_source$endif$],
    FILE_FORMAT = parquetFileFormat,
    REJECT_TYPE = VALUE,
    REJECT_VALUE = 0
)
END
ELSE
BEGIN
CREATE EXTERNAL TABLE [$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$].[$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$](
$templateData.query_input.metadata.data.templateData.query_input.column_metadata_output:getColumnsAzureSynapse();separator = ",\n"$
)
WITH (
	LOCATION = '$templateData.query_input.metadata.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$.parquet',
    DATA_SOURCE = [$if(first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).is_azure_oauth)$$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source$else$$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_data_source$endif$],
    FILE_FORMAT = parquetFileFormat,
    REJECT_TYPE = VALUE,
    REJECT_VALUE = 0
)
END;
>>

azure_synapse_external_table_create_if_schema_drift_file(templateData)::=<<
IF EXISTS (SELECT top 1 * FROM sys.external_tables et
INNER JOIN sys.external_data_sources ds
ON et.data_source_id = ds.data_source_id
INNER JOIN sys.external_file_formats ff
ON et.file_format_id = ff.file_format_id
INNER JOIN sys.schemas s
ON s.schema_id = et.schema_id
where et.name = '$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$' and
ds.name = '$if(first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).is_azure_oauth)$$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source$else$$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_data_source$endif$'
and ff.name = 'parquetFileFormat'
and s.name = '$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$')
BEGIN
DROP EXTERNAL TABLE $first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$
CREATE EXTERNAL TABLE [$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$].[$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$](
$templateData.query_input.metadata.data.templateData.query_input.column_metadata_output:getColumnsAzureSynapse();separator = ",\n"$
)
WITH (
	LOCATION = '$templateData.query_input.metadata.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$.parquet',
    DATA_SOURCE = [$if(first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).is_azure_oauth)$$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source$else$$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_data_source$endif$],
    FILE_FORMAT = parquetFileFormat,
    REJECT_TYPE = VALUE,
    REJECT_VALUE = 0
)
END
ELSE
BEGIN
CREATE EXTERNAL TABLE [$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$].[$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$](
$templateData.query_input.metadata.data.templateData.query_input.column_metadata_output:getColumnsAzureSynapse();separator = ",\n"$
)
WITH (
	LOCATION = '$templateData.query_input.metadata.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$.parquet',
    DATA_SOURCE = [$if(first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).is_azure_oauth)$$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source$else$$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_data_source$endif$],
    FILE_FORMAT = parquetFileFormat,
    REJECT_TYPE = VALUE,
    REJECT_VALUE = 0
)
END;
>>

getColumnsbigquery(columnMap)::=<<
$if(columnMap.istimezone)$
$columnMap.field_name$ VARCHAR(50), $columnMap.field_name$_utc $columnMap.destination_datatype_name$
$else$
$columnMap.field_name$ $columnMap.destination_datatype_name$
$endif$
>>

getColumnsRedshift(columnMap)::=<<
$if(columnMap.istimezone)$
$columnMap.field_name$ VARCHAR(50), $columnMap.field_name$_utc $columnMap.destination_datatype_name$
$else$
$columnMap.field_name$ $columnMap.destination_datatype_name$
$endif$
>>

getColumnsAzureSynapse(columnMap)::=<<
$if(columnMap.istimezone)$
$columnMap.field_name$ VARCHAR(50), $columnMap.field_name$_utc $columnMap.destination_datatype_name$
$else$
$columnMap.field_name$ $columnMap.destination_datatype_name$
$endif$
>>

get_destination_type(templateData)::=<<
"$templateData.query_input.input_data.destination$"
>>


bigquery_database_config(templateData)::=<<
{
	"jwt_token":"$templateData.query_input.input_data.jwt_token$",
    "authentication_type":"auth_token",
	"truststore":false,
	"end_point": "$templateData.query_input.input_data.end_point$",
	"jdbc_driver": "$first(templateData.query_input.table_metadata_output).destination_jdbc_driver$",
	"maxconnectionsperpartition": "1",
	"minconnectionsperpartition" : "0",
	"jdbc_url": "jdbc:bigquery://https://www.googleapis.com/bigquery/v2:443",
	"credential_id": "$first(templateData.query_input.table_metadata_output).destination_credential_id$",
	"credential_type_id" : "$first(templateData.query_input.table_metadata_output).destination_credential_type_id$",
	"file_system_info" : {
		"file_system_type" : "gcs",
		"credential_id": "$first(templateData.query_input.table_metadata_output).filesystem_credential_id$",
		"credential_type_id" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type_id$",
		"credential_type" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type$"
    }
}
>>


hive_database_config(templateData)::=<<
{
	"jwt_token":"$templateData.query_input.input_data.jwt_token$",
    $if(first(templateData.query_input.table_metadata_output).is_destination_credential_kerberos)$
    "authentication_type":"kerberos",
    $else$
    "authentication_type":"auth_token",
    $endif$
	"truststore":false,
	"end_point": "$templateData.query_input.input_data.end_point$",
	$if(first(templateData.query_input.table_metadata_output).is_apache_jdbc_driver)$
    "jdbc_driver": "org.apache.hive.jdbc.HiveDriver",
    $else$
	"jdbc_driver": "$first(templateData.query_input.table_metadata_output).destination_jdbc_driver$",
	$endif$
	"maxconnectionsperpartition": "1",
	"minconnectionsperpartition" : "0",
	"jdbc_url": "$first(templateData.query_input.table_metadata_output).destination_url$",
	"credential_id": "$first(templateData.query_input.table_metadata_output).destination_credential_id$",
	"credential_type_id" : "$first(templateData.query_input.table_metadata_output).destination_credential_type_id$",
	"file_system_info" : {
		"file_system_type" : "$first(templateData.query_input.table_metadata_output).file_system_type$",
		"credential_id": "$first(templateData.query_input.table_metadata_output).filesystem_credential_id$",
		"credential_type_id" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type_id$",
		"credential_type" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type$"
    }
}
>>


Configuration_s3a(templateData)::=<<
spark.conf.set("spark.hadoop.fs.s3a.endpoint",$if(first(templateData.table_metadata_output).filesystem_endpoint)$"$first(templateData.table_metadata_output).filesystem_endpoint$" $else$ "" $endif$ )

spark.conf.set("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
$if(first(templateData.table_metadata_output).filesystem_credential_id)$
spark.conf.set("spark.hadoop.fs.s3a.access.key", Aws.target.access_id)
spark.conf.set("spark.hadoop.fs.s3a.secret.key", Aws.target.secret_access_key)
$endif$
spark.conf.set("spark.hadoop.fs.s3a.fast.upload", "true")
spark.conf.set("spark.hadoop.fs.s3a.multiobjectdelete.enable", "false")
spark.conf.set("spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version", "2")
spark.conf.set("spark.hadoop.fs.s3a.fast.upload.buffer", "bytebuffer")
spark.conf.set("spark.speculation", "false")
spark.conf.set("spark.sql.parquet.filterPushdown", "true")
spark.conf.set("spark.sql.parquet.mergeSchema", "false")
>>

Configuration_hdfs(templateData)::=<<

>>

oracle_parquet_snowflake_intermediate_stg_function()::=<<
case
                when (b.source_datatype_name = 'number') then
                (
                        CASE
                    when (data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function
                     else 'AsIs'
                END
                )
                when (b.source_datatype_name = 'float') then
            (
                    CASE
                                when (a.data_precision between 0 and 53) then 'ParquetStringToDoubleConvert'
                                else 'AsIs'
                        END
                )
                else b.intermediate_stg_function
        end
>>

oracle_parquet_snowflake_source_stg_function()::=<<
$get_source_stg_function()$
>>

oracle_parquet_snowflake_inconsistent_datatype() ::=<<
case
    when (b.source_datatype_name in ('number','decimal')) then
    (
        case
            when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
            else 'number_to_varchar'
        end
    )
    when (b.source_datatype_name ~* 'interval year') then 'interval_year_to_month_to_varchar'
    when (b.source_datatype_name ~* 'interval day') then 'interval_day_to_second_to_varchar'
    when (b.source_datatype_name in ('rowid','urowid','xmltype','blob','date'))then concat(b.source_datatype_name,'_to_varchar')
    when (b.source_datatype_name = 'float') then
    (
        case
            when (a.data_precision <=53) then null
            else 'float_to_varchar'
        end
    )
    when (b.source_datatype_name ~* 'local time zone') then 'timestamp_with_local_time_zone_to_varchar'
    when (b.source_datatype_name ~* 'time zone') then 'timestamp_with_time_zone_to_varchar'
    when (b.source_datatype_name ~* 'timestamp') then 'timestamp_to_varchar'
    else null
end
>>

oracle_parquet_snowflake_destination_datatype_name()::=<<
case
           when (b.source_datatype_name = 'number') then
           (
              CASE
                 when ((a.data_scale <= 37) and (a.data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
                 else 'STRING'
              END
           )
           when (b.source_datatype_name = 'float') then
           (
                  case
                         when (a.data_precision between 0 and 53) then 'DOUBLE'
                         else 'STRING'
                  END
           )
           when(b.source_datatype_name = 'varchar2' or b.source_datatype_name = 'nvarchar2') then
           (
              CASE
                 when (a.data_length between 1 and 4000) then concat(b.destination_datatype_name,'(',a.data_length,')')
                 else 'STRING'
              END
           )
           when(b.source_datatype_name = 'char' or b.source_datatype_name = 'nchar') then
           (
              CASE
                 when (a.data_length between 1 and 2000) then concat(b.destination_datatype_name,'(',a.data_length,')')
                 else 'STRING'
              END
           )
           else b.destination_datatype_name
        end
>>

sql_server_parquet_snowflake_destination_datatype_name()::=<<
CASE
        when (b.source_datatype_name in ('char','varchar','nchar','nvarchar')) then
        (
            CASE
                when (a.data_length = -1) then concat(b.destination_datatype_name,'(',16777216,')')
                else concat(b.destination_datatype_name,'(', a.data_length,')')
            END
        )
        when (b.source_datatype_name = 'decimal' or b.source_datatype_name = 'numeric') then
        (
            CASE
                when (data_precision between 1 and 38) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
                else 'STRING'
            END
        )
        else b.destination_datatype_name
END
>>


sql_server_parquet_snowflake_intermediate_stg_function()::=<<
CASE
        when (b.source_datatype_name = 'decimal' or b.source_datatype_name = 'numeric') then
        (
            CASE
                when (a.data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function
                else 'AsIs'
            END
        )
        else b.intermediate_stg_function
END
>>

mysql_parquet_snowflake_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name='decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    else b.intermediate_stg_function
END
>>

mysql_parquet_snowflake_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
		CASE
			when (a.data_length between 1 and 255) then concat('CHAR','(',a.data_length,')')
			else 'STRING'
		END
    )
    when (b.source_datatype_name = 'varchar') then
    (
		CASE
			when (a.data_length between 1 and 65535) then concat('VARCHAR','(',a.data_length,')')
			else 'STRING'
		END
    )
    when (b.source_datatype_name = 'decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then concat('DECIMAL','(',a.data_precision,',',a.data_scale,')')
			else 'STRING'
		END
    )
    else b.destination_datatype_name
END
>>

postgres_parquet_snowflake_destination_datatype_name()::=<<
CASE
            when (b.source_datatype_name = 'character') then concat('char','(',a.data_length,')')

            when (b.source_datatype_name = 'character varying' or b.source_datatype_name = 'bit' or b.source_datatype_name = 'bit varying') then concat('varchar','(',a.data_length,')')

            when (b.source_datatype_name = 'numeric') then
            (
                CASE
                    when ((a.data_scale<=37) and a.data_precision <=38) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
                    else concat('varchar','(',a.data_precision+1,')')
            END
            )
            else b.destination_datatype_name
        end
>>

postgres_parquet_snowflake_intermediate_stg_function()::=<<
CASE
            when (b.source_datatype_name = 'character') then
            (
                CASE
                    when (a.data_length between 1 and 255) then b.intermediate_stg_function
                    else 'AsIs'
                END
            )
            when (b.source_datatype_name='character varying') then
            (
                CASE
                    when (a.data_length between 1 and 65535) then b.intermediate_stg_function
                    else 'AsIs'
                END
            )
            when (b.source_datatype_name = 'numeric') then
            (
                CASE
                    when (a.data_precision between 1 and 38) then b.intermediate_stg_function
                    else 'AsIs'
                END
            )
            else b.intermediate_stg_function
        end
>>

sql_server_parquet_snowflake_source_stg_function()::=<<
$get_source_stg_function()$
>>

postgres_parquet_snowflake_source_stg_function()::=<<
$get_source_stg_function()$
>>

mysql_parquet_snowflake_source_stg_function()::=<<
$get_source_stg_function()$
>>

sql_server_parquet_snowflake_inconsistent_datatype() ::=<<
case
when (b.source_datatype_name in ('bit','datetime2','time','datetimeoffset','binary','varbinary','timestamp','smalldatetime','datetime')) then concat(b.source_datatype_name,'_to_string')
else null
end
>>

mysql_parquet_snowflake_inconsistent_datatype() ::=<<
case
when (b.source_datatype_name = 'decimal') then
(
case
when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
else 'decimal_to_string'
end
)
when (b.source_datatype_name = 'year') then concat(b.source_datatype_name,'_to_int')
when (b.source_datatype_name in ('time','bit','binary','varbinary','tinyblob','blob','mediumblob','longblob','timestamp','datetime')) then concat(b.source_datatype_name,'_to_string')
else null
end
>>

postgres_parquet_snowflake_inconsistent_datatype() ::=<<
case
                when (b.source_datatype_name = 'numeric') then
                (
                    case
                        when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
                        else 'numeric_to_varchar'
                    end
                )
                when (b.source_datatype_name = 'time without time zone') then 'time_without_time_zone_to_varchar'
                when (b.source_datatype_name = 'time with time zone') then 'time_with_time_zone_to_varchar'
                when (b.source_datatype_name = 'timestamp with time zone') then 'timestamp_with_time_zone_to_varchar'
                when (b.source_datatype_name = 'timestamp without time zone') then 'timestamp_without_time_zone_to_varchar'
                when (b.source_datatype_name in ('money','bit','abstime')) then concat(b.source_datatype_name,'_to_varchar')
                else null
            end
>>

hive_parquet_snowflake_source_stg_function()::=<<
$get_source_stg_function()$
>>

hive_parquet_snowflake_inconsistent_datatype() ::=<<
case
when (b.source_datatype_name = 'decimal') then
(
case
when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
else 'decimal_to_string'
end
)
when (b.source_datatype_name in ('binary','map','struct','array')) then concat(b.source_datatype_name,'_to_string')
else null
end
>>

hive_parquet_snowflake_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name='decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    else b.intermediate_stg_function
END
>>

hive_parquet_snowflake_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
		CASE
			when (a.data_length between 1 and 255) then concat('char','(',a.data_length,')')
			else 'string'
		END
    )
    when (b.source_datatype_name = 'varchar') then
    (
		CASE
			when (a.data_length between 1 and 65535) then concat('varchar','(',a.data_length,')')
			else 'string'
		END
    )
    when (b.source_datatype_name = 'decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then concat('decimal','(',a.data_precision,',',a.data_scale,')')
			else 'STRING'
		END
    )
    else b.destination_datatype_name
END
>>

snowflake_database_config(templateData)::=<<
{
	"jwt_token":"$templateData.query_input.input_data.jwt_token$",
    "authentication_type":"auth_token",
	"truststore":false,
	"end_point": "$templateData.query_input.input_data.end_point$",
	"jdbc_driver": "$first(templateData.query_input.table_metadata_output).destination_jdbc_driver$",
	"maxconnectionsperpartition": "1",
	"minconnectionsperpartition" : "0",
	"jdbc_url": "jdbc:snowflake://$first(templateData.query_input.table_metadata_output).destination_url$",
	"credential_id": "$first(templateData.query_input.table_metadata_output).destination_credential_id$",
	"credential_type_id" : "$first(templateData.query_input.table_metadata_output).destination_credential_type_id$",
	"file_system_info" : {
		"file_system_type" : "$first(templateData.query_input.table_metadata_output).file_system_type$",
		"credential_id": "$first(templateData.query_input.table_metadata_output).filesystem_credential_id$",
		"credential_type_id" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type_id$",
		"credential_type" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type$"
    }
}
>>

snowflake_stage_create(templateData)::=<<
$(["snowflake_",templateData.query_input.database_config.file_system_info.file_system_type,"_stage_create"])(templateData)$
>>

snowflake_storage_integration_create(templateData)::=<<
$(["snowflake_",templateData.query_input.database_config.file_system_info.file_system_type,"_storage_integration_create"])(templateData)$
>>

snowflake_adls_gen2_storage_integration_create(templateData)::=<<
CREATE OR REPLACE STORAGE INTEGRATION $first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_storage_integration
  TYPE = EXTERNAL_STAGE
  STORAGE_PROVIDER = AZURE
  AZURE_TENANT_ID = '<tenant_id>'
  STORAGE_ALLOWED_LOCATIONS = ('azure://$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$.blob.core.windows.net/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$')
>>


snowflake_adls_gen2_stage_create(templateData)::=<<
$if(first(templateData.query_input.metadata.data.query_input.table_metadata_output).is_azure_oauth)$
CREATE OR REPLACE STAGE $first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_stage
STORAGE_INTEGRATION = $first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_storage_integration
URL='azure://$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$.blob.core.windows.net/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$'
$else$
CREATE OR REPLACE STAGE $first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_stage
credentials=(azure_sas_token='<sas_token>')
URL='azure://$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$.blob.core.windows.net/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$'
$endif$
>>

snowflake_s3_storage_integration_create(templateData)::=<<
CREATE OR REPLACE STORAGE INTEGRATION $first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).s3_bucket$_storage_integration
TYPE = EXTERNAL_STAGE
STORAGE_PROVIDER = S3
STORAGE_AWS_ROLE_ARN = '<iam_role>'
STORAGE_ALLOWED_LOCATIONS = ('s3://$first(templateData.query_input.metadata.data.query_input.table_metadata_output).s3_bucket$/')
>>

snowflake_s3_stage_create(templateData)::=<<
$if(first(templateData.query_input.metadata.data.query_input.table_metadata_output).is_aws_iam)$
CREATE OR REPLACE STAGE
$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).s3_bucket$_stage
URL = 's3://$first(templateData.query_input.metadata.data.query_input.table_metadata_output).s3_bucket$'
STORAGE_INTEGRATION = $first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).s3_bucket$_storage_integration
FILE_FORMAT = (TYPE=$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_file_format$)
$else$
CREATE OR REPLACE STAGE
$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).s3_bucket$_stage
URL = 's3://$first(templateData.query_input.metadata.data.query_input.table_metadata_output).s3_bucket$'
CREDENTIALS = (AWS_KEY_ID = '<aws_access_id>' AWS_SECRET_KEY = '<aws_secret_key>')
FILE_FORMAT = (TYPE=$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_file_format$)
$endif$
>>

getColumnsSnowflake(columnMap)::=<<
$if(columnMap.istimezone)$
$columnMap.field_name$ VARCHAR(50) AS (VALUE:$columnMap.field_name$::VARCHAR(50)), $columnMap.field_name$_utc $columnMap.destination_datatype_name$ AS (VALUE:$columnMap.field_name$_utc::$columnMap.destination_datatype_name$)
$else$
$columnMap.field_name$ $columnMap.destination_datatype_name$ AS (VALUE:$columnMap.field_name$::$columnMap.destination_datatype_name$)
$endif$
>>

create_snowflake_table(templateData)::=<<
CREATE OR REPLACE EXTERNAL TABLE
$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$
(
$templateData.query_input.metadata.data.query_input.column_metadata_output:getColumnsSnowflake();separator = ","$
)
WITH LOCATION = @$snowflake_location(templateData)$
FILE_FORMAT = (TYPE= parquet)
AUTO_REFRESH = FALSE
PATTERN='.*[.]parquet'
>>

generate_almaren_artifact_snowflake(templateData)::=<<
$(["generate_almaren_artifact_snowflake_",first(templateData.table_metadata_output).file_system_type])(templateData)$
>>

generate_almaren_artifact_snowflake_s3(templateData)::=<<
$generate_almaren_artifact_redshift(templateData)$
>>

generate_almaren_artifact_snowflake_adls_gen2(templateData)::=<<
$generate_almaren_artifact_azure_synapse(templateData)$
>>

snowflake_destination(templateData)::=<<
$(["snowflake_",first(templateData.table_metadata_output).file_system_type,"_destination"])(templateData)$
>>

snowflake_s3_destination(templateData)::=<<
option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$/$first(templateData.table_metadata_output).s3_bucket$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).destination_table_name$").save()
>>

snowflake_adls_gen2_destination(templateData)::=<<
option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).adls_gen2_container$@$first(templateData.table_metadata_output).storage_account$.dfs.core.windows.net/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).destination_table_name$.parquet").save()
>>

snowflake_location(templateData)::=<<
$(["snowflake_",first(templateData.query_input.metadata.data.query_input.table_metadata_output).file_system_type,"_location"])(templateData)$
>>

snowflake_location_drift(templateData)::=<<
$(["snowflake_",first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).file_system_type,"_location_drift"])(templateData)$
>>

snowflake_s3_location(templateData)::=<<
$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).s3_bucket$_stage/$templateData.query_input.metadata.data.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$/>>

snowflake_s3_location_drift(templateData)::=<<
$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).s3_bucket$_stage/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).data_movement_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$/
>>

snowflake_adls_gen2_location(templateData)::=<<
$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.query_input.table_metadata_output).adls_gen2_container$_stage/$templateData.query_input.metadata.data.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$.parquet/
>>

snowflake_adls_gen2_location_drift(templateData)::=<<
$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_stage/$templateData.query_input.metadata.data.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$.parquet/
>>

create_snowflake_bkp_table(templateData)::=<<
CREATE OR REPLACE TABLE $templateData.destination_db$.$templateData.destination_schema_name$.$templateData.destination_backup_table_name$ as select * from $templateData.destination_db$.$templateData.destination_schema_name$.$templateData.destination_table_name$;
>>

get_partition_information_test(templateData)::=<<
select schema_name,table_name,partition_name,sub_partition_name,
case when sub_partition_name is not null then true
else false
end as is_sub_partition
from
(select
t.schema_name,t.table_name,t.partition_name,t2.sub_partition_name
from (select pti.schema_name ,pi2.partition_name ,
pti.partition_type,pti.table_id,pti.table_name
from nabu.partition_table_info pti
left join nabu.partitions_info pi2 on pti.table_id = pi2.table_id
where pti.table_id in ($templateData.input_data.where_condition$) and pti.valid_to_ts = '9999-12-31' and pi2.valid_to_ts = '9999-12-31' ) t
left join
(select sub_partition_name,spi.partition_name from nabu.sub_partitions_info spi
where spi.table_id in ($templateData.input_data.where_condition$) and spi.valid_to_ts = '9999-12-31') t2 on t.partition_name = t2.partition_name) t3
>>

snowflake_storage_integration_create_if_schema_drift(templateData)::=<<
$(["snowflake_",templateData.query_input.database_config.file_system_info.file_system_type,"_stage_create_if_schema_drift"])(templateData)$
>>

snowflake_stage_create_if_schema_drift(templateData)::=<<
$(["snowflake_",templateData.query_input.database_config.file_system_info.file_system_type,"_stage_create_if_schema_drift"])(templateData)$
>>

snowflake_adls_gen2_storage_integration_create_if_schema_drift(templateData)::=<<
CREATE OR REPLACE STORAGE INTEGRATION $first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$..$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_storage_integration
TYPE = EXTERNAL_STAGE
STORAGE_PROVIDER = AZURE
AZURE_TENANT_ID = '<tenant_id>'
STORAGE_ALLOWED_LOCATIONS = ('azure://$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$.blob.core.windows.net/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$')
>>

snowflake_adls_gen2_stage_create_if_schema_drift(templateData)::=<<
$if(first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).is_azure_oauth)$
CREATE OR REPLACE STAGE $first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$..$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_stage
STORAGE_INTEGRATION = $first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$..$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_storage_integration
URL='azure://$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$.blob.core.windows.net/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$'
$else$
CREATE OR REPLACE STAGE $first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$..$first(templateData.query_input.metadata.data.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$_stage
credentials=(azure_sas_token='<sas_token>')
URL='azure://$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).storage_account$.blob.core.windows.net/$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).adls_gen2_container$'
$endif$
>>

snowflake_s3_storage_integration_create_if_schema_drift(templateData)::=<<
CREATE OR REPLACE STORAGE INTEGRATION $first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).s3_bucket$_storage_integration
TYPE = EXTERNAL_STAGE
STORAGE_PROVIDER = S3
STORAGE_AWS_ROLE_ARN = '<iam_role>'
STORAGE_ALLOWED_LOCATIONS = ('s3://$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).s3_bucket$/')
>>

snowflake_s3_stage_create_if_schema_drift(templateData)::=<<
$if(first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).is_aws_iam)$
CREATE OR REPLACE STAGE $first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).s3_bucket$_stage
URL = 's3://$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).s3_bucket$/'
STORAGE_INTEGRATION = $first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).s3_bucket$_storage_integration
FILE_FORMAT = (TYPE=$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_file_format$)
$else$
CREATE OR REPLACE STAGE
$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).s3_bucket$_stage
URL = 's3://$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).s3_bucket$/'
CREDENTIALS = (AWS_KEY_ID = '<aws_access_id>' AWS_SECRET_KEY = '<aws_secret_key>')
FILE_FORMAT = (TYPE=$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_file_format$)
$endif$
>>

create_snowflake_table_if_schema_drift(templateData)::=<<
CREATE OR REPLACE EXTERNAL TABLE
$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.templateData.query_input.table_metadata_output).destination_table_name$
(
 $templateData.query_input.metadata.data.templateData.query_input.column_metadata_output:getColumnsSnowflake();separator = ","$
)
WITH LOCATION = @$snowflake_location_drift(templateData)$
FILE_FORMAT = (TYPE= parquet)
AUTO_REFRESH = FALSE
PATTERN='.*[.]parquet'
>>
